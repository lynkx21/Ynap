{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve\n",
    "from sklearn.metrics import make_scorer, confusion_matrix\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/science/shared/ipythonNotebooks/leom/Kaggle/Ynap-master/ynap_data\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(os.getcwd(), 'ynap_data')\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transactions.csv', 'df_tot_log.csv', '~$sk_overview.docx', 'df_quarterly.csv', 'df_tot_log_noout.csv', 'df_monthly.csv', 'df_quarterly_log.csv', 'account.csv', 'Thumbs.db', 'df_tot.csv', 'df_quarterly_log_noout.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tot = pd.read_csv(os.path.join(data_path, 'df_quarterly_log.csv'))\n",
    "df_tot['var3'] = df_tot['var3'].astype('category')\n",
    "df_tot['var5'] = df_tot['var5'].astype('category')\n",
    "df_tot['var6'] = df_tot['var6'].astype('category')\n",
    "\n",
    "# make dummies out of categorical variables\n",
    "dummy_idx = np.where(df_tot.dtypes == 'category')[0]\n",
    "df_dummies = pd.get_dummies(df_tot.iloc[:, dummy_idx])\n",
    "df = pd.concat([df_tot.drop(df_tot.iloc[:, dummy_idx].columns.values, axis=1), df_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15695 entries, 0 to 15694\n",
      "Columns: 113 entries, customer_id to var6_1.0\n",
      "dtypes: float64(103), int64(2), uint8(8)\n",
      "memory usage: 12.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance\n",
    "Let's use the standard transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_for_model = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15695, 111)\n",
      "(15695, 2)\n"
     ]
    }
   ],
   "source": [
    "X = df_for_model.drop(['customer_id', 'lapsed_next_period'], axis=1)\n",
    "y = df_for_model[['customer_id', 'lapsed_next_period']]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(657, 111)\n",
      "(11899, 111)\n",
      "0.05232558139534884\n"
     ]
    }
   ],
   "source": [
    "X_train_1 = X_train.loc[y_train[y_train['lapsed_next_period'] == 1].index]\n",
    "X_train_0 = X_train.loc[y_train[y_train['lapsed_next_period'] == 0].index]\n",
    "\n",
    "print(X_train_1.shape)\n",
    "print(X_train_0.shape)\n",
    "print(X_train_1.shape[0]/X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.052245938196877985"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[y_test['lapsed_next_period'] == 1].shape[0] / y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different balance ratios with a couple of easy models\n",
    "\n",
    "def balance_tester(Xtr0, Xtr1, ytr, Xte, yte, zero_quotas = [0.5, 0.66, 0.75], seeds = [21, 2121, 1212], \n",
    "                   method='log', method_params=None):\n",
    "    \n",
    "    if (method == 'rf') & (method_params == None):\n",
    "        # Random Forest parameters\n",
    "        rf_params_bal = {'n_jobs': -1, 'n_estimators': 500, 'warm_start': True, \n",
    "                         'max_depth': 4, 'min_samples_leaf': 2, 'max_features' : 'sqrt',\n",
    "                         'verbose': 0}\n",
    "    \n",
    "    results = pd.DataFrame(columns=['quota', 'accuracy', 'pred_perc', 'recall'])\n",
    "    \n",
    "    for quota in zero_quotas:\n",
    "        accuracy = []\n",
    "        pred_perc = []\n",
    "        recall = []\n",
    "        \n",
    "        n_sample = int(Xtr1.shape[0] * quota / (1 - quota) // 1)\n",
    "        \n",
    "        for seed in seeds:\n",
    "            Xtr0_ = Xtr0.sample(n=n_sample, random_state=seed)\n",
    "            Xtr_ = Xtr0_.append(Xtr1)\n",
    "            ytr_ = ytr.loc[Xtr_.index]\n",
    "            \n",
    "            if method == 'log':\n",
    "                clf = LogisticRegression(max_iter=100, solver='liblinear')\n",
    "            elif method == 'rf':\n",
    "                clf = RandomForestClassifier(**rf_params_bal)\n",
    "            else:\n",
    "                return 'Method not recognized'\n",
    "            \n",
    "            clf.fit(Xtr_, ytr_)\n",
    "            clf_pred = clf.predict(Xte)\n",
    "            cf = confusion_matrix(yte, clf_pred)\n",
    "            \n",
    "            pred_perc.append(clf_pred.sum()/len(clf_pred))\n",
    "            accuracy.append(accuracy_score(yte, clf_pred))\n",
    "            recall.append(cf[1, 1] / cf[1, :].sum())\n",
    "            \n",
    "            \n",
    "        # print(np.array(accuracy).mean())\n",
    "        acc_mean = np.array(accuracy).mean()\n",
    "        pp_mean = np.array(pred_perc).mean()\n",
    "        rec_mean = np.array(recall).mean()\n",
    "        \n",
    "        results = results.append(pd.Series({'quota': quota, 'accuracy': acc_mean,\n",
    "                                            'pred_perc': pp_mean, 'recall': rec_mean}),\n",
    "                                 ignore_index=True)\n",
    "    \n",
    "    return results\n",
    "            \n",
    "quotas = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.825, 0.85, 0.9]\n",
    "# quotas = [0.8, 0.805, 0.81, 0.815, 0.82, 0.825, 0.83, 0.835, 0.84, 0.845, 0.85]\n",
    "balance_res_rf = balance_tester(X_train_0, X_train_1, y_train['lapsed_next_period'], X_test, \n",
    "                                y_test['lapsed_next_period'], quotas, method='rf')\n",
    "\n",
    "balance_res_log = balance_tester(X_train_0, X_train_1, y_train['lapsed_next_period'], X_test, \n",
    "                                y_test['lapsed_next_period'], quotas, method='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quota</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>pred_perc</th>\n",
       "      <th>recall</th>\n",
       "      <th>acc_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.738027</td>\n",
       "      <td>0.289795</td>\n",
       "      <td>0.766260</td>\n",
       "      <td>0.754967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550</td>\n",
       "      <td>0.761389</td>\n",
       "      <td>0.262398</td>\n",
       "      <td>0.727642</td>\n",
       "      <td>0.741141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.783158</td>\n",
       "      <td>0.235744</td>\n",
       "      <td>0.680894</td>\n",
       "      <td>0.721800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.650</td>\n",
       "      <td>0.810024</td>\n",
       "      <td>0.205267</td>\n",
       "      <td>0.646341</td>\n",
       "      <td>0.711815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.700</td>\n",
       "      <td>0.834342</td>\n",
       "      <td>0.175003</td>\n",
       "      <td>0.589431</td>\n",
       "      <td>0.687395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.860359</td>\n",
       "      <td>0.141128</td>\n",
       "      <td>0.514228</td>\n",
       "      <td>0.652680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.800</td>\n",
       "      <td>0.892641</td>\n",
       "      <td>0.097802</td>\n",
       "      <td>0.408537</td>\n",
       "      <td>0.602178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.825</td>\n",
       "      <td>0.904641</td>\n",
       "      <td>0.077944</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.561856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.850</td>\n",
       "      <td>0.918339</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>0.260163</td>\n",
       "      <td>0.523433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.900</td>\n",
       "      <td>0.945312</td>\n",
       "      <td>0.010725</td>\n",
       "      <td>0.079268</td>\n",
       "      <td>0.425686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quota  accuracy  pred_perc    recall  acc_recall\n",
       "0  0.500  0.738027   0.289795  0.766260    0.754967\n",
       "1  0.550  0.761389   0.262398  0.727642    0.741141\n",
       "2  0.600  0.783158   0.235744  0.680894    0.721800\n",
       "3  0.650  0.810024   0.205267  0.646341    0.711815\n",
       "4  0.700  0.834342   0.175003  0.589431    0.687395\n",
       "5  0.750  0.860359   0.141128  0.514228    0.652680\n",
       "6  0.800  0.892641   0.097802  0.408537    0.602178\n",
       "7  0.825  0.904641   0.077944  0.333333    0.561856\n",
       "8  0.850  0.918339   0.056600  0.260163    0.523433\n",
       "9  0.900  0.945312   0.010725  0.079268    0.425686"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_res_log['acc_recall'] = 0.4 * balance_res_log['accuracy'] + 0.6 * balance_res_log['recall']\n",
    "balance_res_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quota</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>pred_perc</th>\n",
       "      <th>recall</th>\n",
       "      <th>acc_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.741637</td>\n",
       "      <td>0.287034</td>\n",
       "      <td>0.774390</td>\n",
       "      <td>0.761289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550</td>\n",
       "      <td>0.768398</td>\n",
       "      <td>0.258363</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.761018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.792609</td>\n",
       "      <td>0.228629</td>\n",
       "      <td>0.703252</td>\n",
       "      <td>0.738995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.650</td>\n",
       "      <td>0.810980</td>\n",
       "      <td>0.204099</td>\n",
       "      <td>0.644309</td>\n",
       "      <td>0.710977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.700</td>\n",
       "      <td>0.834555</td>\n",
       "      <td>0.173516</td>\n",
       "      <td>0.577236</td>\n",
       "      <td>0.680163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.872996</td>\n",
       "      <td>0.127004</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.649198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.800</td>\n",
       "      <td>0.908463</td>\n",
       "      <td>0.074121</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.563385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.825</td>\n",
       "      <td>0.923012</td>\n",
       "      <td>0.052565</td>\n",
       "      <td>0.266260</td>\n",
       "      <td>0.528961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.850</td>\n",
       "      <td>0.940533</td>\n",
       "      <td>0.021451</td>\n",
       "      <td>0.136179</td>\n",
       "      <td>0.457921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.900</td>\n",
       "      <td>0.947754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quota  accuracy  pred_perc    recall  acc_recall\n",
       "0  0.500  0.741637   0.287034  0.774390    0.761289\n",
       "1  0.550  0.768398   0.258363  0.756098    0.761018\n",
       "2  0.600  0.792609   0.228629  0.703252    0.738995\n",
       "3  0.650  0.810980   0.204099  0.644309    0.710977\n",
       "4  0.700  0.834555   0.173516  0.577236    0.680163\n",
       "5  0.750  0.872996   0.127004  0.500000    0.649198\n",
       "6  0.800  0.908463   0.074121  0.333333    0.563385\n",
       "7  0.825  0.923012   0.052565  0.266260    0.528961\n",
       "8  0.850  0.940533   0.021451  0.136179    0.457921\n",
       "9  0.900  0.947754   0.000000  0.000000    0.379102"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_res_rf['acc_recall'] = 0.4 * balance_res_rf['accuracy'] + 0.6 * balance_res_rf['recall']\n",
    "balance_res_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1314, 111)\n",
      "(1314, 2)\n"
     ]
    }
   ],
   "source": [
    "quota = 0.5\n",
    "n_sample = int(X_train_1.shape[0] * quota / (1 - quota) // 1)\n",
    "X_train_0_ = X_train_0.sample(n=n_sample, random_state=101)\n",
    "\n",
    "X_train_ = X_train_0_.append(X_train_1)\n",
    "print(X_train_.shape)\n",
    "\n",
    "y_train_ = y_train.loc[X_train_.index]\n",
    "print(y_train_.shape)\n",
    "\n",
    "df_ = y_train_.join(X_train_)\n",
    "y_corr = df_.drop('customer_id', axis=1).corr().loc['lapsed_next_period']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    }
   ],
   "source": [
    "model_cols = list(X_train_.columns.values)\n",
    "print(len(model_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.813546423135\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression for feature selection\n",
    "log_test = LogisticRegression()\n",
    "log_test = log_test.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "score_log = log_test.score(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "print(score_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>designer_id_199001</td>\n",
       "      <td>[-0.042443852264]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>var6_0.0</td>\n",
       "      <td>[-0.0313401808797]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>quote_spend_returned_198903</td>\n",
       "      <td>[-0.0285207893645]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>ns_per_order_199002</td>\n",
       "      <td>[-0.00614289267061]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>quote_var1_198904</td>\n",
       "      <td>[-0.00226383272152]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>quote_var1_199001</td>\n",
       "      <td>[-0.00211981921744]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>product_type_id_199003</td>\n",
       "      <td>[-0.00209852694281]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>net_spend_199004</td>\n",
       "      <td>[-1.53540004965e-05]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>var4</td>\n",
       "      <td>[0.00385901597282]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>quote_var1_199003</td>\n",
       "      <td>[0.0049926329919]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>quote_var2_199001</td>\n",
       "      <td>[0.00604733461879]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>gs_per_order_199003</td>\n",
       "      <td>[0.00871000780971]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>quote_var1_199002</td>\n",
       "      <td>[0.0149255689482]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>quote_var2_198903</td>\n",
       "      <td>[0.0261875535153]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>order_id_199003</td>\n",
       "      <td>[0.0307850440802]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>ir_per_order_199001</td>\n",
       "      <td>[0.0307995204345]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0                     1\n",
       "21            designer_id_199001     [-0.042443852264]\n",
       "109                     var6_0.0    [-0.0313401808797]\n",
       "49   quote_spend_returned_198903    [-0.0285207893645]\n",
       "73           ns_per_order_199002   [-0.00614289267061]\n",
       "56             quote_var1_198904   [-0.00226383272152]\n",
       "57             quote_var1_199001   [-0.00211981921744]\n",
       "17        product_type_id_199003   [-0.00209852694281]\n",
       "36              net_spend_199004  [-1.53540004965e-05]\n",
       "0                           var4    [0.00385901597282]\n",
       "59             quote_var1_199003     [0.0049926329919]\n",
       "63             quote_var2_199001    [0.00604733461879]\n",
       "80           gs_per_order_199003    [0.00871000780971]\n",
       "58             quote_var1_199002     [0.0149255689482]\n",
       "61             quote_var2_198903     [0.0261875535153]\n",
       "5                order_id_199003     [0.0307850440802]\n",
       "100          ir_per_order_199001     [0.0307995204345]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_features = pd.DataFrame(list(zip(X_train_[model_cols].columns, np.transpose(log_test.coef_))))\n",
    "log_features[(log_features[1] >= -0.05) & (log_features[1] <= 0.05)].sort_values(1, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['var4', 'order_id_199003', 'product_type_id_199003', 'designer_id_199001', 'net_spend_199004', 'quote_spend_returned_198903', 'quote_var1_198904', 'quote_var1_199001', 'quote_var1_199002', 'quote_var1_199003', 'quote_var2_198903', 'quote_var2_199001', 'ns_per_order_199002', 'gs_per_order_199003', 'ir_per_order_199001']\n"
     ]
    }
   ],
   "source": [
    "weak_predictors = list(log_features[(log_features[1] >= -0.05) & (log_features[1] <= 0.05)][0])\n",
    "weak_predictors.remove('var6_0.0') # I can't remove this without the other categories of the variable\n",
    "print(weak_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "model_cols2 = [x for x in model_cols if x not in weak_predictors]\n",
    "print(len(model_cols2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815068493151\n"
     ]
    }
   ],
   "source": [
    "# REPEAT Logistic Regression for feature selection\n",
    "log_test = LogisticRegression()\n",
    "log_test = log_test.fit(X_train_[model_cols2], y_train_['lapsed_next_period'])\n",
    "score_log = log_test.score(X_train_[model_cols2], y_train_['lapsed_next_period'])\n",
    "print(score_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>var6_0.0</td>\n",
       "      <td>[-0.039583838879]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>gs_per_order_198903</td>\n",
       "      <td>[0.0473107561929]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0                  1\n",
       "94             var6_0.0  [-0.039583838879]\n",
       "72  gs_per_order_198903  [0.0473107561929]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_features2 = pd.DataFrame(list(zip(X_train_[model_cols2].columns, np.transpose(log_test.coef_))))\n",
    "log_features2[(log_features2[1] >= -0.05) & (log_features2[1] <= 0.05)].sort_values(1, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28384835935\n",
      "0.744823192099\n"
     ]
    }
   ],
   "source": [
    "# Random Forest example\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 200,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 4,\n",
    "    'min_samples_leaf': 4,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(**rf_params)\n",
    "rf.fit(X_train_[model_cols2], y_train_['lapsed_next_period'])\n",
    "\n",
    "rf_pred = rf.predict(X_test[model_cols2])\n",
    "\n",
    "print(rf_pred.sum()/len(rf_pred))\n",
    "\n",
    "print(accuracy_score(y_test['lapsed_next_period'], rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77439024390243905"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf = confusion_matrix(y_test['lapsed_next_period'], rf_pred)\n",
    "cf[1,1]/cf[1,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def right_classification(y_true, y_pred):\n",
    "    cf = confusion_matrix(y_true, y_pred)\n",
    "    return cf[1, 1] / cf[1, :].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting multiple hyperparameters for every classifier we are going to implement\n",
    "\n",
    "# Random Forest\n",
    "rf_params_gs = {\n",
    "    'n_jobs': [-1],\n",
    "    'n_estimators': [100, 125, 150, 175, 200], #, 250, 300, 350, 400, 500],\n",
    "    'warm_start': [True], \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'min_samples_leaf': [4, 5, 6], #[2, 3]\n",
    "    'max_features' : ['sqrt'],\n",
    "    'verbose': [0]\n",
    "}\n",
    "\n",
    "# Extra Trees\n",
    "et_params_gs = {\n",
    "    'n_jobs': [-1],\n",
    "    'n_estimators': [600, 650, 700, 800],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'min_samples_leaf': [2, 3, 4],\n",
    "    'verbose': [0]\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params_gs = {\n",
    "    'n_estimators': [100, 125, 150, 175, 200],\n",
    "    'learning_rate': [0.03, 0.05, 0.07]\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params_gs = {\n",
    "    'n_estimators': [100, 125, 150, 175], # 200, 250, 300],\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'min_samples_leaf': [4, 5, 6],\n",
    "    'verbose': [0]\n",
    "}\n",
    "\n",
    "# SVC parameters\n",
    "svc_params_gs = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [1e-5, 5e-5, 1e-4, 5e-4], #1e-3, 0.01, 0.05]\n",
    "}\n",
    "\n",
    "# Logistic regression parameters\n",
    "log_params_gs = {\n",
    "    'solver': ['liblinear'],\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'verbose': [0]\n",
    "}\n",
    "\n",
    "# XGBoosting parameters\n",
    "xgb_params_gs = {\n",
    "    'objective':['binary:logistic'],\n",
    "    'learning_rate': [5e-4, 1e-3, 2e-3], #0.01], #so called `eta` value\n",
    "    'max_depth': [2, 3, 4], #5, 6],\n",
    "    'min_child_weight': [11],\n",
    "    'silent': [1],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.7],\n",
    "    'n_estimators': [300, 400, 500], #number of trees, change it to 1000 for better results\n",
    "    'missing':[-999]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoring = {'right_class': make_scorer(right_classification)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Istantiate the classifiers\n",
    "rf = GridSearchCV(RandomForestClassifier(), rf_params_gs, cv=5, scoring=scoring, refit='right_class')\n",
    "et = GridSearchCV(ExtraTreesClassifier(), et_params_gs, cv=5, scoring=scoring, refit='right_class')\n",
    "ada = GridSearchCV(AdaBoostClassifier(), ada_params_gs, cv=5, scoring=scoring, refit='right_class')\n",
    "gb = GridSearchCV(GradientBoostingClassifier(), gb_params_gs, cv=5, scoring=scoring, refit='right_class')\n",
    "svc = GridSearchCV(SVC(), svc_params_gs, cv=5, scoring=scoring, refit='right_class')\n",
    "log = GridSearchCV(LogisticRegression(), log_params_gs, cv=5, scoring=scoring, refit='right_class')\n",
    "xgbm = GridSearchCV(xgb.XGBClassifier(), xgb_params_gs, cv=5, scoring=scoring, refit='right_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_runner(clf, Xtr, ytr, Xte, yte):\n",
    "    print('-'*40)\n",
    "    print(clf.estimator)\n",
    "    print('-'*40)\n",
    "    clf.fit(Xtr, ytr)\n",
    "    clf.best_params_\n",
    "    print(clf.best_params_)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "#     means = clf.cv_results_['mean_test_score']\n",
    "#     stds = clf.cv_results_['std_test_score']\n",
    "#     for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "#         print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std*2, params))\n",
    "        \n",
    "    print()\n",
    "    y_true, y_pred = yte, clf.predict(Xte)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(accuracy_score(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(right_classification(y_true, y_pred))\n",
    "    \n",
    "    return clf, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_cols = model_cols2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------------------------------------\n",
      "{'max_depth': 5, 'min_samples_leaf': 5, 'warm_start': True, 'n_jobs': -1, 'n_estimators': 125, 'max_features': 'sqrt', 'verbose': 0}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.74      0.85      2975\n",
      "          1       0.14      0.77      0.24       164\n",
      "\n",
      "avg / total       0.94      0.75      0.82      3139\n",
      "\n",
      "0.745778910481\n",
      "[[2214  761]\n",
      " [  37  127]]\n",
      "0.774390243902\n"
     ]
    }
   ],
   "source": [
    "rf_clf, rf_ypred = classifier_runner(rf, X_train_[model_cols], y_train_['lapsed_next_period'],\n",
    "                                      X_test[model_cols], y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------------------------------------\n",
      "{'max_depth': 3, 'min_samples_leaf': 4, 'n_jobs': -1, 'verbose': 0, 'n_estimators': 600}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.73      0.84      2975\n",
      "          1       0.14      0.78      0.23       164\n",
      "\n",
      "avg / total       0.94      0.73      0.81      3139\n",
      "\n",
      "0.731124561962\n",
      "[[2167  808]\n",
      " [  36  128]]\n",
      "0.780487804878\n"
     ]
    }
   ],
   "source": [
    "et_clf, et_ypred = classifier_runner(et, X_train_[model_cols], y_train_['lapsed_next_period'],\n",
    "                                      X_test[model_cols], y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "----------------------------------------\n",
      "{'learning_rate': 0.05, 'n_estimators': 100}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.74      0.85      2975\n",
      "          1       0.14      0.78      0.24       164\n",
      "\n",
      "avg / total       0.94      0.74      0.81      3139\n",
      "\n",
      "0.744504619306\n",
      "[[2209  766]\n",
      " [  36  128]]\n",
      "0.780487804878\n"
     ]
    }
   ],
   "source": [
    "ada_clf, ada_ypred = classifier_runner(ada, X_train_[model_cols], y_train_['lapsed_next_period'],\n",
    "                                        X_test[model_cols], y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------------------------------------\n",
      "{'max_depth': 2, 'min_samples_leaf': 6, 'verbose': 0, 'n_estimators': 100}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.73      0.84      2975\n",
      "          1       0.14      0.79      0.24       164\n",
      "\n",
      "avg / total       0.94      0.73      0.81      3139\n",
      "\n",
      "0.733035998726\n",
      "[[2171  804]\n",
      " [  34  130]]\n",
      "0.792682926829\n"
     ]
    }
   ],
   "source": [
    "gb_clf, gb_ypred = classifier_runner(gb, X_train_, y_train_['lapsed_next_period'], X_test, y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "----------------------------------------\n",
      "{'kernel': 'linear', 'C': 1e-05}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.26      0.41      2975\n",
      "          1       0.07      0.96      0.12       164\n",
      "\n",
      "avg / total       0.94      0.29      0.39      3139\n",
      "\n",
      "0.294042688754\n",
      "[[ 765 2210]\n",
      " [   6  158]]\n",
      "0.963414634146\n"
     ]
    }
   ],
   "source": [
    "svc_clf, svc_ypred = classifier_runner(svc, X_train_, y_train_['lapsed_next_period'], X_test, y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc 0.6956658559892\n",
      "et 0.7688241555877999\n"
     ]
    }
   ],
   "source": [
    "print('svc ' + str(0.294042688754 * 0.4 + 0.963414634146 * 0.6))\n",
    "print('et ' + str(0.733035998726 * 0.4 + 0.792682926829 * 0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "----------------------------------------\n",
      "{'solver': 'liblinear', 'max_iter': 100, 'verbose': 0}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.74      0.84      2975\n",
      "          1       0.13      0.72      0.22       164\n",
      "\n",
      "avg / total       0.94      0.74      0.81      3139\n",
      "\n",
      "0.738770309016\n",
      "[[2201  774]\n",
      " [  46  118]]\n",
      "0.719512195122\n"
     ]
    }
   ],
   "source": [
    "log_clf, log_ypred =classifier_runner(log, X_train_[model_cols], y_train_['lapsed_next_period'],\n",
    "                                      X_test[model_cols], y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "----------------------------------------\n",
      "{'max_depth': 2, 'min_child_weight': 11, 'objective': 'binary:logistic', 'silent': 1, 'learning_rate': 0.002, 'n_estimators': 400, 'subsample': 0.8, 'missing': -999, 'colsample_bytree': 0.7}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.73      0.84      2975\n",
      "          1       0.14      0.77      0.23       164\n",
      "\n",
      "avg / total       0.94      0.73      0.81      3139\n",
      "\n",
      "0.733673144313\n",
      "[[2176  799]\n",
      " [  37  127]]\n",
      "0.774390243902\n"
     ]
    }
   ],
   "source": [
    "xgbm_clf, xgbm_ypred = classifier_runner(xgbm, X_train_[model_cols], y_train_['lapsed_next_period'],\n",
    "                                         X_test[model_cols], y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "rf_best = SklearnHelper(clf=RandomForestClassifier, params=rf_clf.best_params_)\n",
    "et_best = SklearnHelper(clf=ExtraTreesClassifier, params=et_clf.best_params_)\n",
    "ada_best = SklearnHelper(clf=AdaBoostClassifier, params=ada_clf.best_params_)\n",
    "gb_best = SklearnHelper(clf=GradientBoostingClassifier, params=gb_clf.best_params_)\n",
    "svc_best = SklearnHelper(clf=SVC, params=svc_clf.best_params_)\n",
    "log_best = SklearnHelper(clf=LogisticRegression, params=log_clf.best_params_)\n",
    "xgbm_best = SklearnHelper(clf=xgb.XGBClassifier, params=xgbm_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_best = rf_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "et_best = et_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "ada_best = ada_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "gb_best = gb_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "svc_best = svc_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "log_best = log_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "xgbm_best = xgbm_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_best_score = right_classification(y_pred=rf_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "et_best_score = right_classification(y_pred=et_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "ada_best_score = right_classification(y_pred=ada_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "gb_best_score = right_classification(y_pred=gb_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "svc_best_score = right_classification(y_pred=svc_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "log_best_score = right_classification(y_pred=log_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "xgbm_best_score = right_classification(y_pred=xgbm_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_best_score_test = right_classification(y_pred=rf_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "et_best_score_test = right_classification(y_pred=et_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "ada_best_score_test = right_classification(y_pred=ada_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "gb_best_score_test = right_classification(y_pred=gb_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "svc_best_score_test = right_classification(y_pred=svc_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "log_best_score_test = right_classification(y_pred=log_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "xgbm_best_score_test = right_classification(y_pred=xgbm_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score on test</th>\n",
       "      <th>Score on training</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.990868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.798780</td>\n",
       "      <td>0.902588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forests</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.875190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.719512</td>\n",
       "      <td>0.859970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extratrees</td>\n",
       "      <td>0.768293</td>\n",
       "      <td>0.844749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.774390</td>\n",
       "      <td>0.843227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoosting</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.838661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Score on test  Score on training\n",
       "4  Support Vector Machine       0.975610           0.990868\n",
       "3       Gradient Boosting       0.798780           0.902588\n",
       "0          Random Forests       0.780488           0.875190\n",
       "5     Logistic Regression       0.719512           0.859970\n",
       "1              Extratrees       0.768293           0.844749\n",
       "6                 XGBoost       0.774390           0.843227\n",
       "2             AdaBoosting       0.780488           0.838661"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Random Forests', 'Extratrees', 'AdaBoosting', \n",
    "              'Gradient Boosting', 'Support Vector Machine', \n",
    "              'Logistic Regression','XGBoost'],\n",
    "    'Score on training': [rf_best_score, et_best_score, ada_best_score, gb_best_score, \n",
    "              svc_best_score, log_best_score, xgbm_best_score],\n",
    "    'Score on test': [rf_best_score_test, et_best_score_test, ada_best_score_test, gb_best_score_test, \n",
    "              svc_best_score_test, log_best_score_test, xgbm_best_score_test]})\n",
    "models.sort_values(by='Score on training', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>RF</th>\n",
       "      <th>Extra</th>\n",
       "      <th>Ada</th>\n",
       "      <th>GB</th>\n",
       "      <th>XGB</th>\n",
       "      <th>Median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>order_id_199004</td>\n",
       "      <td>0.087924</td>\n",
       "      <td>0.056460</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.054518</td>\n",
       "      <td>0.101667</td>\n",
       "      <td>0.087924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>designer_id_199004</td>\n",
       "      <td>0.096264</td>\n",
       "      <td>0.076295</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.052981</td>\n",
       "      <td>0.139167</td>\n",
       "      <td>0.076295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>gs_per_item_199004</td>\n",
       "      <td>0.027530</td>\n",
       "      <td>0.020287</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.061681</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.061681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>product_id_199004</td>\n",
       "      <td>0.057834</td>\n",
       "      <td>0.053766</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.043109</td>\n",
       "      <td>0.061667</td>\n",
       "      <td>0.057834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>gross_spend_199004</td>\n",
       "      <td>0.083987</td>\n",
       "      <td>0.047750</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.025599</td>\n",
       "      <td>0.076667</td>\n",
       "      <td>0.047750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>gs_per_order_199004</td>\n",
       "      <td>0.053241</td>\n",
       "      <td>0.025326</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.017618</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>product_type_id_199002</td>\n",
       "      <td>0.014930</td>\n",
       "      <td>0.032374</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.021781</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>0.032374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>ns_per_order_199004</td>\n",
       "      <td>0.030692</td>\n",
       "      <td>0.017286</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.036748</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.030692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>product_type_id_199004</td>\n",
       "      <td>0.040313</td>\n",
       "      <td>0.080517</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>0.028333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ns_per_ib_199004</td>\n",
       "      <td>0.028253</td>\n",
       "      <td>0.011941</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.056729</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.028253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>quote_spend_returned_199002</td>\n",
       "      <td>0.006347</td>\n",
       "      <td>0.037023</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.017616</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.017616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>order_id_199002</td>\n",
       "      <td>0.016978</td>\n",
       "      <td>0.019266</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.006737</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.016978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>gross_spend_199003</td>\n",
       "      <td>0.012721</td>\n",
       "      <td>0.014881</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.005529</td>\n",
       "      <td>0.038333</td>\n",
       "      <td>0.014881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>gs_per_order_199002</td>\n",
       "      <td>0.014241</td>\n",
       "      <td>0.008611</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.003211</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.014241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>product_id_199002</td>\n",
       "      <td>0.013395</td>\n",
       "      <td>0.028343</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025833</td>\n",
       "      <td>0.013395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>designer_id_199002</td>\n",
       "      <td>0.018687</td>\n",
       "      <td>0.033023</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.011667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>gross_spend_199002</td>\n",
       "      <td>0.011557</td>\n",
       "      <td>0.020883</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.029167</td>\n",
       "      <td>0.011557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>net_spend_199002</td>\n",
       "      <td>0.009445</td>\n",
       "      <td>0.010183</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.015865</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>product_id_199003</td>\n",
       "      <td>0.011525</td>\n",
       "      <td>0.017498</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.004663</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>gs_per_item_199003</td>\n",
       "      <td>0.009831</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.031385</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.009831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>gs_per_item_199002</td>\n",
       "      <td>0.010488</td>\n",
       "      <td>0.005255</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.016456</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.008333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>ns_per_order_199003</td>\n",
       "      <td>0.008106</td>\n",
       "      <td>0.003131</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.017044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>quote_spend_returned_199003</td>\n",
       "      <td>0.006825</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.014361</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>ns_per_ib_199003</td>\n",
       "      <td>0.007499</td>\n",
       "      <td>0.002839</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.010099</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.007499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>ns_per_order_198904</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.070640</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.005495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>ib_per_order_199003</td>\n",
       "      <td>0.007496</td>\n",
       "      <td>0.004470</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.030388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>ib_per_order_199002</td>\n",
       "      <td>0.005868</td>\n",
       "      <td>0.004458</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.023124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>item_bought_199002</td>\n",
       "      <td>0.015621</td>\n",
       "      <td>0.020696</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.004394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>quote_var2_199004</td>\n",
       "      <td>0.004174</td>\n",
       "      <td>0.008976</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.010382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>ir_per_order_199002</td>\n",
       "      <td>0.010318</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.006853</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>quote_var1_198903</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.011298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>var6_1.0</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.008455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>ns_per_ib_198904</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>var3_0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>var3_1.0</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>var3_2.0</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>var3_3.0</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>var6_0.0</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>gs_per_order_199001</td>\n",
       "      <td>0.003570</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>order_id_198903</td>\n",
       "      <td>0.001225</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>ir_per_order_199003</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>ir_per_order_199004</td>\n",
       "      <td>0.014542</td>\n",
       "      <td>0.008166</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>product_id_199001</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>0.002905</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>product_type_id_198903</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>product_type_id_198904</td>\n",
       "      <td>0.003485</td>\n",
       "      <td>0.003404</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>product_type_id_199001</td>\n",
       "      <td>0.002715</td>\n",
       "      <td>0.006333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>designer_id_198903</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>designer_id_198904</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>net_spend_198903</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>item_bought_198904</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>item_bought_199001</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>item_bought_199003</td>\n",
       "      <td>0.008998</td>\n",
       "      <td>0.011403</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>item_bought_199004</td>\n",
       "      <td>0.031108</td>\n",
       "      <td>0.058847</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>item_returned_198903</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>item_returned_198904</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>item_returned_199004</td>\n",
       "      <td>0.010573</td>\n",
       "      <td>0.032759</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>quote_var2_198904</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>quote_var2_199002</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>quote_var2_199003</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>quote_spend_returned_199004</td>\n",
       "      <td>0.019287</td>\n",
       "      <td>0.012294</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Feature        RF     Extra   Ada        GB       XGB  \\\n",
       "4               order_id_199004  0.087924  0.056460  0.21  0.054518  0.101667   \n",
       "20           designer_id_199004  0.096264  0.076295  0.05  0.052981  0.139167   \n",
       "60           gs_per_item_199004  0.027530  0.020287  0.09  0.061681  0.082500   \n",
       "10            product_id_199004  0.057834  0.053766  0.07  0.043109  0.061667   \n",
       "26           gross_spend_199004  0.083987  0.047750  0.01  0.025599  0.076667   \n",
       "56          gs_per_order_199004  0.053241  0.025326  0.04  0.017618  0.087500   \n",
       "14       product_type_id_199002  0.014930  0.032374  0.08  0.021781  0.093333   \n",
       "55          ns_per_order_199004  0.030692  0.017286  0.01  0.036748  0.032500   \n",
       "15       product_type_id_199004  0.040313  0.080517  0.00  0.000000  0.028333   \n",
       "59             ns_per_ib_199004  0.028253  0.011941  0.03  0.056729  0.001667   \n",
       "46  quote_spend_returned_199002  0.006347  0.037023  0.06  0.017616  0.008333   \n",
       "3               order_id_199002  0.016978  0.019266  0.01  0.006737  0.018333   \n",
       "25           gross_spend_199003  0.012721  0.014881  0.02  0.005529  0.038333   \n",
       "61          gs_per_order_199002  0.014241  0.008611  0.02  0.003211  0.018333   \n",
       "8             product_id_199002  0.013395  0.028343  0.00  0.000000  0.025833   \n",
       "18           designer_id_199002  0.018687  0.033023  0.00  0.002688  0.011667   \n",
       "24           gross_spend_199002  0.011557  0.020883  0.00  0.002099  0.029167   \n",
       "30             net_spend_199002  0.009445  0.010183  0.03  0.015865  0.010000   \n",
       "9             product_id_199003  0.011525  0.017498  0.01  0.004663  0.004167   \n",
       "70           gs_per_item_199003  0.009831  0.002757  0.00  0.031385  0.052500   \n",
       "65           gs_per_item_199002  0.010488  0.005255  0.00  0.016456  0.008333   \n",
       "66          ns_per_order_199003  0.008106  0.003131  0.03  0.017044  0.000000   \n",
       "47  quote_spend_returned_199003  0.006825  0.004194  0.08  0.014361  0.007500   \n",
       "69             ns_per_ib_199003  0.007499  0.002839  0.02  0.010099  0.005000   \n",
       "77          ns_per_order_198904  0.005495  0.000748  0.06  0.070640  0.002500   \n",
       "67          ib_per_order_199003  0.007496  0.004470  0.00  0.030388  0.000000   \n",
       "62          ib_per_order_199002  0.005868  0.004458  0.00  0.023124  0.000000   \n",
       "35           item_bought_199002  0.015621  0.020696  0.00  0.004394  0.000833   \n",
       "54            quote_var2_199004  0.004174  0.008976  0.00  0.010382  0.000000   \n",
       "63          ir_per_order_199002  0.010318  0.003572  0.00  0.006853  0.004167   \n",
       "..                          ...       ...       ...   ...       ...       ...   \n",
       "49            quote_var1_198903  0.000115  0.000026  0.00  0.011298  0.000000   \n",
       "95                     var6_1.0  0.000290  0.000024  0.00  0.008455  0.000000   \n",
       "81             ns_per_ib_198904  0.002653  0.000293  0.00  0.000000  0.000000   \n",
       "88                     var3_0.0  0.000000  0.000010  0.00  0.000000  0.000000   \n",
       "89                     var3_1.0  0.000233  0.000061  0.00  0.000000  0.000000   \n",
       "90                     var3_2.0  0.000556  0.000051  0.00  0.000000  0.000000   \n",
       "91                     var3_3.0  0.000331  0.000048  0.00  0.000000  0.000000   \n",
       "94                     var6_0.0  0.000169  0.000104  0.00  0.000000  0.000000   \n",
       "84          gs_per_order_199001  0.003570  0.000607  0.00  0.000000  0.000000   \n",
       "0               order_id_198903  0.001225  0.002520  0.00  0.000000  0.000000   \n",
       "68          ir_per_order_199003  0.002386  0.003125  0.00  0.000000  0.000000   \n",
       "58          ir_per_order_199004  0.014542  0.008166  0.00  0.000000  0.000000   \n",
       "7             product_id_199001  0.002346  0.002905  0.00  0.000000  0.000000   \n",
       "11       product_type_id_198903  0.002381  0.001287  0.00  0.000000  0.000000   \n",
       "12       product_type_id_198904  0.003485  0.003404  0.00  0.000000  0.000000   \n",
       "13       product_type_id_199001  0.002715  0.006333  0.00  0.000000  0.000000   \n",
       "16           designer_id_198903  0.001928  0.001681  0.00  0.000000  0.000000   \n",
       "17           designer_id_198904  0.001635  0.005989  0.00  0.000000  0.000000   \n",
       "27             net_spend_198903  0.002183  0.001513  0.00  0.000000  0.000000   \n",
       "33           item_bought_198904  0.001876  0.002642  0.00  0.000000  0.000000   \n",
       "34           item_bought_199001  0.001896  0.002393  0.00  0.000000  0.000000   \n",
       "36           item_bought_199003  0.008998  0.011403  0.00  0.000000  0.000000   \n",
       "37           item_bought_199004  0.031108  0.058847  0.00  0.000000  0.000000   \n",
       "38         item_returned_198903  0.000864  0.003949  0.00  0.000000  0.000000   \n",
       "39         item_returned_198904  0.003676  0.003118  0.00  0.000000  0.000000   \n",
       "43         item_returned_199004  0.010573  0.032759  0.00  0.000000  0.000000   \n",
       "51            quote_var2_198904  0.001032  0.001516  0.00  0.000000  0.000000   \n",
       "52            quote_var2_199002  0.001104  0.000050  0.00  0.000000  0.000000   \n",
       "53            quote_var2_199003  0.002221  0.001084  0.00  0.000000  0.000000   \n",
       "48  quote_spend_returned_199004  0.019287  0.012294  0.00  0.000000  0.000000   \n",
       "\n",
       "      Median  \n",
       "4   0.087924  \n",
       "20  0.076295  \n",
       "60  0.061681  \n",
       "10  0.057834  \n",
       "26  0.047750  \n",
       "56  0.040000  \n",
       "14  0.032374  \n",
       "55  0.030692  \n",
       "15  0.028333  \n",
       "59  0.028253  \n",
       "46  0.017616  \n",
       "3   0.016978  \n",
       "25  0.014881  \n",
       "61  0.014241  \n",
       "8   0.013395  \n",
       "18  0.011667  \n",
       "24  0.011557  \n",
       "30  0.010183  \n",
       "9   0.010000  \n",
       "70  0.009831  \n",
       "65  0.008333  \n",
       "66  0.008106  \n",
       "47  0.007500  \n",
       "69  0.007499  \n",
       "77  0.005495  \n",
       "67  0.004470  \n",
       "62  0.004458  \n",
       "35  0.004394  \n",
       "54  0.004174  \n",
       "63  0.004167  \n",
       "..       ...  \n",
       "49  0.000026  \n",
       "95  0.000024  \n",
       "81  0.000000  \n",
       "88  0.000000  \n",
       "89  0.000000  \n",
       "90  0.000000  \n",
       "91  0.000000  \n",
       "94  0.000000  \n",
       "84  0.000000  \n",
       "0   0.000000  \n",
       "68  0.000000  \n",
       "58  0.000000  \n",
       "7   0.000000  \n",
       "11  0.000000  \n",
       "12  0.000000  \n",
       "13  0.000000  \n",
       "16  0.000000  \n",
       "17  0.000000  \n",
       "27  0.000000  \n",
       "33  0.000000  \n",
       "34  0.000000  \n",
       "36  0.000000  \n",
       "37  0.000000  \n",
       "38  0.000000  \n",
       "39  0.000000  \n",
       "43  0.000000  \n",
       "51  0.000000  \n",
       "52  0.000000  \n",
       "53  0.000000  \n",
       "48  0.000000  \n",
       "\n",
       "[96 rows x 7 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = pd.DataFrame(list(zip(X_train_[model_cols].columns,\n",
    "    np.transpose(rf_best.feature_importances_),\n",
    "    np.transpose(et_best.feature_importances_),\n",
    "    np.transpose(ada_best.feature_importances_),\n",
    "    np.transpose(gb_best.feature_importances_),\n",
    "    np.transpose(xgbm_best.feature_importances_),\n",
    "    )), columns=['Feature','RF','Extra','Ada','GB','XGB'])\n",
    "  \n",
    "summary['Median'] = summary.median(1)\n",
    "summary.sort_values('Median', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuEAAAKVCAYAAABh8zVFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xl8VOXZ//HPNdkhIWEJBBIUDHsQUERRS8EqoCib66PW\nVtFaRehjrdaNum9VSx+1RX/WBa1WUeuKyCIKWmQRwi47ohICYUkggYRkZu7fHzOGDKvaYWaSfN+v\nV17M3Oc651zncCa55zr3Ocecc4iIiIiISOR4op2AiIiIiEh9o064iIiIiEiEqRMuIiIiIhJh6oSL\niIiIiESYOuEiIiIiIhGmTriIiIiISISpEy4iIiIicghm9oKZFZnZskNMNzN70szWmtkSMzvxhyxX\nnXARERERkUMbD5x9mOnnAO2DP9cCT/+QhaoTLiIiIiJyCM65z4AdhwkZCrzsAuYAGWbW8kjLjQ9X\ngiIiIiIi4VC1bX3EHumemJn7WwIV7O8965x79kcsIhv4rsb7jcG2wsPNpE64iIiIiNRbwQ73j+l0\nh4WGo4iIiIiI/HQFQOsa73OCbYelSriIiIiIxBa/L9oZ/BjvA6PM7HXgFGCnc+6wQ1FAnfBaLZLj\npeoC73vjop1CreMq90Y7hVrFbT3cdTtyMJ5jWx85SKqlX/tKtFOodXZ+cEe0U6h1UgaMtGjnEEvM\n7DWgH9DMzDYCdwMJAM65Z4BJwCBgLbAHuOqHLFedcBERERGJLc4f7QyqOecuPcJ0B9zwY5erMeEi\nIiIiIhGmSriIiIiIxBZ/7FTCjxZVwkVEREREIkyVcBERERGJKS6GxoQfLaqEi4iIiIhEmDrhIiIi\nIiIRpuEoIiIiIhJbdGGmiIiIiIiEmyrhIiIiIhJbdGGmiIiIiIiEmyrhIiIiIhJb/L5oZ3DUqRIu\nIiIiIhJhqoSLiIiISGzRmHAREREREQk3VcJFREREJLboPuEiIiIiIhJuqoSLiIiISExxGhMuIiIi\nIiLhpkq4iIiIiMQWjQkXEREREZFwUyVcRERERGKLxoSLiIiIiEi4qRMuIiIiIhJhGo4iIiIiIrHF\n74t2BkedKuEiIiIiIhGmSriIiIiIxBZdmCkiIiIiIuGmSriIiIiIxBY9rEdERERERMJNlXARERER\niS0aEy4iIiIiIuGmSriIiIiIxJZ6MCZcnXAJmzEPjeWzWfNo0jiDd195JtrpxIRZ64t4dPoy/M4x\nvNsxjOjdPmR64a49/OnDRZTurcLvHL/7eWf65Lagyufn3smLWbllJz6/47yuOVy937x11awN23hs\n5kr8fsewrjmM6NU2ZHrhrnLumrqM0r1e/M4x+vT29GmbyaSVhbw0f0N13Jptpbx2WW86Nm8U4S2I\nrLjcbiQOvAI8HrwLZ1A164OQ6daoKUnDroOkBpjHQ+X01/GtXRwyPWXko1TO/Dfe2ZMinX5UzFq3\nhUenLQl8Lrsfy4jTOoZML9y5hz99sCDwufQ7fndGHn3aZVHl83P/Rwv5qrAEj8Et/bvR69jMKG1F\n5AwY0I+xf7kXT1wcL77wGo89/veDxg0fNogJE56l96mDyM9fwpln9uHBB24nMTGRyspKbrv9AWbM\n+CLC2UfHrK828Oi/Z+L3O4afmseIAb1Cpm/asYt7Xp1GcVk5jRok89CvBtKicRorN27loQmfUFZR\nSZzHuGbAyQzs2SFKWyFHW73qhJvZMOAdoLNzbuVBpo8HJjrn3jrMMsYDfYGdQDLwmnPu3jDnuNo5\n91W4lhkpwwb157ILhnDH/Y9HO5WY4PM7Hv54Kc9c3JsWaSlc/vLn9G2XRW6ztOqYf3yxhgGdWnHx\nCW1Yt62UUW/N5aPcFkxbtYkqn5+3RvSjvMrL+c/P4OzO2WSnN4jeBkWAz+945NMVPH1+T1qkJnP5\na3Poe1wmuU1Tq2Oem7ee/u2zuLh7a9ZtL2P0u/n0uTqTQZ1aMqhTSyDQAb/pg0V1vgOOGYnnXEnF\nKw/jdu0g+Zr78a7Kx20rqA5J6DMM7/I5eBdMx5plk3zZLZQ/eWP19MQBvwzplNd1Pr/j4SmLeebS\n02nRKIXLX/yUvu1bkpu571j5x6xVDOiczcU9j2Pd1l2MemM2H7XL4t8LNwDw1m/OZMfuvdww4Qte\nvaofHrMobc3R5/F4eOKJBxg06DI2bixk9hcfMnHiVFasXBMSl5rakFGjRjB3bn512/ZtOxh+/lUU\nFm4hr0tHJk58lbbHnRTpTYg4n9/Pw2/O4JkbhtMiI5XLH3udvscfR27LptUxY9/5nPNO7syQU7ow\nb9V3PPnBFzz4q4GkJMZz/xUDOLZ5Y4p2lnHZo69xaudjadQgKYpbFB3O6YmZdc2lwH+C//43bnHO\n9QB6AL82s7ZHmuFHGAZ0CePyIuakHseT3ijtyIH1xLLCYlpnNCQnoyEJcR4Gdm7FjLWbQ2LMYHel\nF4CyvVVkpiYH2jHKq3x4/X72ev0kxHlITaz735mXbd5J6/QG5KQ3COyzDlnMWFcUEmNYjX3mJTP1\nwD9Ok1dtZmCHrIjkHE2e7Fz8xVtwJVvB78O3fA7xHXvuF+WwpBQALDkFV1pcPSWuY0/8JUX4t26M\nYNbRtWzTDlo3bkhO4+DnsksOM9YUhsQYB/9crt+2i5ODle8mDZNIS0pgeWExdVmvXj1Yt24DX3/9\nLVVVVbzxxnsMHjzggLh77rmFx/8yjoqKvdVtixYvp7BwCwDLv1pFSkoyiYmJEcs9WpZ9s4XWzdLJ\naZZOQnwcA3t2YMbS9SEx6zfv4OQOrQHo1SGnevqxzRtzbPPGADRPT6VJagOKy/ZEdgMkYupNJ9zM\nUoGfAVcD/xNsMzP7m5mtMrOPgeY14u8ysy/NbJmZPWt20FJHcvDf3cF5zjSzhWa21MxeMLOkI7Q/\nYmZfmdkSM3vczE4DhgCPmdkiM8s9WvtDjr6isgqy0lKq37dIS6aotCIk5rrTO/Lh8o0MGDeNUW/N\n47azugJwVseWpCTE0f/v0zj7mY/5Va9c0lPq/h+vot0VtEhLrn7fIi2Zrbv3hsT89tRcJq0sZOBz\nMxn9Xj639ut8wHKmrt7M2R3rfifc0prgdm6vfu927cDSGofEVM18m/jjf0bKjU+RfOkfqZz8UmBC\nQhIJpw+maubbkUw56opKK8hqVPNzmXLg5/Lnnflw2XcMeOojRr0xm9sGdAOgQ4t0ZqwpxOv3U1Cy\nm682l7BlV3lE84+07FYt2fjdvi8pBQWbaZXdMiSmR4+utM5pxUcffXLI5Zw//FwWLlpKZWXlUcs1\nVhSVlJHVeF9BqkVGKkUlZSExHbKbMX3xWgA+WbyO3RWVlOwOPZaWbthMlc9H62YZRz/pWOT8kfuJ\nknrTCQeGApOdc6uB7WbWExgOdCRQef4VcFqN+L8553o557oCKcB5NaY9ZmaLgI3A6865IjNLBsYD\nlzjnjicw1Of6w7Q3Da4/zznXDXjAOfcF8D7BSrtzbt3R2RUSKyavKGBI19ZMHdmfv114MmM+XIjf\nOZYVluAxY+rI/ky69kz++eU6Npbsjna6MWHyqkIGd2nFlGv68tTQExkzZSl+56qnLy0sITk+jnbN\ndFYGIK7rqVQt/ozy/xtNxWuPkjRsJGAk9ruAqjkfQdXeIy6jvpm8/DuGdDuGqaPP4W8Xn8qY9xfg\nd45h3Y+lRVoKl70wg8emLaF7TpM6PRTlhzAzHnv0bv54632HjOnSuQMPPnQ7N9xwWwQzi203De/D\ngjUFXPLnfzF/bQHNM1Lx2L4u2daduxnzzynce3l/PJ76fYzVZfWpE34p8Hrw9evB9z8nMKbb55zb\nBNT8Gn+Gmc01s6XAL4C8GtO+H46SBZwZrGB3BL4OdvIBXgou/1DtO4EK4HkzOx/4QeebzOxaM5tv\nZvOfe/m1H7P9EmHNU5PZXLqvsrGltILmNaq8AO8s+ZYBnVoB0D27CXu9fkr2VPLRigJOPy6ThDgP\nTRom0SOnCcs374xo/tHQvGEyW2pUJbeUVpDZMHS4ybvLChgQHGrSvVUGlV4/JeX7qmtT6kkVHMCV\n7sDS940ztUZNQoabACT06IfvqzkA+DeuhfgEaJCGJzuXxLMuJeV3/0fCKWeT+LOhxPfqH9H8o6F5\nWjKbd9X8XJYf+Llc/A0DOmcD0D2nKXt9Pkr2VBLv8XBL/268cc0v+L+LTqW0oopjm6RSlxVsKiSn\n9b7Kd3Z2FpsK9lXG09JSycvryLSpb7J61WxOOeUE3v73C5x4YrdgfEvefPM5Roy4kfXrv4l4/tHQ\nPCOVzcWl1e+3lJTRPCP0OGmensrY35zHhFsvY/TgUwGqx32Xle9l9DPvMeq80+jWNvSsQ73i90fu\nJ0rqRSfczJoQ6Eg/Z2YbgFuAiwkM/TtYfDIwDrgwWL3+B/uGnlRzzpUBMwgMc/lRnHNe4GTgLQJV\n9sk/cL5nnXMnOedOuuZX/+3Qdjma8lpm8G3xbgpK9lDl8zNlxSb6tgvtHLZslMLcb7YBsH57KZVe\nH40bJNKyUQrzvgkMMyiv9LJ0UzFt6/gfe4C8rEZ8W7KHgp3BfbZ6M/1ym4fEZKUlM+/bwL5Zv6OM\nvT4/jYNDdfzOMXX1FgbWk064v2A9niZZWEYmeOKIy+uNd/WC0Jhd24lrGxjmZM1aBTrhe3ZRMf5+\nyp+8kfInb6Rq7mQq//Me3i+nRWMzIiqvVWO+LS6joGR34Bj7aiN924d2dFo2asDcDVuBwDjwSq+f\nxg0SKa/yUh4cKz776yLiPRZyQWddNH/+Ytq1a0ubNq1JSEjg4ouHMnHivuNk165SWmV3o0PHU+nQ\n8VTmzl3I+ReMID9/CenpjXjv3Ze4886HmT17fhS3IrLyjmnBt1tLKNi2kyqvjykLVtP3+ONCYorL\nyvH7A2fwnp86n2G9A5eCVXl93PTcRM47uTP9T6gfd8Sqz+r+lV4BFwL/dM799vsGM5sJbAcuMbOX\nCIwHPwP4F/s63NuCY8kvJNBZDmFm8cApwFPAKqCNmbVzzq0FrgBmHqo9uNwGzrlJZjYL+P6qjVKg\nVp5Hv+XuR/hy4RJKSnZx5rBfMvLqK7hg8MBopxU18R4Pt53VlevfnIPfOYYe35p2zdIY9/lKumRl\n0K99Fjedkcd9Uxbz6vz1YHDvoB6YGZec0Ia7PlrE+c9/CsCQrq3pUNfv9EFgn916RidGvpMf2Gd5\n2eQ2TWXc7LV0ad6IfrnNuennHbn/4694ZeE3GMZ9A/L4/pKN/I3FZKUlk1PH7yJTzfmp/Gg8yZff\nCubBu2gmbmsBCf0uwL/pa3yr86mc+ipJg68h/pSzAah87/9FOenoivd4uG1Ad65/fRZ+Pwztfizt\nMhsxbuZXdGnZmH4dWnLTmV2576OFvDpvLWDce96JmBk7du9l5Otf4DFonpbCA0PqwZ0+fD5uvPFP\nfDjxVTxxHl4aP4GvVqzm7rtuZkH+4pAO+f5GXn8lubltuPPOG7nzzsAdeQadexlbt24/5Dx1QXyc\nh9su6sf1494N/B7r3YV2LZsy7sPZdDmmBf2OP475azby5AezMIye7bK5/aJ+AExduIb8tZso2V3B\n+3MDN0m775cD6JRT92+FWR+ZqzGWsq4ys0+BPzvnJtdo+x3QGfAB/YFvgSrgBefcW2b2AIEhK5uB\n1cA3zrl79rtFYSIwHfidc86Z2ZnA4wS+3HwJXO+c23uwdqAJ8B6BDr8BjzvnXjKz0wlU3vcSqMQf\nclx41bb1df8/L4y8742Ldgq1jqvUeOEfw23dEe0Uah3Psa2jnUKtkn7tK9FOodbZ+cEd0U6h1kkZ\nMDLqA9ErFrwbsT5Ocs9hUdneelEJd86dcZC2J48wzxhgzEHarzzMPNOBE35geyGB4Sj7x86ilt6i\nUERERER+mHrRCRcRERGRWsSvh/WIiIiIiEiYqRIuIiIiIrElig/RiRRVwkVEREREIkyVcBERERGJ\nLVF8iE6kqBIuIiIiIhJhqoSLiIiISGzRmHAREREREQk3VcJFREREJLZoTLiIiIiIiISbKuEiIiIi\nEltUCRcRERERkXBTJVxEREREYopzvmincNSpEi4iIiIiEmGqhIuIiIhIbNGYcBERERERCTd1wkVE\nREREIkzDUUREREQktuix9SIiIiIiEm6qhIuIiIhIbNGFmSIiIiIiEm6qhIuIiIhIbNGYcBERERER\nCTdVwkVEREQktmhMuIiIiIiIhJsq4SIiIiISWzQmXEREREREwk2VcBERERGJLRoTLiIiIiIi4aZK\nuIiIiIjEFlXCRUREREQk3FQJFxEREZHYUg/ujqJOeC3mfW9ctFOoVeKHjox2CrVO1Zt/jXYKtUtV\nVbQzqH2OaRftDGoV51y0U6h1/HNmRTuF2meA/l5GgoajiIiIiIhEmCrhIiIiIhJbdGGmiIiIiIiE\nmyrhIiIiIhJb6sGFmaqEi4iIiIhEmCrhIiIiIhJbNCZcRERERETCTZVwEREREYktGhMuIiIiIlK/\nmdnZZrbKzNaa2W0HmX6smU03syVmNsPMco60THXCRURERCS2+P2R+zkCM4sD/g6cA3QBLjWzLvuF\nPQ687JzrBtwHPHyk5aoTLiIiIiJyaCcDa51z651zlcDrwND9YroAnwRff3qQ6QdQJ1xEREREYksM\nVcKBbOC7Gu83BttqWgycH3w9HEgzs6aHW6g64SIiIiJSb5nZtWY2v8bPtT9hMTcDfc1sIdAXKAB8\nh5tBd0cRERERkdjiXARX5Z4Fnj1MSAHQusb7nGBbzWVsIlgJN7NU4ALnXMnh1qtKuIiIiIjIoX0J\ntDeztmaWCPwP8H7NADNrZmbf96tvB1440kJVCRcRERGR2BJDT8x0znnNbBQwBYgDXnDOLTez+4D5\nzrn3gX7Aw2bmgM+AG460XHXCRUREREQOwzk3CZi0X9tdNV6/Bbz1Y5ap4SgiIiIiIhGmSriIiIiI\nxJYYGo5ytKgSLiIiIiISYaqEi4iIiEhscaqEi4iIiIhImKkSLiIiIiKxRWPCRUREREQk3FQJFxER\nEZHYEsHH1keLKuEiIiIiIhGmSriIiIiIxBaNCRcRERERkXBTJVxEREREYosq4SIiIiIiEm6qhIuI\niIhIbNETM0VEREREJNxUCRcRERGRmOL8uk+4iIiIiIiEmSrh8qPMWl/Eo9OX4XeO4d2OYUTv9iHT\nC3ft4U8fLqJ0bxV+5/jdzzvTJ7cFVT4/905ezMotO/H5Hed1zeHq/eatj8Y8NJbPZs2jSeMM3n3l\nmWinExNmbdjGYzNX4vc7hnXNYUSvtiHTC3eVc9fUZZTu9eJ3jtGnt6dP20wmrSzkpfkbquPWbCvl\ntct607F5owhvQWTFtetO4rlXgXnwLphO1efvhUy39KYknX8DpDTEzEPl1H/hW7MwZHrK6L9S+emb\neGd9EOn0o2LW8q959I3p+P2O4ad3Y8TZp4RM37R9J/e8PJnisj00apDCQyPOpUXjNDZt38lNz7yL\n3zm8Pj+XnnEiF/28R5S2InIGDOjH2LH3Eefx8MKLr/HYY38/aNzw4YN4Y8I/6N37HBbkL6FJk8ZM\neP1ZTjqpOy+//Ab/e+OYCGcePXG53UgceAV4PHgXzqBqv8+WNWpK0rDrIKkB5vFQOf11fGsXY+nN\nSBn5GP7thQD4N66lctIL0dgEiYA61Qk3Mx+wtEbT6865Rw4Tf4dz7qEfuY53gLZAKpAJfB2cNNI5\n98WPTLlW8fkdD3+8lGcu7k2LtBQuf/lz+rbLIrdZWnXMP75Yw4BOrbj4hDas21bKqLfm8lFuC6at\n2kSVz89bI/pRXuXl/OdncHbnbLLTG0Rvg2LAsEH9ueyCIdxx/+PRTiUm+PyORz5dwdPn96RFajKX\nvzaHvsdlkts0tTrmuXnr6d8+i4u7t2bd9jJGv5tPn6szGdSpJYM6tQQCHfCbPlhU5zvgmJE4+Goq\nxj+A27Wd5OsexrtyPm5rQXVIQt8L8C6bjffLaVhmNslX3E752FHV0xPP+XVIp7yu8/n9PPzaNJ75\n34tp0TiNyx/+J3275ZLbqll1zNh/z+C83nkMObUr81Z+w5PvfsaDV51LZnoqL//xchIT4tlTUckF\n971I327taJ6Repg11m4ej4cnn3iQcwZdysaNhcyZPYmJE6eyYsWakLjU1IaMHnU1c+fmV7dVVFRw\nzz2PkpfXiby8jpFOPXrMSDznSipeeRi3awfJ19yPd1U+bluNz2WfYXiXz8G7YDrWLJvky26h/Mkb\nAXDFW6h49o5oZR87dIvCWqfcOdejxs8hO+BBBz3KLeCg+8Y5N9w51wO4Bvi8xrq+2G8ZdeoLDsCy\nwmJaZzQkJ6MhCXEeBnZuxYy1m0NizGB3pReAsr1VZKYmB9oxyqt8eP1+9nr9JMR5SE2sc7voRzup\nx/GkN0o7cmA9sWzzTlqnNyAnvUHgGOuQxYx1RSExhtU4xrxkpiYdsJzJqzYzsENWRHKOJk9OO/zb\nN+OKi8Dnw7f0C+I799ovymHJgS+7ltwAV1pcPSWucy/8xUX4izZGMOvoWrahkNbNG5OTmUFCfBwD\ne3VixpK1ITHrC7dzcsdjAOjV8RhmLA5MT4iPIzEh8Hur0uvDubo/ZvXkXiewbt0Gvv76W6qqqpjw\nxnsMHjzwgLh77/kjjz0+joqKiuq2PXvKmfXFl1RU7I1kylHnyc7FX7wFV7IV/D58y+cQ37HnflEO\nS0oBwJJTQj6XUn/UtU74Acws3cxWmVnH4PvXzOw3ZvYIkGJmi8zsVTNrE4x7GVgGtDazp81svpkt\nN7N7f8C6NprZI2a2EBhuZu3NbIqZLTCzz8ysQzCuhZm9HVz2PDPrHWz/hZktDuaUb2YNj96e+fGK\nyirISkupft8iLZmi0oqQmOtO78iHyzcyYNw0Rr01j9vO6grAWR1bkpIQR/+/T+PsZz7mV71ySU9J\njGj+EvuKdlfQIi25+n2LtGS27g79A/7bU3OZtLKQgc/NZPR7+dzar/MBy5m6ejNnd6z7nXBr1AS3\nc3v1e7dzO5bWJCSm6pM3ie/eh5Sbnyb5itup/DB4ajsxiYSfDaXq0zcjmXLUFRWXkdV43xffFhlp\nFBWXhcR0yGnO9IWBSu8ni9awu6KSkrJyADbv2MVF97/I2bc/w5UDT67TVXCAVtlZbNy4qfp9QUEh\n2a1CP1sn9OhKTuuWfPTR9EinF5Msbb/P5a4dWFrjkJiqmW8Tf/zPSLnxKZIv/SOVk1/aN39GJsm/\neZDkX4/Bc0w9OoOwP+eP3E+U1LVSZIqZLarx/mHn3AQzGwWMN7MngMbOuX8AmNmoYFUbM2sDtAd+\n7ZybE2y70zm3w8zigOlm1s05t+QIORQ5504Izv8pcI1zbp2ZnQ78DRgAPAk86pybE1zvRKArcAtw\nrXNurpmlAhX7L9zMrgWuBXjqiv5c3bfbj91HR9XkFQUM6dqaX52cy+KCHYz5cCFvjejHssISPGZM\nHdmf0ooqrvrXLHq3aUZORkx9z5BaYPKqQgZ3acWverZh8aYSxkxZyltXnIbHDIClhSUkx8fRrpnO\nMADEdTudqvwZeL+YiKd1e5IuGE353/5A4hkXUzX7Q6isX1XKH+KmC/rxyOsf8/6cZZzYLofmGal4\nPIHjK6tJI97801UUlZTx+6ffof+JHWnaqP7+HjMzHnvsbq6+5vfRTqVWiet6KlWLP8M7ZxKenHYk\nDRtJ+dO34spK2PPE/0J5GZ6WbUi6+CbKn74VKsujnbIcBXWtE17+fae6JufcNDO7CPg70P0w83/z\nfQc86OJgpzceaAl0AY7UCZ8AYGYZQG/g3xbsHLBvf58FdKzR3tjMUoBZwBNm9irwb+dcaHkmsC3P\nAs8ClD9/c0TPhTZPTWZz6b5fBFtKK2heo2oJ8M6Sbxl3UW8Aumc3Ya/XT8meSj5aUcDpx2WSEOeh\nScMkeuQ0YfnmneqES4jmDZPZUuPsypbSCjIbhg43eXdZAX8fHji1271VBpVePyXllTRpEIibUk+q\n4BCssKU3rX5v6U1xpTtCYhJ6/oKKlwKXvvi/WwPxCdAgDU9OO+LyToEBl2PJDcE58FbinTslotsQ\nac0bp7K5uLT6/ZaSUpo3Dq1mN89IZex1wwDYU1HJ9IWradQg+YCYdtnNyF+zkf496261clPBZnJy\nWlW/z85uScGmfcMQ09JSycvrxMfT3gIgKyuTt99+kfPPv4oF+Uf6c1k3udL9PpeNmhww3CShRz8q\n/vVnIHDx5fefS/bsgvLAn35/4QZc8RY8TbPwF35NvaNbFNYNwfHdnYE9QOPDhO6uMU9b4GbgTOdc\nN+BDIPlQMx5kGQZs22+Metca006u0Z7tnCt3zj1AoMqdCswxs5i6fUheywy+Ld5NQckeqnx+pqzY\nRN92oZ2dlo1SmPvNNgDWby+l0uujcYNEWjZKYd43gdNz5ZVelm4qpm2Tun0aV368vKxGfFuyh4Kd\nwWNs9Wb65TYPiclKS2bet4Fjaf2OMvb6/DQODm3yO8fU1VsYWE864f6CdXiatsQyMiEujrjjT8O7\ncn5oTMk24nIDv3osMzvwx373Liqev5vysaMoHzuKqtmTqPzsnTrfAQfIO7Yl3xYVU7CthCqvjylf\nrqRvt3YhMcVle/AHOwDPT57LsNOOB2BLcSkVlVUA7NpdwcK1BbTJCh3+U9d8OX8R7dq1pU2b1iQk\nJHDJxUOZOHFq9fRdu0pp2ep42nfoTfsOvZk7N79ed8AB/AXr8TTJCnwuPXHE5fXGu3pBaMyu7cS1\nDX4um7UKfC737Ap0xIMFOsvIxJpk4S8uOmAdUjfUtUr4ofweWEHgQswXzexU51wVUGVmCcHX+2tE\noEO908xaAOcAM37oCp1zxWZWaGbDnXPvBL8IHO+cWwx8DNwA/BXAzHo45xaZWW5wuMsSMzsF6Ais\nOeRKIize4+G2s7py/Ztz8DvH0ONb065ZGuM+X0mXrAz6tc/ipjPyuG/KYl6dvx4M7h3UAzPjkhPa\ncNdHizj/+U8BGNK1NR3q+p0rfoBb7n6ELxcuoaRkF2cO+yUjr76CCw5y0VN9Ee/xcOsZnRj5Tn7g\nGMvLJrf+g28+AAAgAElEQVRpKuNmr6VL80b0y23OTT/vyP0ff8UrC7/BMO4bkMf3Z5XyNxaTlZZM\nTn25647fT+XEF0j+9Z2BW6Hlf4or2kjCLy7Gv2kdvpULqJz8MklDf0v8aeeCg8q3x0U766iKj/Nw\n2yVncf2Tb+H3+xl62vG0a9WMce//hy7HZtGvezvmr/qOJ9/9DDOjZ/scbv+fs4DABZtj//0phuFw\n/Kp/L9pnZ0Z5i44un8/H/944hg8//BdxHg/jX5rAV1+t5u67b2bBgsVMnDjtsPOvWT2HRo1SSUxM\nZMiQsxl07qUH3FmlznF+Kj8aT/LltwZuHbpoJm5rAQn9LsC/6Wt8q/OpnPoqSYOvIf6UswGofO//\nARB3TCcS+12I8/sCy5n0AlTsPtza6q56cHcUq0tXdx/kFoWTgReBdwlUnkvNbCxQ6py728z+DAwB\n8oE7gYk1qtWY2XjgNOA7YCfwvnNufHBaP+Bm59x5NeI3Al2dcyXB98cBTwNZQCLwinPuQTPLDLZ3\nIPBF6FPn3A1m9jTQB/ATGPYywjlXeajtjfRwlNoufujIaKdQ61S9+ddop1CruE1bop1CreP5xYBo\np1CrNBp4d7RTqHVKxvSLdgq1TsO7XrUjRx1de54aGbE+ToPR46KyvXWqEu6cizvEpM41Ym6q8fpW\n4NYacV1rzuScu/Iw65rBfpVx51zOfu/XAweUNZ1zW4ELD9J+/aHWJyIiIlJv1INKeL0YEy4iIiIi\nEkvqVCVcREREROqAOjRc+lBUCRcRERERiTBVwkVEREQktmhMuIiIiIiIhJsq4SIiIiISW/TETBER\nERERCTd1wkVEREREIkzDUUREREQktjhdmCkiIiIiImGmSriIiIiIxBZdmCkiIiIiIuGmSriIiIiI\nxBSnh/WIiIiIiEi4qRIuIiIiIrFFY8JFRERERCTcVAkXERERkdii+4SLiIiIiEi4qRIuIiIiIrFF\nY8JFRERERCTcVAkXERERkdii+4SLiIiIiEi4qRIuIiIiIrFFY8JFRERERCTc1AkXEREREYkwDUcR\nERERkdiih/WIiIiIiEi4qRIuIiIiIrFFF2aKiIiIiEi4qRIuIiIiIjHF6WE9IiIiIiISbqqEi4iI\niEhsqQdjwtUJr8Vc5d5op1CrVL3512inUOskXPT7aKdQq1RNGBvtFGqfij3RzqBW8Xh0AvtHS1BX\nR2KTjkwRERERiS31oBKur9QiIiIiIhGmSriIiIiIxBY9MVNERERERMJNlXARERERiS0aEy4iIiIi\nIuGmSriIiIiIxBSnSriIiIiIiISbOuEiIiIiIhGm4SgiIiIiEls0HEVERERERMJNlXARERERiS1+\nPaxHRERERETCTJVwEREREYktGhMuIiIiIiLhpkq4iIiIiMQWVcJFRERERCTcVAkXERERkZjinCrh\nIiIiIiISZqqEi4iIiEhs0ZhwEREREREJN1XCRURERCS2qBIuIiIiIiLhpkq4iIiIiMQUp0q4iIiI\niEj9ZmZnm9kqM1trZrcdZPoxZvapmS00syVmNuhIy1QnXERERETkEMwsDvg7cA7QBbjUzLrsFzYG\neMM5dwLwP8C4Iy1Xw1FEREREJLbE1nCUk4G1zrn1AGb2OjAU+KpGjAMaBV+nA5uOtFBVwkVERESk\n3jKza81sfo2fa/cLyQa+q/F+Y7CtpnuAX5rZRmASMPpI61UlXERERERiiz9yq3LOPQs8+18u5lJg\nvHPuL2Z2KvBPM+vqnDvklqgSLiIiIiJyaAVA6xrvc4JtNV0NvAHgnJsNJAPNDrdQdcJFREREJKY4\nv4vYzw/wJdDezNqaWSKBCy/f3y/mW+BMADPrTKATvvVwC1UnXERERETkEJxzXmAUMAVYQeAuKMvN\n7D4zGxIM+wPwGzNbDLwGXOmcO2wPX2PCRURERCS2xNbdUXDOTSJwwWXNtrtqvP4KOP3HLFOdcPlR\nZm3YxmMzV+L3O4Z1zWFEr7Yh0wt3lXPX1GWU7vXid47Rp7enT9tMJq0s5KX5G6rj1mwr5bXLetOx\neSPqMu2v8Brz0Fg+mzWPJo0zePeVZ6KdTkyYtWEbj322Cr9zDMvLZsRJ+x1jpeXcNXV5jWOsHX3a\nBI+x/A3VcWu2lfHapb3pmJkW4S2IvFkrvuHRt/+D3/kZ3rsLI87qGTJ9045d3PPaJxSXVdCoQRIP\nXdGfFhmprNy4lYfenEnZ3krizMM1/Xsy8MT2UdqKyBnQvx9/+cs9xMXF8cKLr/H44we//fGwYecw\n4fVnOfW0c8nPX8KZZ/bhgftvIzExkcrKSm6/40FmzPgiwtlHR9xxx5N41uXg8eBdNJOqOR+GTLdG\nTUg671pIaoB5PFTOeAPfuiVYejNSfvMw/h2FAPgL1lE55aVobIJEQMx0ws2sBfBXoDdQDFQCjzrn\n3vmJy7sHKHPOPW5m9wGfOec+/gnL6QG0Cn4DwsyuBB4jMCA/gcBpiV855/b8lDyPtL5Y4vM7Hvl0\nBU+f35MWqclc/toc+h6XSW7T1OqY5+atp3/7LC7u3pp128sY/W4+fa7OZFCnlgzq1BIIdChv+mBR\nne9Qan+F37BB/bnsgiHccf/j0U4lJvj8jkdmrOTp4ScGjrEJc+nbdv9j7Gv6t2/Bxd2Cx9j7C+lz\n1UGOsYmL60UH3Of38/Bbn/HM9UNokZHK5WPfpG/XtuRmNamOGfveF5zXqxNDTu7EvNUbeXLibB78\nZX9SEhO4/5dncWxmBkU7d3PZX97g1E7H0KhBUhS36OjyeDw88cQDDDr3MjZuLOSLWROZOHEaK1eu\nCYlLTW3IqFFXM3dufnXbtm07OP+CERQWbqFLl45M/OAVjsvtFelNiDwzEgf8iorXH8Xt2kHylffg\nXbMQt33fbaMTThuKd8U8vAs/wZq2Ivnimyh/+mYAXEkRFS/cdail1x8RvDtKtMTEmHAzM+BdAh3l\n45xzPQkMes/ZL+4nfWlwzt31UzrgQT2A/R89OsE518M5l0fgy8IlP3HZP3R9MWHZ5p20Tm9ATnoD\nEuI8DOyQxYx1RSExhrG70gtA2V4vmakH/nGavGozAztkRSTnaNL+Cr+TehxPeqO631H8oZZt2Unr\njBrHWPssZqwPvQ7IjH3HWKWXzIYHOcZW159jbNk3RbRulk5Os3QS4uMYeEJ7Ziz9OiRm/ZYdnNw+\ncAvgXu2zq6cf2zyDYzMzAGie3pAmqSkU7y6P7AZEWK9ePVi3bgNff/0tVVVVvPHm+wwePOCAuHvu\nvpm/PD6Oir17q9sWL15OYeEWAL76ahUpKckkJiZGLPdo8bQ6Dn/xFlzJVvD78K2YS3yHE/eLclhS\nMgCWnIIrK4l8ohJ1MdEJB34BVDrnqs8vO+e+cc49ZWZXmtn7ZvYJMN3MUs1supnlm9lSMxv6/Txm\ndqeZrTaz/wAda7SPN7MLg697mtlMM1tgZlPMrGWwfYaZ/dnM5gWX0Sd4Bex9wCVmtsjMQjrbwS8F\nDQlU7jGzNmb2iZktCeZ4zBHaLzKzZWa22Mw+O9L6oq1odwUt0pKr37dIS2br7r0hMb89NZdJKwsZ\n+NxMRr+Xz639Oh+wnKmrN3N2x7r/B1/7S462orK9tKjxxa1FatKBx9gpuUxatZmBz3/G6PcXcmu/\nTgcsZ+rqLfXmGCvaWUZW431nClpkpFK0c3dITIdWzZi+ZD0AnyxZz+69VZTsrgiJWfrNFqq8flo3\nTT/6SUdRq1ZZfLdxXwW3oKCQ7Fahx0qPHl3JyWnFR5M/OeRyhg8fxKJFS6msrDxqucYKS22M27Wj\n+r0r3YGlNQ6Jqfr8HeLzTiPlhr+SfNEfqJz2yr750zNJvuo+ki+/HU9Oh4jlHWti7O4oR0WsdMLz\ngPzDTD8RuNA51xeoAIY7504EzgD+YgHfV8+/ryQfcM7LzBKAp4LL6gm8ADxYIyTeOXcycCNwt3Ou\nEriLfZXvCcG4S8xsEYEhKU2AD4LtTwEvOee6Aa8CTx6h/S5goHOuOzDkMOurNSavKmRwl1ZMuaYv\nTw09kTFTluKvcXHw0sISkuPjaNdM1UzQ/pKjb/KqzQzu3JIpV/+cp4acwJgpy0KPsc07SU6Io12N\nISz13U1DT2fBuk1c8tgE5q/bRPP0hnjMqqdv3bmbMa98zL2X/QKPxw6zpLrPzHj00bu49bb7DxnT\nuXMHHnrwDm4YdXsEM4ttcV16U7X0P5T//fdUvPkXkgZfCxiurIQ9435PxYt3UTn9NZKGXgeJyUdc\nntROsdIJD2Fmfw9Wh78MNk1zzn3/tdKAh8xsCfAxgceGtgD6AO845/Y453Zx4P0bIVAd7wpMC3ai\nxxA65OXt4L8LgDaHSXGCc64HkAUsBW4Jtp8K/Cv4+p/Az47QPgsYb2a/AeIOs75qNR+t+sJ/lv2Q\nWcKmecNktpTuqwZtKa044NT2u8sKGBA8rd29VQaVXj8l5fsqH1PqUVVX+0uOtuapSWwp21f53lK2\n98Bj7KsCBrQPHmMtM6j0+Skpr6qePmX1Zs6uJ0NRAJqnp7K5uKz6/ZaSMpqnN9wvpiFjR5zDhFsu\nYfS5pwBUj/suq6hk9D8mMurcU+jWpu7vt02bNtM6p1X1++zslhRs2lz9Pi0tlbwuHZk69Q1WrfqC\nU04+gX+/9QInntgtGJ/Fm2/8gxFX38j69d9EPP9ocGXFWKN91xhYWhNcaXFITEL3vvhWzAMCF18S\nlwANUsHnhfLAmRn/5g244iI8Ter+cXZQ/gj+REmsdMKXE6h2A+Ccu4HADc8zg001zxVeHmzvGewI\nbyFwQ/QfwoDlwSpzD+fc8c65moPbvv9r5uMHXLQavP/jB8DPf+D695//OgJfBFoDC8ys6Q+Y51nn\n3EnOuZNG/KzrT1ntT5aX1YhvS/ZQsHMPVT4/U1Zvpl9u85CYrLRk5n27HYD1O8rY6/PTOCUwBtDv\nHFNXb2FgPelUan/J0ZbX4vtjrDxwjK3ZTL/jMkNistKSmfddoIYROMZ8NE5JAILH2JotDOzQIuK5\nR0veMc35dttOCrbvosrrY8rCNfTt2iYkprisHH/wFPXzH+cz7JTAMLEqr4+bnp/EeSd1on+PdpFO\nPSrmz19Mu3ZtaNOmNQkJCVx80RAmTpxWPX3XrlKyc7rTseNpdOx4GnPnLeSCC0eQn7+E9PRGvPvO\nS9w55mFmz54fxa2ILP+mr/E0boGlNwNPHHGdT8G7ZmFozK7txLXpAoA1bQnxCbCnFFLSAhdyAJaR\niTXJwl9y2Oe9SC0WK3dH+YRAdft659zTwbYGh4hNB4qcc1VmdgZwbLD9MwJV5YcJbNdg4P/tN+8q\nINPMTnXOzQ4OT+ngnFt+mNxKgcONBfgZsC74+gsCQ2L+SeDLwueHazezXOfcXGCumZ1DoDN+pPVF\nTbzHw61ndGLkO/n4nWNoXja5TVMZN3stXZo3ol9uc276eUfu//grXln4DYZx34A8LPgLJX9jMVlp\nyeSkH+q/tm7R/gq/W+5+hC8XLqGkZBdnDvslI6++ggsGD4x2WlET7/Fwa7+OjHwvH7/fMTSvVeAY\nmxM8xo5rzk0/68D9n3zFK4u+wYD7zuq67xgrKCYrtX4dY/FxHm67oA/XP/N+YJ+d0pl2LZsybtJc\nuhzTnH5d2zJ/bQFPTpyDGfTMbcXtF/YFYOqiteSvK6RkdwXvz1sBwH2XnUmnnMzDrbJW8/l83Hjj\nn5j4wSvExcUx/qUJrFixmrvu+gP5C5Yw8cNph5z3+uuvJDe3DXfecSN33nEjAOeedzlbt26PVPrR\n4fxUTvsnyf9zC5gH75LPcNsKSOgzHH/hBnxrFwaGmgwaQXyvgYCj8sPnAIg7piOJfc7H+b3gHJWT\nx0PF7sOtrc6K5ljtSLEjPMwnYoIXSP4VOIXAYz53A88AKcBJzrlRwbhmBKrPqcB8Arc0PMc5t8HM\n7gR+DRQReHxofvAWheOBic65t4K3AHySQGc+Hvg/59w/zGwGcLNzbn5wHfOdc23MrAmBJyQlAA8H\n8/n+FoUeYCOBpyIVmdmxwItAs+A2XOWc+/Yw7W8D7QlU6KcTGIveuOb6DjcufM/To2PjP0/qrISL\nfh/tFGqVqgljo51CrWPH1d8Lz36KjOGPRTuFWmfH3b+Idgq1TsPbX4r6xQ47hveNWB+nyTszo7K9\nsVIJxzlXSKBafDDja8RtIzDG+mDLeJDQCy2/b7+yxutFHGT4iHOu337raBN8vYMDL/Icz0E4574h\ncKeXH9p+/kEWc7D1iYiIiEgdEjOdcBERERERQA/rERERERGR8FMlXERERERiilMlXEREREREwk2V\ncBERERGJLaqEi4iIiIhIuKkSLiIiIiIxRWPCRUREREQk7FQJFxEREZHYokq4iIiIiIiEmyrhIiIi\nIhJTNCZcRERERETCTpVwEREREYkpqoSLiIiIiEjYqRIuIiIiIjFFlXAREREREQk7VcJFREREJLY4\ni3YGR50q4SIiIiIiEaZOuIiIiIhIhGk4ioiIiIjEFF2YKSIiIiIiYadKuIiIiIjEFOfXhZkiIiIi\nIhJmqoSLiIiISEzRmHAREREREQk7VcJFREREJKY4PaxHRERERETCTZVwEREREYkpGhMuIiIiIiJh\np0q4iIiIiMQU3SdcRERERETCTpVwEREREYkpzkU7g6NPnfBazG3dEe0UapeqqmhnUOtUTRgb7RRq\nlYRLbop2CrVO1fP3RzuFWsXnrwdXq4WZ/7uiaKcgclDqhIuIiIhITNGYcBERERERCTt1wkVERERE\nIkzDUUREREQkpmg4ioiIiIiIhJ0q4SIiIiISU+rDLQpVCRcRERERiTBVwkVEREQkpmhMuIiIiIiI\nhJ0q4SIiIiISU5xTJVxERERERMJMlXARERERiSnOH+0Mjj5VwkVEREREIkyVcBERERGJKX6NCRcR\nERERkXBTJVxEREREYorujiIiIiIiImGnSriIiIiIxBQ9MVNERERERMJOnXARERERkQjTcBQRERER\niSnORTuDo0+VcBERERGRCFMlXERERERiii7MFBERERGRsFMlXERERERiih5bLyIiIiIiYadKuIiI\niIjEFD22XkREREREwk6VcBERERGJKbpPuIiIiIiIhJ0q4SIiIiISU3R3FBERERERCTtVwkVEREQk\npujuKCIiIiIiEnaqhMuPEpfbjcSBV4DHg3fhDKpmfRAy3Ro1JWnYdZDUAPN4qJz+Or61i0Omp4x8\nlMqZ/8Y7e1Kk04+4uHbdSTz3KjAP3gXTqfr8vZDplt6UpPNvgJSGmHmonPovfGsWhkxPGf1XKj99\nE+9++7qumrVhG499tgq/cwzLy2bESW1DpheWlnPX1OWU7vXid47Rp7ejT5tMJq0s5KX8DdVxa7aV\n8dqlvemYmRbhLYgtYx4ay2ez5tGkcQbvvvJMtNOJCZ62XUk887LA77HFn+GdG/q7yNKakHjuNVhy\nAzAPlTPfwr9+CdaoKcnXPITbsRkA36Z1VE19ORqbEFEDB/Rj7Nj7iPN4eOHF13j0sb8fNG748EG8\nOeEfnNL7HBbkL6FJk8a88fqznHRSd156+Q3+98YxEc48euK69CT5ouvAPFR9MZnKqW+GTLfGmST/\n+g9YSip4POx990V8y7+Ehmmk/OZO4o7pQNWcaex94+kobUH01Ye7oxyxE25mZc651P9mJWbWCnjS\nOXfhIaZnAJc558b9kPiDzD8e6AvsBAy4yTk3/b/JOZzM7Dpgj3Oudv+2NiPxnCupeOVh3K4dJF9z\nP95V+bhtBdUhCX2G4V0+B++C6VizbJIvu4XyJ2+snp444JchnfI6zYzEwVdTMf4B3K7tJF/3MN6V\n83Fba+yvvhfgXTYb75fTsMxskq+4nfKxo6qnJ57z65BOeV3n8zsembGSp4efSIvUZC6fMJe+bTPJ\nbbrvV9Bz876mf/sWXNytNeu2lzH6/YX0uSqTQZ1aMqhTSwDWbCvlpomL630HHGDYoP5cdsEQ7rj/\n8WinEhvMSOx/BXsnPI4r3UHyr+/Ct3YRbvum6pCE0wbjW/kl3kWfYk1bkXTR76l45hYAXEkRFePv\njlb2EefxeHjyiQc5e9ClbNxYyJzZk/hg4lRWrFgTEpea2pDfjbqauXPzq9sqKiq4+55HycvrRF5e\nx0inHj3mIfmSG9jz5B24km00uPUJvEvm4t/8bXVI4jmX4l3wOVWff4gn6xhSbriP3X+6Eqoqqfzg\nn3haHYun5bHR2waJiIgMR3HObTpChzoDGPkj4g/mFudcD+BGICzlHjMLy5kC59wztb4DDniyc/EX\nb8GVbAW/D9/yOcR37LlflMOSUgCw5BRcaXH1lLiOPfGXFOHfujGCWUePJ6cd/u2bccVF4PPhW/oF\n8Z177RflAtU2wJIbhO6vzr3wFxfhL6of+wtg2ZadtM5oQE56AxLiPAxsn8WM9VtDYsxgd6UXgLJK\nL5kNkw5YzuTVmxnYISsiOce6k3ocT3ojfRn5nqflcbiSItzOwO8x74p5xLU/ISTGAXz/eywpBVdW\nEvlEY8TJvU5g3boNfP31t1RVVfHGG+8xZPDAA+LuveePPPb4OCoqKqrb9uwpZ9YXX1JRsTeSKUed\np00H/Fs34bZvBp8X74KZxHfvHRrk9v3uJ6UBbuf2wOvKvfjWLcdVVUY2aYmKn9QJN7M2ZvaJmS0x\ns+lmdkywPdfM5pjZUjN7wMzKasQvC77OM7N5ZrYoOH974BEgN9j22H7xcWb2uJktC8aPPkJ6s4Hs\nGrn2NLOZZrbAzKaYWctge6/g8r5f5/fru9LM3jezT4DpwbZbzOzLYPy9wbaGZvahmS0O5nZJsP0R\nM/sqGPt4sO0eM7s5+LpHcB8tMbN3zKxxsH2Gmf05uG9Wm1mfn/J/czRZWpN9vygAt2sHltY4JKZq\n5tvEH/8zUm58iuRL/0jl5JcCExKSSDh9MFUz345kylFljfbbXzu3Y2lNQmKqPnmT+O59SLn5aZKv\nuJ3KD18ITEhMIuFnQ6n6NPQUZl1XVLaXFqn7OtUtUpPYujv0D/hvT8ll0qrNDHz+M0a/v5Bb+3U6\nYDlTV2/h7I7qhMuBLK0xbteO6veudAeWut/vsf+8S3zeqSSP/AtJF/2eymmv7Js/PZPkK+8h6dJb\n8eS0j1je0dIqO4vvNu47S7CxoJBWrUI/Wyf06Err1i2Z9FHMnICOKk9GM/zF+4oH/uJtWHrTkJjK\nD18h/uQzaPjgP2lww31UTKi/w04Oxe8sYj/R8lMr4U8BLznnugGvAk8G258AnnDOHQ8cqnx3XTCm\nB3BSMO42YJ1zrodz7pb94q8F2gA9aqzvcM4G3gUws4Rgrhc653oCLwAPBuNeBH4bzMO33zJODM7T\n18wGAO2Bk4EeQE8z+3lwPZucc92dc12ByWbWFBgO5AVzfeAg+b0M3BqcvhSoeV4z3jl3MoFq/kHP\nd5rZtWY238zmvzB/7RF2ReTFdT2VqsWfUf5/o6l47VGSho0EjMR+F1A15yOoql8VkSOJ63Y6Vfkz\nKH/8eir++TBJF4wOnC4/42KqZn8Ildpf+5u8ajODO7dkytU/56khJzBmyjL8NQYPLt28k+SEONo1\n/a9G0Uk9Ft/lFLxL/0PFuD+w982/knTebwD7/+zdd5xU9fX/8deZLSywVOm7CEoHC0pTYyJRwQIq\n2BU19q+JJUo0sWM0amJLYqLxp7HFimKlqGBXpPcqHWHpsAsssOzuzPn9McOyCyyIDjN3d99PH/Nw\n772fuXPuhzsznzlz7mfwLRvZ9p8/UPDSfRR+/ibpZ1wH6RnJDjepzIzHHh3EbX+8P9mhVCipXXtS\nNPZTttx1KVufupeMy2+Lfs0nVcpPHYQfC7we+/sV4PhS63ek7l7f9U4xY4A7zexPQAt337aPxzoZ\n+H/uXgzg7hvKafeomc2LPe7fYuvaAYcBo8xsKnA3kB2rQa/l7mPKiXVUqcfpHbtNASYD7YkOymcA\nvWLZ61+6+0aiNekFwPNmdjawtfROzawOUNfdv4qtehn4VakmO9LEk4h+8NiNuz/r7l3dveuVXVuX\n0xUHhm/eUObTvNWuX6Z8AiCtc0/Cs8cCEFm+AFLToEYtQlmtSD/5Iqrf9A/SepxK+vFnkdqtV0Lj\nTzTftEt/1TkI31z29E3rciLhmdHTMLJs/s7+ym5Neu8BVB/4b9KOPZ30X/UntcfuXwFXNo0yq7E6\nf+cHj9X523crN3l/dg6920QzcUc2rUthOELetqKS7Z/MW8WpKkWRcvjmXKz2zm+krFZ9PL/s61jq\nEb8iPHcCAJEVC2PPy0wIF0PBluh+Vi/F89YQql+5z7UVOatont2sZDk7qykrVqwqWa5VK5NOndrz\n2aghLJg3lh49jua9d1+ky9FHJCPcQIjkrSNUr2HJcqhegzLfigKkHXcKxZO/jrZfPBdLS8Nq1k5o\nnEHnbgm7JUvCpyh099eBM4FtwAgzOzFOu77N3dsCfyKa8YboRZqzYhn2zu5+uLv3/hH72lLqbwMe\nLrWP1u7+vLvPI5oxnwH8xczujX1Q6A4MAfoCH+/nMewYfYQJ4Mw1kZxFhOo3weo2hFAKKZ2OoXje\npLJtNq0n5ZDDALAGzaJvXls3UfDSA2x78ma2PXkzReM+pvDbDyieMCoZh5EwkZyFhA5qGu2vlBRS\nDj+O4rkTy7bJW0dKq1h/NcyK9teWTRQ8P4htT9zAtiduoGjMCAq/fo/icZ8k4zASqlPj2vyQt5Wc\njdsoCkf4ZP4qeh7asEybJrUyGL8s+mFm0YZ8tofD1KueBkDEnZHzV3NK28YJj10qhsjKxVi9Rlid\nBhBKIbVDd8ILyl787JvWE2rRAQA7qCmkpMHWzVC9Vkm20uo0xOo1JpK3drfHqEwmTJxK69aH0LJl\nc9LS0jj//LMYOmxkyfZNmzbTpNnhtG57DK3bHsO4cZPpf/YVTJo8PYlRJ1dk6TxCjZphBzWGlFRS\nu/PJgI4AACAASURBVJxA8fSxZdp47hpS2nUGINSkOaSm4/kbkxGu/EhmdqqZfW9mC8zs9j1s/3us\nxHlqrKx4nxeT/NSB3nfAhUSz4AOAb2LrxwLnAINj2/d0EIcCi9z9yVgt+RHANKC8K4dGAf9nZl+4\ne7GZ1d9LNhzg38CVZnYK8AXQ0MyOdfcxsfKUtu4+y8w2m1kPdx9XXqwxnwAPmNlr7p5vZllAEdG+\n2+Dur8Y6+mozywRquPsIMxsNLCq9I3ffaGa5scz5N8ClwFdUFB6h8KOXyBjwp+iUe1O/wtfmkNbz\nHCIrFhOeN5nCka9R7YyrSe1xKgCFH/y/JAedRJEIhcNeIOM3d0WnQpv8Bb5mOWknnk9kxULCcydR\n+PH/qHbW/5F6XB9wKHz36WRHnVSpoRB/6tmO330wmUjEOatTM1odlMnTYxfQsVFteh7aiIHHt+WB\nz2fz6tSlGHD/yYdhsYHR5JxcmmRmkF2nRnIPJEBuG/RXJkyZTl7eJk7qdwm/u+pSztnDhXVVhkco\nHPUa1c7/Q/R1bMY3+LoVpB3fj8iqJYQXTKXw88Gkn3o5ad16R5+XI54HIKV5W9J+2R/CYXCn6JOX\nSzLjlVU4HOb3N9/NiOGvkxIK8dLLg5k9ex73DbqViZOmMWzY3pMpC+aNpXbtTNLT0znrzFM5rc9F\nu82sUulEIhQM/g81bvgLhFIoGjOSyMofSO97KeGl8wjPGMf2d/5LxoCbSD+xP7hT8MoTJXev+cBL\n0Ys2U1JJPfI4tv3rrjIzq1QVQfrZejNLAZ4CehEto55gZh+6++wdbdz9llLtbwSO2m1Hu+7X9zER\no5lFgBWlVj0BvEO0proBsBa4wt1/iF1k+SpQnWgWeIC7Z5lZS2CYux8W+/RwKdGB7CqiUxNuMLPX\niQ7IP4od6I72qcAjRGuwi4Dn3P3fu8T4Uqz9kNjyOcDv3P0kM+tMtGa9DtGB8z/c/Tkz6wE8B0SI\nDoS7uvsvzOzy2N83lNr/74GrY4v5wCVAa+DR2P2LgN8COcAHQAbRDPpj7v6ymd0H5Lv7Y7F4ngFq\nEB2kX+HuuWb2JXCru080swbARHdvubd/my33D6gCs2jGUVHRvttIGdakUbJDqFDSLhiY7BAqnKLn\nH0h2CBVK7XtG7ruRlJF7ddUtjfmpaj39UdJHwOOanZ2wMU6PFe/u9XjN7FjgPnc/JbZ8B4C7P1xO\n+++AQe6+10+p+xyE7w8zqwFsc3c3swuBi9z9rLg9QByZWaa775i95Xagqbv/Pslh7RcNwveTBuH7\nTYPw/aNB+P7TIHz/aBC+/zQI339BGISPTeAg/NiV7/0f0YlAdnjW3Z/dsWBm5wKnuvvVseVLgR6l\nE7al2rYgWhmS7e67TvxRRrzrjrsA/7bod8N5wJVx3n889Yl9kkkFlgKXJzccEREREUm02ID72X02\n/HEuBIbsawAOcR6Ex+qcj4znPg8Udx9MtHZdRERERAIkSDXhRMuNm5dazo6t25MLget/zE4TPjuK\niIiIiEgFMgFoY2aHmFk60YH2h7s2MrP2QD2i03HvU+CmwRMRERGRqi2Z83fvKjY73w1EZ8xLAV6I\nzbR3P9GJNHYMyC8E3vQfecGlBuEiIiIiInvh7iOAEbusu3eX5fv2Z58ahIuIiIhIoESSHUACqCZc\nRERERCTBlAkXERERkUBxglMTfqAoEy4iIiIikmAahIuIiIiIJJjKUUREREQkUCIJ+9H65FEmXERE\nREQkwZQJFxEREZFAiejCTBERERERiTdlwkVEREQkUDRFoYiIiIiIxJ0y4SIiIiISKPrZehERERER\niTtlwkVEREQkUFQTLiIiIiIicadMuIiIiIgEimrCRUREREQk7pQJFxEREZFAUSZcRERERETiTplw\nEREREQkUzY4iIiIiIiJxp0G4iIiIiEiCqRxFRERERAIlUvmrUZQJFxERERFJNGXCRURERCRQIrow\nU0RERERE4k2ZcBEREREJFE92AAmgQXgFFmrRPNkhVCwHt052BBVPwdZkR1ChFD3/QLJDqHDSrron\n2SFUKHbPyGSHUOGktM5Odggie6RBuIiIiIgEin62XkRERERE4k6ZcBEREREJlIhpdhQREREREYkz\nZcJFREREJFCqwuwoyoSLiIiIiCSYMuEiIiIiEiiaHUVEREREROJOmXARERERCZRI5Z8cRZlwERER\nEZFE0yBcRERERCTBVI4iIiIiIoESofLXoygTLiIiIiKSYMqEi4iIiEig6Md6REREREQk7pQJFxER\nEZFA0RSFIiIiIiISd8qEi4iIiEig6GfrRUREREQk7pQJFxEREZFA0ewoIiIiIiISd8qEi4iIiEig\naHYUERERERGJO2XCRURERCRQNDuKiIiIiIjEnTLhIiIiIhIoyoSLiIiIiEjcaRAuIiIiIpJgKkcR\nERERkUBxTVEoIiIiIiLxpky4iIiIiASKLswUEREREZG4UyZcRERERAJFmXAREREREYk7ZcJlv4xe\nuJpHRk0n4k7/I1tw5XHtymxfuXEr9wydxObtRUQizk2/7sQvWzehKBzhgY+mMHtlHiGD23odQbcW\nDZN0FIkzetZiHnnrMyIRp/8vjuDKU3uU2b5i/Ubu+9/H5OZvpXaN6jx0ZR8a16vFivUbGfjM+0Tc\nKQ5HuOjXR3Perzon6SgSa/ScpTzy7rdEPEL/Yzpy5cldymxfsWET973xObn5BdSuUY2HLu1F47qZ\nzF2+lofe/or87YWkWIire3XhlKPbJOkoEid0yGGkn3QxhEIUT/ua4nEjymy3WvVJ73M1llEDLETh\nV0OILJqO1T6IjKsfwjesAiC8YiFFI/+XjEMIlLsfeoKvR4+nfr26vP/qM8kOJxB69+7JE0/cT0oo\nxAsvvsGjjz61x3b9+5/OW4Of45hjTmPS5OnUr1+PwW8+S9euR/K//73F72++O8GRJ0+oZSfSe14Y\nfV7O+IbiCR+X2W616pN+6hVYtdjz8tt3iCyeGd3WIIv0ky/F0qsDEQpeexDCxUk4iuTyZAeQAEkf\nhJtZGJgRi2UxcKm758Vhvy2BYe5+WBz29RJwArAxtuoFd3/y5+63nMfqCRS6+3cHYv8/RzjiPPzJ\nNJ656Bc0rl2dAS9+wQltmtKqYe2SNs+N/p7eHbI4v8uhLFy7iRveGsNHrZvwzpQlAAy55iQ2bNnO\n9YO/47UrehKyyjsHUTgS4eE3RvHM78+ncb1aDHj4FU44ohWtmjUoafPEO1/S95hOnHnsYYyfu5Qn\n3/+aB6/oQ8M6mfzvjwNIT0tla0Eh59z/Iicc0ZpGdTOTeEQHXjgS4eEhX/PMb8+kcd1MBjzxNicc\ndgitmtQvafPEB9/Rt1t7zuzenvHzlvPksDE8eEkvqqen8cAlJ9OiYV3WbNzCxY+/xbHtD6Z2jWpJ\nPKIDzIz0XpeyffBj+OYNZPzmXsILpuLrV5Q0STvuDMJzJ1A89QvsoGZUO+8WCp65DQDPW0PBS4OS\nFX0g9Tu9FxefcyZ3PvBYskMJhFAoxJP/fJDTTr+I5ctXMnbMCIYNG8mcOfPLtMvMrMmNN1zFuHGT\nS9YVFBRw332P0KlTezp1arfrrisvM9JPvJjt7/wd35xLxoC7CC+chm9YWdIkrUcfwt9PpHj6V1j9\nplTrfxMFz98BFqLaaVez/aPn8XXLIaMmRMJJPBg5kIJQjrLN3TvHBssbgOuTHVA5bovF2Xl/BuBm\nlrKfj9MTOG4/75MQM1dsoHm9mmTXq0laSohTOmbz5fyVZdoYsKUw+ok9f3sRDTMzAFi0bhPdY5nv\n+jWrUataGrNW5iY0/kSbuWQlzRvVI7thXdJSUzilW3u+nL6gTJtFK9fTvd3BAHRrdzBfTotuT0tN\nIT0t+hm5sDiMe1XICcDMpWto3qAO2Q3qRPvsqDZ8OWNxmTaLVm+ge5ssALq1ySrZ3qJRXVo0rAtA\nozo1qZ9Zndwt2xJ7AAkWanoonrcG37gWImGK54wnpc1RZdo4QLXqAFi16nj+z85xVGpdOx9Ondq1\nkh1GYHTvdhQLFy5h8eIfKCoqYvBbH3DGGafs1u7P9/2RRx97moKCgpJ1W7duY/R3Eygo2J7IkJMu\n1OQQPG8tvnFd9Hk5dwIprcp+k+l42efllujzMtSyI5F1y6MDcICCLVBFXv93FbHE3ZIlCIPw0sYA\nWQBmlmlmn5nZZDObYWZnxda3NLM5Zvacmc0ys5FmVj22rYuZTTOzaZQazJtZhpm9GNvPFDP7dWz9\n5Wb2vpmNMrMlZnaDmQ2MtRlrZvV3D3EnM7sots+ZZva3UuvzzezxWBzHxuL6yswmmdknZtY01u4m\nM5ttZtPN7M1Y9v464BYzm2pmv4xj3/5sazYX0KR29ZLlxrWqs2ZzQZk21/2qA8NnLqP3vz7ihrfG\ncHvvIwBo27gOX85fSXEkQk7eFmavymP1pso9QFqTm0+TejvfzBvXrcWa3PwybdpmN+KzKdGM0udT\n57OloJC8/Gi/rNqwifMeeJFT73iGy0/pXumz4ABrNubTpN7O42xcN5M1G7eUadO2WQM+m74IgM+n\nL2LL9iLytpQ9D2csXU1RcYTmB9U58EEnkdWqh2/aULLsmzdgmfXKtCn69n1SOx1Lxu8ep9p5t1A4\n6tWd96/TkIzL76PaRX8ilF35S3dk/zXLasLy5Tu/WcnJWUlWsyZl2hzV+TCymzflo48+S3R4gWSZ\ndfHNpZ6X+blYrbpl2hSNGUpqhx5kXPMI1frfROHnbwAQqtsY3Kl29s1kDLib1K67f+CRyiMwg/BY\nxvgk4MPYqgKgv7sfDfwaeNyspHahDfCUu3cC8oBzYutfBG509yN32f31gLv74cBFwMtmlhHbdhhw\nNtANeBDY6u5HEf1AcFmpfTwaGxhPNbPDzawZ8DfgRKAz0M3M+sXa1gTGxeIYB/wLONfduwAvxB4H\n4HbgKHc/ArjO3ZcAzwB/j2Xcv9lDP11rZhPNbOLzX07dR68m3sezlnHmEQcz8sbT+Pf5x3L3h5OI\nuNPvyBY0rlWdi1/4kkdHTefI7PqVuhTlxxp4Tk8mzV/GBQ++zMR5y2hUN5NQKNovTerX5u17ruDD\nB65h6JhZrN+0ZR97qxoGnvULJi1cwQWPDmbiwhU0qlOzzLm0duMW7n71U/588YklfVmVpXbsQfGM\nbyl4+g9sf/vvVOt7DWD4lo1s+88fKHjpPgo/f5P0M66D9Ix97k+kNDPj0UcH8cc/3p/sUCqU1Hbd\nKZ71HQXP/ZHt7z1JtdOuAgxCKYSy2rB9xH8pGPwIKa2PItS8fbLDTYpIAm/JkvSacKC6mU0lmgGf\nA4yKrTfgITP7FdE+ygIax7YtdvcdI9BJQEszqwvUdfevY+tfAU6L/X080YEw7j7XzJYCbWPbvnD3\nzcBmM9sIDI2tnwEcUSrO29x9yI6FWGb+S3dfG1t+DfgV8D4QBt6JNW1HdKA/KvYZIgXYUcMxHXjN\nzN6P3W+f3P1Z4FmAbS/fntDvqBrVymBVqez16s3baFSr7Jv2e9OW8vSF0WqaI7MPYns4TN7WQurX\nrMZtvXZ252Uvf0WL+pU7s9uoXiarcjeXLK/O20yjemWPuVHdTJ64LvrZbWtBIZ9NmUftGhm7tWmd\n1YDJ85fTq0vlrqtsVCeTVaW+LVidl0+jOjV3aVOTJ66MPrW3bi/ks2kLS+q+8wsKufG5YdzQpwdH\ntCybrauMfHMuVnvnF3ZWqz6eX7bMK/WIX7H9rScAiKxYCKlpUCMTtm4uudjLVy/F89YQqt+EyKol\nCYtfgm9Fziqys5uVLGdlNSVnxaqS5Vq1MunUqT2fjoq+PTZp0pB3332Rs8++gkmTpyc83iDw/Dys\nVqnnZWY9fHPZMrDUw45n+7v/ACCychGkpEH1TDw/l8jyeVAQfR0ML55BqPHBRJbNTdwBSMIEIRO+\nzd07Ay2IDrx3lJEMABoCXWLbVwM7RielC8zC/LwPE6X3FSm1HPkZ+y1w9x1XUhgwq1Q9+eHu3ju2\nrQ/wFHA0MMHMgvChqFydmtXjh9x8cvK2UBSO8Mns5ZzQpmmZNk1r12DckrVAtA68sDhCvRrpbCsq\nZlusVnzM4jWkhqzMBZ2VUacWTflhTS456/IoKg7zyYS5nHBE6zJtcvO3EolEP0s9//E4+h13OACr\nczdTUFgEwKYtBUxZkEPLJnutjqoUOh3ciB/WbSRn/aZon02ZzwmHtSzTJjd/284++3Qy/Xp0AKCo\nOMzA50fQt2t7enVuveuuK6XIysVYvUZYnQYQSiG1Q3fCC6aUaeOb1hNqEe0jO6hp9M1+62aoXgti\n3yBYnYZYvcZE8tYm/Bgk2CZMnErr1ofQsmVz0tLSuOD8sxg2bGTJ9k2bNtO02eG0aXsMbdoew7hx\nk6v0ABwgsmoJVrcRVjv2vGzfjfCiaWXa+Ob1hA6OPS/rN4l+ON62mfCSWYQaZEFqOliIlOy2RNav\n3NPDVHrKhCeQu281s5uA983saaAOsMbdi2I13C32cf88M8szs+Pd/Vuig/gdvoktf25mbYGDge+J\nDn5/qvHAk2bWAMglWubyrz20+x5oaGbHuvsYM0sjmoWfAzR39y/M7FvgQiAT2AwEcnSaGgpxe+8j\n+e2bo4lE4KwjW9C6YW2e/mo2HZvWo2fbpgw86TDu/2gKr41fABh/7ns0ZsaGLdv53ZvfETJoVKs6\nfzmza7IP54BLTQlx+wUn89snhxCJRDjruMNp3awBT3/4LR1bNKHnka2Z+P0ynnz/a8yMLm2yuePC\nk4HoBZtPvPMFhuE4l/XqRpusyj+lY2pKiNvP+SW/feZDIhHnrB4daN30IJ4eMY6OBzei52GHMHFB\nDk8OG4sZdGnVjDvOPQGAkVMXMHnhSvK2FPDh+DkA3H/xSbTPrsT95hEKR71GtfP/ABadCs3XrSDt\n+H5EVi0hvGAqhZ8PJv3Uy0nr1hscCkc8D0BK87ak/bI/hMPgTtEnL0cvAqvibhv0VyZMmU5e3iZO\n6ncJv7vqUs7Zw4WIVUU4HOb3N9/N8OGvkxIK8dLLg5k9ex6DBt3KpEnTGDZs1F7vP3/eWGrXziQ9\nPZ0zzzyV0/tctNvMKpWORyj84nWqnXMzmFE8czS+fgVpx51JZNVSwoumUfjV26T3uoy0LidHn5ef\nvBi97/atFE0eRcbFdwFOePEMIotnJPVw5MCxZM+6YGb57p5Zanko8BbwEdHSkExgInAMO8tLSqYe\nNLNbgUx3v8/MdtRcOzASON3dD4vVf/8H6AoUAwNjg9/Lga7ufkNsX0tiy+tKb4tNUTisdDlKrP1F\nwJ1Es93D3f1P5RxTZ+BJoh8sUoF/AC8BX8TWGfCqu/819iFhCNEPZzfuqS58h0SXo1R4B1eN7Ghc\nFWxNdgQVik+fsu9GUkbaVfckO4QKpUazQF2vXyFs/OvpyQ6hwqkx8LmkX1Dz2MGXJGyMc+sPrybl\neJOeCS89WI0tn1Fq8dhy7lYy97e7P1bq70lA6Ysy/xhbXwBcsYfHfonoYHjHcss9bXP3y8uJ/Q3g\njT2s3/WYphKtF9/V8Xu47zzK1qKLiIiISCUThJpwEREREZEqJemZcBERERGR0pL5IzqJoky4iIiI\niEiCKRMuIiIiIoGSzKkDE0WZcBERERGRBFMmXEREREQCpSrMwaxMuIiIiIhIgikTLiIiIiKBEqkC\nuXBlwkVEREREEkyZcBEREREJFM2OIiIiIiIicadBuIiIiIgEiifw9mOY2alm9r2ZLTCz28tpc76Z\nzTazWWb2+r72qXIUEREREZFymFkK8BTQC1gOTDCzD919dqk2bYA7gF+4e66ZNdrXfjUIFxEREZFA\nCVhNeHdggbsvAjCzN4GzgNml2lwDPOXuuQDuvmZfO1U5ioiIiIhUWWZ2rZlNLHW7dpcmWcCyUsvL\nY+tKawu0NbPRZjbWzE7d1+MqEy4iIiIigRKxxD2Wuz8LPPszd5MKtAF6AtnA12Z2uLvnlXcHZcJF\nRERERMqXAzQvtZwdW1facuBDdy9y98XAPKKD8nJpEC4iIiIiUr4JQBszO8TM0oELgQ93afM+0Sw4\nZtaAaHnKor3tVOUoIiIiIhIoQfrZencvNrMbgE+AFOAFd59lZvcDE939w9i23mY2GwgDt7n7+r3t\nV4NwEREREZG9cPcRwIhd1t1b6m8HBsZuP4oG4SIiIiISKMHJgx84qgkXEREREUkwZcJFREREJFAC\n9mM9B4Qy4SIiIiIiCaZMuIiIiIgESpBmRzlQlAkXEREREUkwZcJFREREJFAqfx5cmXARERERkYRT\nJlxEREREAkWzo4iIiIiISNwpEy4iIiIigaLZUUREREREJO6UCa/A6lz7arJDqFDcK/+n6ngLhfQ5\nfX+EI1WhijG+7J6RyQ6hQtm64ptkh1Dh1G7+62SHUOFsG5jsCDQ7ioiIiIiIHADKhIuIiIhIoFSF\n7xWVCRcRERERSTANwkVEREREEkzlKCIiIiISKF4FLs1UJlxEREREJMGUCRcRERGRQNGFmSIiIiIi\nEnfKhIuIiIhIoOhn60VEREREJO6UCRcRERGRQKn8eXBlwkVEREREEk6ZcBEREREJFNWEi4iIiIhI\n3CkTLiIiIiKBonnCRUREREQk7pQJFxEREZFAcdWEi4iIiIhIvCkTLiIiIiKBoppwERERERGJOw3C\nRUREREQSTOUoIiIiIhIoujBTRERERETiTplwEREREQkUXZgpIiIiIiJxp0y4iIiIiARKxFUTLiIi\nIiIicaZMuIiIiIgESuXPgysTLiIiIiKScMqEi4iIiEigRKpALlyZcBERERGRBFMmXEREREQCRb+Y\nKSIiIiIicadBuOyX3r17MnPGV8ye/S233Xp9ue369zudwu3LOfroIwA46aRfMnbMCCZP+pSxY0bQ\ns+dxiQo5qXr37snMmV8zZ/a33HbbXvqr/+kUFebQJdZf9evXY9TIt8ndMI9//uMviQo3EHr36smM\n6V8ye9Y33Hrr78pt16/faWwvWFbmHBvz3XAmTRzFmO+GV5lz7JTePZk182vmzv6WP+7jHCve5Rz7\ndOTb5FXFc0zPy7i6+6En+FWfC+l3yXXJDiUwevU6gWnTPmfmzK+49dbfltuuX7/T2LZtKUcffTgA\nJ554PKNHD2PChE8YPXoYJ5xQNV7H9iSSwFuyqBwlzszsLuBiIEz03/Y9IMPd7yjVpjPwhrt3MLMl\nwDJ3/2Wp7VOBVHc/LKHB70MoFOKf//wLp59+McuXr2TMd8MZNmwkc+bOL9MuM7MmN9xwJePGTS5Z\nt37dBvqffQUrV66mU8d2DBv2Gocc2jXRh5BQoVCIJ//5IKedfhHLl69k7JgR0f6as3t/3XjDVWX6\nq6CggPvue4ROndrTqVO7RIeeNCXnWJ/oOfbd6GEMGzaKuXs8x8r22bp1Gzj7nCtZuXI1HTu2Y9jQ\nVzm0VbdEH0JC7TjHTi11jg0t5xy7aQ/n2KAqeo7peRlf/U7vxcXnnMmdDzyW7FACIRQK8Y9/PECf\nPgPIyVnFt99+yLBhn+7xdez6669g/PhS75Xrczn33CtZuXINHTu2ZejQV2jVqkeiD0ESRJnwODKz\nY4G+wNHufgRwMvAFcMEuTS8E3ii1XMvMmsf20SERsf4U3bp1ZuHCJSxe/ANFRUW89dYHnHFG793a\n3XffbTz2+NMUFGwvWTd12ixWrlwNwKzZ31O9egbp6ekJiz0Zunc7qkx/DX7rA84445Td2v35vj/y\n6GNPU1BQULJu69ZtjP5uQpk+rAp2O8fe/nDP59igW3n8sacp2L6zf6aVOsdmV9Fz7K23PuBMnWN7\npedl/HXtfDh1atdKdhiBseN1bMmSZRQVFfH220Pp27fXbu0GDfoDjz/+TJnzKfo6tgaA2bPnkZFR\n+V/HyhPBE3ZLFg3C46spsM7dtwO4+zp3/xrINbPSH2XPp+wg/C12DtQv2mVbYGQ1a8ryZStLlnNy\nVtEsq2mZNp07H0bz7GZ89NHn5e7n7P59mDJ1BoWFhQcs1iBoltWE5ctXlCzn5Kwkq1mTMm2O6nwY\n2c2b8tFHnyU6vEBq1qwJy/bRZ507H0Z2djM++rj8c6x//9OZWkXOsdL9tTxnJc32cI41b96UETrH\nAD0v5cBr1qwJy5eXfq9cSVbWnl/HPt7n69jMSv86VpWpHCW+RgL3mtk84FNgsLt/RXRQfSEwzsyO\nATa4e+nvpd4BXgQeA84ABgCX7ukBzOxa4FqAlJS6hFJqHqhj2W9mxqOPDOLqa24pt03HDm158KE7\n6NNnQAIjCyYz49FHB3HV1eX3l5RlZjzyyL1cc83Actt06NCWhx68kz59dY6ZGY89OogrdY79aHpe\nyoFmZvztb3dzzTW3ltumQ4c2/OUvt9O37yUJjEwSTZnwOHL3fKAL0UHyWmCwmV0ODAbONbMQu5ei\nAKwnmi2/EJgDbN3LYzzr7l3dvWuiB+A5K1aS3Xxn5jsrqwkrcnZ+2q9VK5NOndoxauTbzPt+DD16\nHMW777xQcuFcVlZT3n77v1x55c0sWrQ0obEnw4qcVWRnNytZzspqSs6KVSXL0f5qz6ejhjB/3lh6\n9Diad999seQisKpoxYpVNN9Xn3Vsx8iRb/H999/Ro/tRvDOk9DnWhLffeo4rr6o651jp/srOasqK\nPZxjn40awoLYOfZeVT/H9LyUA2zFilVkZ5d+r2xKTk7Zc6xjx3aMHPkmc+d+S/fuRzFkyPMlF2dm\nZTVh8OBnufrqgSxe/EPC4w8KT+B/yaJBeJy5e9jdv3T3QcANwDnuvgxYDJwAnEN0UL6rwcBTBLQU\nBWDixGm0bn0ILVs2Jy0tjfPPP4thw0aVbN+0aTPNso6gbbtjadvuWMaNm8LZ51zJ5MnTqVOnNh+8\n/zJ33fUwY8ZMTOJRJM6EiVPL9NcF55/FsGEjS7Zv2rSZps0Op03bY2jT9hjGjZvM2WdfwaTJfSRj\nwQAAIABJREFU05MYdXJFz7GWO8+x887c7RzLyj6Sdu2Oo1274xg3fgrnnLvzHHv/vZe56+6qe46d\nf/5ZDN3lHGvS7HBatz2G1rFzrH8VP8f0vJQDbcd7ZYsW0XPsvPPOYPjwsq9jzZsfRfv2x9O+/fGM\nHz+Fc8+9ismTZ1CnTm3effdF7rnnb1Xmdawq0yA8jsysnZm1KbWqM7AjHfcG8Hdgkbsv38Pd3wMe\nAT45sFH+dOFwmJtvvofhw15j+vQvGDJkKLPnzGPQvbfu8aKT0n7328tp1aold911MxPGf8KE8Z/Q\nsOFBCYo8OcLhML+/+W6GD3+dGdO/5O0hQ5k9ex6DBu27vwDmzxvLo4/ey2WXnc/iRRPp0KHNPu9T\n0e04x4YNfZXp075gyDvDmDNnHvfe+wf69tl7n/12xzl2582MH/cx48d9XGXOsRHDX2fm9C+jz8nZ\n87jvR55jC+aN5bFH7+U3l53Pkip0jul5GV+3DforA/7vFpb8sJyT+l3CO0MD+zaWEOFwmFtuuZeh\nQ//H1Kmf8c47w5kzZz733DOQPn1O3ut9r7vuN7Rq1ZI77riJsWNHMHbsiEr/OlaeqjBFoblX/l8k\nShQz6wL8C6gLFAMLgGvdfZ2ZNQBWAje6+zOl7rME6Oru60qtawkM29cUhenVsvWPtx90ru+/UEif\n0/dHOJLMl/OKyZIdQAWzdcU3yQ6hwqnd/NfJDqHC2bZtadKfmme3ODNhb9rvLv0wKcerCzPjyN0n\nAXucWT82yE7bw/qWe1i3BAjUHOEiIiIiiVIVEmdKc4mIiIiIJJgy4SIiIiISKMn8EZ1EUSZcRERE\nRCTBlAkXERERkUCpCpe5KxMuIiIiIpJgyoSLiIiISKAk85csE0WZcBERERGRBFMmXEREREQCRbOj\niIiIiIhI3CkTLiIiIiKBol/MFBERERGRuNMgXEREREQkwVSOIiIiIiKBoh/rERERERGRuFMmXERE\nREQCRT/WIyIiIiIicadMuIiIiIgEin6sR0RERERE4k6ZcBEREREJFP1Yj4iIiIiIxJ0y4SIiIiIS\nKKoJFxERERGRuFMmXEREREQCRfOEi4iIiIhI3CkTLiIiIiKBEtHsKCIiIiIiEm/KhIuIiIhIoFT+\nPLgy4SIiIiIiCadBuIiIiIjIXpjZqWb2vZktMLPb97D9cjNba2ZTY7er97VPlaOIiIiISKAE6cd6\nzCwFeAroBSwHJpjZh+4+e5emg939hh+7X2XCRURERETK1x1Y4O6L3L0QeBM46+fuVINwEREREQmU\nCJ6wm5lda2YTS92u3SWcLGBZqeXlsXW7OsfMppvZEDNrvq9jVDlKBbZx6J3JDqFCiYwdnewQKp40\nvUTsj8iyNckOocJJaZ2d7BAqlNrNf53sECqcTcu+SHYIEnDu/izw7M/czVDgDXffbmb/B7wMnLi3\nO+gdVkREREQCxYP1Yz05QOnMdnZsXQl3X19q8b/AI/vaqcpRRERERETKNwFoY2aHmFk6cCHwYekG\nZta01OKZwJx97VSZcBEREREJlCDNjuLuxWZ2A/AJkAK84O6zzOx+YKK7fwjcZGZnAsXABuDyfe1X\ng3ARERERkb1w9xHAiF3W3Vvq7zuAO/ZnnxqEi4iIiEigeIAy4QeKasJFRERERBJMmXARERERCZSA\nzY5yQCgTLiIiIiKSYMqEi4iIiEigBGl2lANFmXARERERkQRTJlxEREREAkU14SIiIiIiEncahIuI\niIiIJJjKUUREREQkUHRhpoiIiIiIxJ0y4SIiIiISKPrZehERERERiTtlwkVEREQkUCKaolBERERE\nROJNmXARERERCRTVhIuIiIiISNwpEy4iIiIigaKacBERERERiTtlwkVEREQkUFQTLiIiIiIicadM\nuIiIiIgEimrCRUREREQk7pQJFxEREZFAUU24iIiIiIjEnQbhIiIiIiIJpnIUEREREQkUXZgpIiIi\nIiJxp0y4iIiIiASKLswUEREREZG4UyZc9svo2Ut45J2viESc/sd24sre3cpsX7FhE/e9Norc/G3U\nrpHBQ5edQuN6tZi7fC0PDf6c/IJCUkLG1b27c0qXtkk6isRJaXUE6adcCqEQxVO+pGj00DLbrfZB\nVOt3HVSrgYVCFH72JuEF07A6Daj+u0eJrF8JQGT5AgpHvJCMQ0i4lEMPJ/3kAdE+m/oVRWOHl9lu\ntetTre+1O/vsy7cIL5we7bNrHiayIdZnOQsp/OTlZBxCQqV07ELGedeBhSj67mMKR75dZrvVa0jG\nb/6AVc+EUIjt779IeNYEqFmL6tfcRcrBbSkaO4rtb/0nSUeQeKGWnUjveWH0HJvxDcUTPi6z3WrV\nJ/3UK7BqNcBCFH77DpHFM6PbGmSRfvKlWHp1IELBaw9CuDgJR5E4vXqdwGOPDSIlJYWXXnqTxx7b\n87nSr99pvPHGM/ziF32ZPHkGJ554PA88cDvp6WkUFhZx550P8dVX3yU4+uC5+6En+Hr0eOrXq8v7\nrz6T7HACyz2S7BAOuCo7CDez5sDXQBd332Bm9YDJwK+BNODvQAcgD9gEDHL3r83scuBRICfWbg5w\nmbtvjVNcnYFm7j4iHvuLp3AkwsNvf8kz1/encd1MBjz6Jiccfiitmh5U0uaJ976hb/cOnNmjI+O/\nX8aTQ7/jwctOoXp6Kg9c2psWjeqxZmM+Fz/yBsd2aEHtGtWSeEQHmBnpp11OwasP45s2kHH1AxR/\nPxlfl1PSJO2X/SieNZbiSZ9hDbLIuPg2tj15MwCeu5qCZ+9MVvTJYUZ678soePORaJ9dfh/F86fg\n61eUNEk77iyK54yneMrn2EHNyDh/INv+cysAnreGghfuTVb0iWchMi64nq1P3onnraPGn/5J8fRx\nRFb9UNIk/bSLKJ70DUXfDCfU5GCqX38/W+65HIoKKRz6CqFmLQg1bZG8Y0g0M9JPvJjt7/wd35xL\nxoC7CC+chsc+vAGk9ehD+PuJFE//CqvflGr9b6Lg+TvAQlQ77Wq2f/Q8vm45ZNSESDiJB3PghUIh\n/vGPB+jTZwA5Oav49tsPGTbsU+bOnV+mXWZmTa6//grGj59csm79+lzOPfdKVq5cQ8eObRk69BVa\nteqR6EMInH6n9+Lic87kzgceS3YokmRVthzF3ZcB/wH+Glv1V+BZYBUwHHjW3Vu5exfgRuDQUncf\n7O6d3b0TUAhcEMfQOgOnx3F/cTNz6WqaN6hDdoM6pKWmcEqXtnw5Y1GZNotWbaB72+YAdGubXbK9\nRaN6tGhUD4BGdTKpn1mD3Py4fG4JrFBWKyK5q/G8tRAJE541ltR2XXZp5Vi16gBYRnV8c27iAw2Q\nULNDy/bZnHGktj16l1aOVcsAYn2Wn5f4QAMi1LItkbUr8PWrIFxM8aSvSD3ymLKN3LGMGtG/q9fA\nN66P/l24nfDCWXhRYWKDTrJQk0PwvLX4xnUQCVM8dwIprTqXaeM47HheVquOb4meY6GWHYmsWx4d\ngAMUbIFKPoNDt26dWbhwCUuWLKOoqIi33x5K3769dms3aNAfePzxZygo2F6ybtq0WaxcuQaA2bPn\nkZGRQXp6esJiD6qunQ+nTu1ayQ4j8CJ4wm7JUmUH4TF/B44xs5uB44HHgAHAGHf/cEcjd5/p7i/t\nemczSwVqArmx5ZZm9rmZTTezz8zs4H2sP8/MZprZNDP72szSgfuBC8xsqpnFc3D/s63Jy6dJvZ0v\nHI3rZrImL79Mm7ZZDfhs2gIAPp+2kC0FheRt2VamzYwlqygKh2neoO6BDzqJrFb9nQMewDdtwGrV\nK9Om6Kt3ST38eKrf/C8yLvojhR/vLJ+wug3JuOZBMn5zN6GD2yUs7mSyzHr4pg0ly755D332zXuk\ndjqO6tf/nYzz/kDhqFd33r9OQzKuuJ+MAXcQyq785U6hug2I5K4tWY7krsPqHFSmTeHwV0nt/mtq\nPvgKNa6/n4LBVafsZE8ssy6+udQ5lp+L1Sr7WlQ0ZiipHXqQcc0jVOt/E4WfvwFAqG5jcKfa2TeT\nMeBuUruektDYk6FZsyYsX77zW4KcnJVkZTUp06Zz58PIzm7Gxx9/Xu5++vc/nalTZ1JYWLU+9Ins\nTZUehLt7EXAb0cH4zbHlTkTLUvbmAjObSrQkpT6wo9D3X8DL7n4E8Brw5D7W3wuc4u5HAme6e2Fs\n3Y5M++BdH9jMrjWziWY28fkR3/60Az+ABvb/JZPm53DB315n4oIcGtXNJGQ7T7O1G7dw9yuf8OcB\nvQiFLImRBkPKYcdSNO1rtv3jRgreeIRq/X4HGJ6fx9Z//p6C5+6icOSrVOt/PaRXT3a4gZDS8RiK\nZnzLtqduoeDtx6l2xrWU9NnTt1Dw4r0UfvYG1c66DtIzkh1u0qV27UnR2E/ZctelbH3qXjIuvw1M\nz729SW3XneJZ31Hw3B/Z/t6TVDvtKsAglEIoqw3bR/yXgsGPkNL6KELN2yc73KQyM/72t7v505/+\nUm6bDh3a8Je/3M4NN9yRwMikonP3hN2SpUoPwmNOA1YCh+1po5m9F8tWv1tq9WB37ww0AWYQHcgD\nHAu8Hvv7FaLZ9b2tHw28ZGbXACk/Jlh3f9bdu7p716tOP37fd4ijRnUzWZW7uWR5dV4+jepmlm1T\nJ5MnrunL4D9dzI1nHAtQUvedv207Nz7zATf0PY4jDmmauMCTxDdvKJOVtNr1dys3Sevck/DssUD0\n4ktS06BGreiFXtui3zJEVi7Bc1cTOqhs9qky8vxcrHb9kmWrtYc+O/IEwnPGA9GLL0lJgxqZsT7b\nEl2/agmeu4ZQ/crdZ5G8dYTqNSxZDtVrUObbF4C0406hePLX0faL52JpaVjN2gmNM0g8Pw+rVeoc\ny6yHby5b0pR62PGEv58IQGTloug5Vj0Tz88lsnweFORDcSHhxTMINT44ofEn2ooVq8jO3vl6nZXV\nlJycVSXLtWpl0rFjO0aOfJO5c7+le/ejGDLkeY4++vBY+yYMHvwsV189kMWLf9ht/yJVWZUehMcu\nguwFHAPcYmZNgVlASRGqu/cHLiea8S7Dox+fhgK/+imP7+7XAXcDzYFJZnbQPu6SVJ0ObswPa/PI\nWbeRouIwn0yaxwmHH1qmTW7+NiKR6KfK50dOpN8xHQEoKg4z8L/D6Nu9A72OapPw2JMhkrOIUP0m\nWN2GEEohpdMxFM+bVLbNpvWkHBL9/GcNmkUH4Vs3RQfisWyl1W2I1W9CJHdNwo8h0SIrFhOq1xir\n0yDaZx16UDx/Stk2m9aT0jJ6XtlBTWN9thmq76HP8tbu9hiVSWTpPEKNmmEHNYaUVFK7nEDx9LFl\n2njuGlLaRWueQ02aQ2o6nr8xGeEGQmTVEqxuI6x29BxLbd+N8KJpZdr45vWEDu4AgNVvEj3Htm0m\nvGQWoQZZkJoOFiIlu23JDEaV1cSJ02jd+hBatGhOWloa5513BsOHjyrZvmnTZpo3P4r27Y+nffvj\nGT9+CueeexWTJ8+gTp3avPvui9xzz98YM2ZiEo9CKqKqUBNelWdHMaIXZt7s7j+Y2aNEa8KvBu4w\nszNL1YXX2MuujgcWxv7+DriQaLZ7APDN3tabWSt3HweMM7PTiA7GNwOBvGIjNSXE7ef15LdPv0/E\nnbOO6Ujrpgfx9PAxdDy4MT0PP5SJ85fz5NDRGEaX1lnccV5PAEZOmc/kBSvI21LAh+NmA3D/Jb1p\nn91wL49YwXmEwo9eImPAn8Ci0+352hzSep5DZMViwvMmUzjyNaqdcTWpPU4FoPCD/wdAysHtSe95\nLh4JR/cz4oXoRWCVnUcoHPUKGRfeFu2z6V/j63JI+2V/IiuXEF4wJVpqcvqVpHY7BXAKh/8XgJSD\n25H+y7PxSDG4U/jxS5W/zyIRCgb/hxo3/AVCKRSNGUlk5Q+k972U8NJ5hGeMY/s7/yVjwE2kn9gf\n3Cl45YmSu9d84KXoRZspqaQeeRzb/nVXmZlVKiWPUPjF61Q752Ywo3jmaHz9CtKOO5PIqqWEF02j\n8Ku3Se91GWldTgaHwk9ejN53+1aKJo8i4+K7ACe8eAaRxTOSejgHWjgc5pZb7mXo0P+RkpLCyy+/\nxZw587nnnoFMnjyd4cM/Lfe+1133G1q1askdd9zEHXfcBMAZZ1zK2rXry71PVXDboL8yYcp08vI2\ncVK/S/jdVZdyzhmV//oC2Z0lsxYmmczsWuAkd78gtpwCTABuAVYDTwDtY39vBh5x9093maIwBCwH\nLnf3NWbWAngRaACsBa6IDfDLW/8u0AYw4DPgZqAe8AnR6Q8f3lNd+A7bRj5dNf/xfqLI2NHJDqHi\nSauyn9N/ksiyyv9tRbyltM5OdggVykF3jUx2CBXOpmVfJDuECietwaFJv3Akq16nhI1xcnJnJeV4\nq+w7rLs/S3RKwh3LYUqVoVDONIGxWVJeKmfbUuDE/Vh/9h52swHotof1IiIiIlJJVNlBuIiIiIgE\nU6QKVGpU6QszRURERESSQYNwEREREZEEUzmKiIiIiASKJ3HqwERRJlxEREREJMGUCRcRERGRQKkK\nU2grEy4iIiIikmDKhIuIiIhIoCTz5+QTRZlwEREREZEEUyZcRERERAJFNeEiIiIiIhJ3yoSLiIiI\nSKDoZ+tFRERERCTulAkXERERkUBRTbiIiIiIiMSdMuEiIiIiEiiaJ1xEREREROJOmXARERERCRTV\nhIuIiIiISNxpEC4iIiIikmAqRxERERGRQNGP9YiIiIiISNwpEy4iIiIigeKaolBEREREROJNmXAR\nERERCRTVhIuIiIiISNwpEy4iIiIigaIf6xERERERkbhTJlxEREREAkWzo4iIiIiISNwpEy4iIiIi\ngaKacBERERERiTurCp80JLHM7Fp3fzbZcVQk6rP9o/7af+qz/aP+2n/qs/2j/tq7tPSshA1Qiwpz\nLFGPVZoy4XIgXJvsACog9dn+UX/tP/XZ/lF/7T/12f5Rf1VxqgkXERERkUCpCnUayoSLiIiIiCSY\nMuFyIKjGbf+pz/aP+mv/qc/2j/pr/6nP9o/6ay+Kk1SnnUi6MFNEREREJMFUjiIiIiIikmAahIuI\niIiIJJgG4SIiIiIiCaZBuPxsZnbIj1kn8nOY2Xk/Zp3IT2Vmv/gx60R+Kr2OSWkahEs8vLOHdUMS\nHkUFY2bVzcxif7cys9PNTDMWle+OH7lOiA4ezWyUmc0zs0VmttjMFiU7roD7149cJ/JT6XVMSugN\nX34yM2sPdALqmNnZpTbVBjKSE1WF8g3wKzOrA3wOTAYuBC5LalQBY2anAacDWWb2ZKlNtYHi5ERV\nITwP3AJMAsJJjiXQzOxY4DigoZkNLLWpNpCSnKiCy8yml7cJcHc/IpHxVAR6HZM90SBcfo52QF+g\nLnBGqfWbgWuSElHFEnL3rWZ2JfAfd/+rmU1NdlABtAKYCJxJdEC5w2aig0zZs43u/lGyg6gg0oFM\nou+JtUqt3wScm5SIgi1C9AcNXweGAtuSG06FoNcx2Y3mCZefzcyOdfcxyY6jookNuK8BngSucfeZ\nZjbD3Q9PcmiBZGZp7l4U+7se0Nzdy8vIVXlm9leiWdx3ge071rv75KQFFXBm1sLdl8b+DgGZ7r4p\nyWEFUuyb0IuIJmBmEx2Qj3R3ZXX3Qq9jUppqwiUe+ptZbTNLM7PPzGytmV2S7KAqgIHAn4FhsQH4\noURLVGTPRsXOs/pES3eeM7O/JzuoAOsBdAUeAh6P3R5LakTB93DsHKsJzARmm9ltyQ4qiNx9rrsP\ncvejiWbD/4cyuj+GXsekhDLh8rOZ2VR372xm/YmWpwwEvnb3I5McWoVgZtXcffu+W1ZtZjbF3Y8y\ns6uJZo8Gmdl01Z9KvJR6LRsAHA3cDkzSObY7M8sieg1LfyAXeAt4z93zkxpYwOl1TEpTJlziIS32\n/z7A2+6+MZnBVBRm1t3MZgDzY8tHmplmYihfqpk1Bc4HhiU7mKAzszpm9oSZTYzdHo9dBCzlSzOz\nNKAf8GGsbECZql2Y2VdEs99pwBXAb4DhQHoswyvl0+uYlNAgXOJhqJnNBboAn5lZQ6AgyTFVBE8S\n/eZgPYC7TwN+ndSIgu1+4BNgobtPiJXvzE9yTEH2AtGLvs6P3TYBLyY1ouD7f8ASoCbwtZm1INpv\nUlYLoB7wf0SfkxNjt0mx/0v59DomJVSOInERy35sdPewmdUAarv7qmTHFWRmNt7du+/4ejK2bprK\neCQedpRW7Gud7J2ZpepiQxE5EJQJl58t9vXtJcBgMxsCXEUsuyt7tczMugNuZilmdjMwL9lBBZWZ\nZZvZe2a2JnZ7x8yykx1XgG0zs+N3LMR++VFTye2FmTU2s+fN7KPYckeipRZSipnNNrO7Yllc2Q96\nHZPSNAiXePgP0VKUp2O3o2PrZO9+S/Qi1oOB1cAxsXWyZy8CHwLNYrehqLxib34LPGVmS8xsKfBv\n4LokxxR0LxEtFWgWW54H3Jy0aILrIqLzqo8ys/Fmdsv/b+/eoz2f6z2OP18GEQaVREKIyv0ypXJc\nDydxpEQ5lHQ4TlGinFLndLooquMsJ5cu5FZ0KEVHrjFSJDRMw1qUyKWUCGkkzLzOH9/Pb/xmz957\nLnvP7/P9/fbrsdZv7d/n+5u91mvNmv2dz/5+39/3W9Lq8/umAHIeiy4pR4kxG66EImUVMd5SXrFo\nJE0GSL/r+ZN0k+0pQ0rE8m9sFJK2Bt4O7AX8GjjX9ql1U7VXzmPRLVfCYzzMkrRuZ1FuUWZM9nxI\nWk/S5ZKml/Umko6unavFHpG0fyndmVR60afsaYhOj35JR5YR7AcBB3WtY2QzJb2Q0hGlbDDT7WkU\ntm+wfQTwLprpySdVjtR2OY/FHNmEx3g4Cpgq6ZrSuupq4EOVM/WD02iG9cwu6xk0tfUxvPfQdPn4\nfXm9jaY9WsxtufJ1hRFeMbIjaUoF1pV0Hc0AmvfXjdRekqaUNpj3Ap+k6S6TspTR5TwWc6QcJcaF\npOcBG5TlnRk+M3+59R3RHmrG1G8N3EhzLhPNueyZqsFaSNLnaDaSjwL/C5xn+4G6qSL6T66Ex5iV\n7iiHAJ8or4PLsRjdI5JeznO3vvekuTISw0hXgYUj6QtqxmMvJekqSX/slKrEvGzPBk62/azt223f\nlg34iJ4CDrQ9xfbxwI6SLpL0pQzrGV3OY9Etm/AYD0O7o2xJuqMsiMOArwOvLLdzP0q6V4wmXQUW\nzi7lYczdaQbQrEdTOhYju0rSXpJUO0jL7QncDiBpW+A4mtKdx4GvVczVD3IeizlSjhJjlu4oC0/S\nJGBP2xeUUeKy/VjtXG2WrgILR9JttjeSdBrwHduX5edydJKeoKmpn0XTU12AbU+uGqxlun/uJJ0M\n/NH2J4d+FvPKeSy65Up4jId0R1lItmcBHyvvH88GfIGkq8DCuVjSHTR3pq6StApNGUGMwPYKtpew\nvZTtyWWdDfi8lpS0ZHm/E83D+HM+q5Cnn+Q8FnPkSniMmaSdaG6n3U1z5WgtmnrBqVWDtZykY2mG\n9JwHzOwcTz/n4UlaCzgReB1NHf31wAds31c1WIuV+tzHbc+S9Hxgsu08dzAKSXsA25blNbYvrpmn\njSR9HHgT8DDNsLEtbFvSesBZtt9QNWCL5TwW3bIJj3GR7igLT9L9XUvz3K3vNStFigEiaW/gMttP\nSPp3mkm2x9ieVjlaa0k6DpgCnFMO7QvcbDv9+4coPdRXA66wPbMcWx9YPv/GIhZMNuExJuW3+pm2\nHy4n5W2Au2xfWDlaa0na2vYNtXP0C0nL0Ezke5TmIaajaK5U/hr4jO2HK8ZrLUm/sL2JpG2AY4Av\nAp+w/drK0VpL0i+AzUqnlM6zG7fY3qRusuh3OY/FcFITHotM0n/Q1ALeIOkY4ATgRcDhkk6oGq7d\nTqkdoM+cDexCM+TiGppyp5OAJ4Azq6Vqv85zGbsBX7P9A2Dpinn6xUpd71esliIGTc5jMY88QBFj\nsS/wKuD5wH3AS2w/WR7YubVqshgkry5dPpYEHrC9XTl+maTpNYO13G8lfRXYGfh8KRnLhZfRHQvc\nImkqTXnYtjStQyPGKuexmEc24TEWT9l+Gnha0q9tPwlg+1lJT1fO1mbrSPr+SB/a3qOXYfrA0zDn\n39XvhnyWLjwj2wd4I/Bfth+TtBrpEz4q29+SdA1NXTjAR/Iga4yTnMdiHtmEx1isJOmtNFeMJpf3\nlHVu447sj8DxtUP0kTUkfYnm31XnPWX90nqx2q3clXqI5jmNXwHPlq8xhKTDbJ9Uli+wPeIvyRGL\nKOexmEcezIxFJmnUKV+2D+xVln4iaZrtLWrn6BeSDhjtc9tn9SpLP5H0n8BWwAa215e0OvDttI+b\nV/fPZH4+Y3HIeSyGkyvhsciyyV5kv6kdoJ90/nOStLftb3d/VtrwxfDeAmwOTAOw/TtJK9SN1Bcy\nsj7GXTbZMZxswmNcSNoN2BBYpnPM9qfrJWov252yHSRtBLyauf/ezq6Rqw8cDXx7AY5F4+kyQMUA\nkparHajFVpL0FpoHV7tL6wCw/d06sWJQlFah63TO75K+A7ygfHyM7atH/OYYWNmEx5hJ+gpNh5Qd\ngNOAtwE3Vg3VB0q5wPY0m/BLgF2Bn9C0sopC0q400/le2lVHCTCZps45hnd+6Y6ykqSDaVqjnVo5\nU1v9COg8EH0t8I9dnxnIJjzG6lPA+7vWGwDvBpYDPkbT7jcmmNSEx5h1DQXpfF0euNT239XO1maS\nZgCb0gwD2VTSqsA3be9cOVqrSNoU2Az4NPCJro+eAKbafrRKsD4gaWea3sQCLrd9ZeVIEROSpJts\nT+laf7dzV1TSdXlWY2LKlfAYD38tX58sD389QjPOOEb3V9uzJT0raTLwEPCy2qHaxvZ0YLqkc20/\nUztPPyiTHn9oewcgG+8FJGkl4F3A2nT9/2j7A7UyxcDoHgI1V1kisGqPs0RLZBMe4+E3XaQyAAAL\njElEQVTi8p/XF2keAjNNWUqM7uby93Yq8HPgL8BP60ZqtddI+iTNpLklaa7u2vY6VVO1kO1ZkmZL\nWtH247Xz9JFLgBuAGcDsyllisNwhabcyuXYOSbsDd1bKFJWlHCXGVZnKt0z+4x+dJAFr2L6/rNcG\nJtv+Rc1cbSbpDuAIml9Y5gy3sP1ItVAtJukimu4oVwIzO8dzVXdkaU8Yi4ukVwAXA9dTOhYBWwKv\nB3a3/cta2aKebMJjkQ3tIDBUOgqMTtIM2xvXztEvJP3M9mtr5+gXI/UlTqu0kUk6guaO1MXA3zrH\nbf+pWqgYCJLWBP4A7EfTSQzgduBcYIrtH9fKFvVkEx6LrGtYz4tpfpvvPN29A3C97d2rBOsTks4C\nTrJ9U+0s/UDSccAkmk4V3RukaSN+U8RCkHQo8FngMZqyOkjJU4wDSXcDXwGOtz2rHFuVZnryK21v\nVTNf1JFNeIyZpCuAA2w/WNarAWfa/oe6ydqtlFesB9xLUy7QqXHepGqwlpI0dZjDtr1jz8P0gdJ9\nZ+gJ/nHgZpq+xCnjGaJslF5j++HaWWKwSFoZOI7mgtXhwMbAkcAXgC/bzjMIE1AezIzx8LLOBrz4\nA83DczG6/JKyEEqnj1hwl9LUzp9b1u+g6ef/e+BM5u6FHY27gCdrh4jBU1qpHiLpcOCHwO+ArW0/\nUDdZ1JRNeIyHqyRdDnyrrN8OXFExT784xvY7uw9I+gbwzhH+/IRWbt1+Dljd9q6SXg28zvbXK0dr\nq78f8pDhjM6Dh5L2r5aq3WYCt5a7Lt0lT3mYNcakdML6PPBa4I00A8gulXR4pmVOXNmEx5jZPqyM\nfN62HLoeeEnFSP1iw+5F6e28ZaUs/eBM4Azg42X9S+A8IJvw4U2S9BrbNwJImkJTUw+ZNDqSC8sr\nYrxNA04BDrX9LHCFpM2AUyTda3vfuvGihmzCY7z8hqbWbW/gHuCCqmlaTNLRNGOKl5X0585h4Gky\nVnw0L7J9fvn7w/azkmbN75smsIOA08sEW2gmjB4kaTng2Hqx2sv2WZKWBtYvh+7MgKgYJ9sOLT2x\nfSvwekkHV8oUleXBzFhkktYH9i2vh2muSn7YdurBF4CkY20fXTtHv5B0DbAXcGUpqdga+Lzt7eom\nazdJKwKkd//8SdoeOIvmooJoJtgeYPvairEiYkDlSniMxR3Aj2kGDdwFc/rsxoK5q3tRylH+3fan\nKuVpuyOB7wPrSroOWAV4W91I7ZUa+kVyPLCL7TthzoWGb5EysYhYDJaoHSD62luBB4Gpkk6VtBPN\n1aNYMDtJukTSapI2ohmXvULtUG1V+oFvR1P2dAiwYSaMjupM4HJg9bL+JfDBamn6w1KdDThAmWK4\nVMU8ETHAUo4SY1ZqTN9MU5ayI3A28D3b6ZAyH5LeDpxM05Xhn2xfVzlS60ja0fbVI01ozWTW4Um6\nyfYUSbfY3rwcu9X2ZrWztZWk04HZwDfLof2ASbbfUy9VRAyqlKPEmNmeSdOL+NwykGBv4COkTeGo\nJL2CZmjDBcCrgHeWDVP6FM9tO5pprMP1tTbNBM2Y10xJL6QM7Ck19KkLH917gUOBTkvCH9N0tIiI\nGHe5Eh5RSZmYeajtqySJpub5PbY3nM+3RsyXpC2AE4GNgNtoauj3tj29arCIiACyCY+oRtJk238e\ncmz9UocahaQjR/vc9n/3Kku/kbQksAHNsxpptzcCSTModwyGY3uTHsaJiAkiD2ZG9JikfwOw/WdJ\new/5+N29T9R6K5TXVjTlAi8tr38Fthjl+yY828/avt32bcD2kq6snamldqcpd7qsvPYrr0uBSyrm\niogBlivhET3WGR0+9P1w63iOpGuB3Ww/UdYrAD+wve3o3zmxSNoR+ApNV5QLaUZln0FzNfyzeZB1\nZN0PsXYdy89kRCwWuRIe0Xsa4f1w63jOqjRTRTueLsdibscD/wK8EPgO8FPgTNtbZgM+X5L0hq7F\n68n/kxGxmKQ7SkTveYT3w63jOWcDN0r6XlnvSTPdMOZm29eU9xdK+q3tk2oG6iP/DJxepowKeBRI\ne8KIWCxSjhLRY5Jm0fQFF7As0GlJKGAZ2xkOMgJJWwLblOW1tm+pmaeNJN0NfLjr0BeBozqLXA2f\nv7IJx3ZaOkbEYpNNeET0FUkvBpbprG3fVzFO60g6Y5SPncEzI5P0PGAvYG267hTb/nStTBExuFKO\nEhF9QdIeNPXOqwMPAWsCdwDpq97F9oG1M/Sxi2gGGv0c+FvlLBEx4LIJj4h+8Rlga+CHtjeXtAOw\nf+VMMVjWsP3G2iEiYmLIU98R0S+esf0IsISkJWxPpekdHjFerpe0ce0QETEx5Ep4RPSLxyQtD1wL\nnCPpIZoHXCPGyzbAuyXdQ1OOIpo6+kzMjIhxlwczI6IvSFoO+CvNHbz9gBWBc8rV8RhC0qE0fz+P\nlfXKwL62T6mbrL0krTXccdv39jpLRAy+bMIjovUkTaKpBd+hdpZ+IelW25sNOTbPRMiYVzrwREQv\npCY8IlrP9ixgdqd/cyyQSZLmTGAtv8gsXTFP60naQ9KvgHuAHwG/AS6tGioiBlZqwiOiX/wFmCHp\nSrpqwW1/oF6kVrsMOE/SV8v6kHIsRpYOPBHRMylHiYi+IOmArmXnxCXbGV0/DElL0Gy8dyqHrgRO\nK3cVYhiSbra9laTpwOa2Z0uabnvT2tkiYvDkSnhEtJqkN9P0bz65rG8EVqHZiH+kZrY2sz0b+HJ5\nxYJJB56I6JlcCY+IVpN0HfAO2/eX9a3AjsDywBm2dxrt+ycaSefb3kfSDJ67YzBH2u2NLB14IqKX\nciU8Itpu6c4GvPiJ7T8Bfyqbppjb4eXr7lVT9CHbnaves4GzSknPvsA59VJFxKBKd5SIaLuVuxe2\nD+tartLjLK1n+8Hy9n227+1+Ae+rma2tJE2WdLSkkyTtosZhwN3APrXzRcRgyiY8ItruZ5IOHnpQ\n0iHAjRXy9Iudhzm2a89T9IdvABsAM4CDgKnA3sCett9cM1hEDK7UhEdEq5XBKRfSjBGfVg5vCTyP\nZpP0h1rZ2kjSe2mueK8L3NX10QrAdbbTcm8ISTNsb1zeTwIeBNa0/VTdZBExyLIJj4i+IGlHYMOy\nvN321TXztFUZaLQycCzw0a6Pnii19DGEpGm2txhpHRGxOGQTHhExgCStCzxg+2+Stgc2Ac62/Vjd\nZO0jaRbPtSIUsCzwZHlv25NrZYuIwZVNeETEACqtHLcC1gYuAS4CNrT9ppq5IiKikQczIyIG02zb\nzwJvBU60fRSwWuVMERFRZBMeETGYnpG0L/Au4OJybKmKeSIioks24RERg+lA4HXAZ23fI+nlNK34\nIiKiBVITHhERERHRYxlbHxExQCSdb3sfSTOAea6y2N6kQqyIiBgiV8IjIgaIpNVsPyhpreE+L+Pr\nIyKismzCIyIiIiJ6LOUoEREDSNITzFuO8jhwM/Ah23f3PlVERHRkEx4RMZhOAB4AzqWZ/PgOYF1g\nGnA6sH21ZBERkXKUiIhBJGm67U2HHLvV9mbDfRYREb2VPuEREYPpSUn7SFqivPYBniqf5epLRERl\nuRIeETGAJK0D/A/NwB6AnwJHAL8FtrT9k1rZIiIim/CIiIiIiJ5LOUpExACStIak70l6qLwukLRG\n7VwREdHIJjwiYjCdAXwfWL28/q8ci4iIFkg5SkTEAOp0QpnfsYiIqCNXwiMiBtMjkvaXNKm89gce\nqR0qIiIauRIeETGAJK0FnEjTHcXA9cD7bd9fNVhERADZhEdETBiSPmj7hNo5IiIim/CIiAlD0n22\n16ydIyIiUhMeETGRqHaAiIhoZBMeETFx5NZnRERLLFk7QEREjB9JTzD8ZlvAsj2OExERI0hNeERE\nREREj6UcJSIiIiKix7IJj4iIiIjosWzCIyIiIiJ6LJvwiIiIiIgeyyY8IiIiIqLH/h+LDkXRUPLt\nGQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0a15291710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# correlation between model predictions on training data\n",
    "base_predictions_train = pd.DataFrame( {'RandomForest': rf_best.predict(X_train_[model_cols]),\n",
    "     'ExtraTrees': et_best.predict(X_train_[model_cols]),\n",
    "     'AdaBoost': ada_best.predict(X_train_[model_cols]),\n",
    "     'SVM' : svc_best.predict(X_train_[model_cols]),\n",
    "     'GradientBoost': gb_best.predict(X_train_[model_cols]),\n",
    "     'Logistic Regression': log_best.predict(X_train_[model_cols]),\n",
    "     'XGBoost': xgbm_best.predict(X_train_[model_cols])\n",
    "    })\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "foo = sns.heatmap(base_predictions_train.corr(), vmax=1.0, square=True, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=5, min_samples_split=2,\n",
       "            min_weigh...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard',\n",
       "         weights=[2, 1, 1, 2, 1, 3, 1])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Voting classifier\n",
    "clf_vote = VotingClassifier(\n",
    "    estimators=[\n",
    "        #('tree', clf_tree),\n",
    "        ('rf', rf_best),\n",
    "        ('et', et_best),\n",
    "        ('ada', ada_best),\n",
    "        ('gb', gb_best),\n",
    "        ('xgb', xgbm_best),\n",
    "        ('svm', svc_best),\n",
    "        ('logistic', log_best)\n",
    "        ],\n",
    "    weights=[2,1,1,2,1,3,1],\n",
    "    voting='hard')\n",
    "clf_vote.fit(X_train_[model_cols], y_train_['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8995433789954338"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_vote_score_train = right_classification(y_pred=clf_vote.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "clf_vote_score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81707317073170727"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_vote_score_test = right_classification(y_pred=clf_vote.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "clf_vote_score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2114  861]\n",
      " [  30  134]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test['lapsed_next_period'], clf_vote.predict(X_test[model_cols])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 776 2199]\n",
      " [   4  160]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test['lapsed_next_period'], svc_best.predict(X_test[model_cols])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
