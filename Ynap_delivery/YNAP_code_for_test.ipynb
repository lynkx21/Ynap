{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOOX Net-A-Porter - Technical Exercise\n",
    "\n",
    "The purpose of this exercise is to try to identify customers at risk of lapse in order to take actoin before they stop to shop.\n",
    "\n",
    "#### This is the code for the final test. \n",
    "I'll skip most of the part of the analysis that can be found on the document.\n",
    "Here I'll import the final model I've saved there and it will run on the data provided here.\n",
    "\n",
    "Given all the considerations done in the document, I'm setting the code as if there are two input datasets, one with the customer table *account* and one with the *transactions* with the same columns as in the beginning.\n",
    "\n",
    "Please check the folder where I'm loading the data (*ynap_data*), thanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve\n",
    "from sklearn.metrics import make_scorer, confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "import xgboost as xgb\n",
    "\n",
    "# data in .../ynap_data\n",
    "data_path = os.path.join(os.getcwd(), 'ynap_data')\n",
    "df_acc = pd.read_csv(os.path.join(data_path, 'account_test.csv'), sep=',')\n",
    "df_trans = pd.read_csv(os.path.join(data_path, 'transactions_test.csv'), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering on transactions\n",
    "df_trans['order_date'] = df_trans['order_date'].apply(lambda x: dt.datetime.strptime(x, '%Y-%m-%d'))\n",
    "df_trans['order_quarter'] = df_trans['order_date'].apply(\n",
    "    lambda x: str(x.date().year) + str((x.date().month-1)//3+1).zfill(2))\n",
    "df_trans['item_returned'] = df_trans['net_spend'].apply(lambda x: 1 if x == 0 else 0)\n",
    "df_trans['item_bought'] = df_trans['net_spend'].apply(lambda x: 0 if x == 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# column finder is a function that finds all the columns of a dataframe for a given substring\n",
    "def column_finder(df, col, method=2):\n",
    "    if method == 1:\n",
    "        # method 1:\n",
    "        cols_as_string = ' '.join(df.columns.values)\n",
    "        cols_found = list(re.findall(col + '.*?\\ ', cols_as_string))\n",
    "        cols_found = [x.strip(' ') for x in cols_found]\n",
    "    elif method == 2:\n",
    "        # method 2:\n",
    "        cols_found = []\n",
    "        for col_elem in df.columns.values:\n",
    "            if col in col_elem:\n",
    "                cols_found.append(col_elem)\n",
    "    else:\n",
    "        raise ValueError\n",
    "            \n",
    "    return cols_found\n",
    "\n",
    "# get_piv_counts_and_sums performs all the aggregations and the calculation of the average metrics\n",
    "# if necessary, it performs a transposition of the table.\n",
    "def get_piv_counts_and_sums(df, grouping_columns='customer_id'):\n",
    "    \n",
    "    dist_count_cols = ['order_id', 'product_id', 'product_type_id', 'designer_id']\n",
    "    count_cols = ['var1', 'var2']\n",
    "    sum_cols = ['gross_spend', 'net_spend', 'item_bought', 'item_returned', 'var1', 'var2']\n",
    "    \n",
    "    # maybe not necessary\n",
    "    if type(grouping_columns) != list:\n",
    "        glist = []\n",
    "        glist.append(grouping_columns)\n",
    "    else:\n",
    "        glist = grouping_columns\n",
    "   \n",
    "    get_dist_counts = df.groupby(glist)[dist_count_cols].nunique().reset_index()\n",
    "    get_counts = df.groupby(glist)[count_cols].count().reset_index()\n",
    "    get_sums = df.groupby(glist)[sum_cols].sum().reset_index()\n",
    "    get_sums = get_sums.rename(columns = {'var1': 'var1_sum', 'var2': 'var2_sum'})\n",
    "    get_sums['quote_spend_returned'] = (get_sums['gross_spend'] - \n",
    "                                        get_sums['net_spend']) / get_sums['gross_spend']\n",
    "    \n",
    "    counts_and_sums = pd.merge(left=get_dist_counts, right=get_sums, how='inner', on=glist)\n",
    "    counts_and_sums = pd.merge(left=counts_and_sums, right=get_counts, how='inner', on=glist)\n",
    "    counts_and_sums['quote_var1'] = counts_and_sums['var1_sum'] / counts_and_sums['var1']\n",
    "    counts_and_sums['quote_var2'] = counts_and_sums['var2_sum'] / counts_and_sums['var2']\n",
    "    counts_and_sums.drop(['var1_sum', 'var1', 'var2_sum', 'var2'], axis=1, inplace=True)\n",
    "    \n",
    "    if len(glist) > 1:\n",
    "        glist.pop(glist.index('customer_id'))\n",
    "        if len(glist) == 1:\n",
    "            glist = glist[0]\n",
    "        \n",
    "        cs_pivot = counts_and_sums.pivot(index='customer_id', columns=glist).fillna(0)\n",
    "        csp_new_cols = []\n",
    "        for col in cs_pivot.columns.get_values():\n",
    "            csp_new_cols.append('_'.join(col))\n",
    "                \n",
    "        cs_pivot.columns = csp_new_cols\n",
    "        cs_pivot = cs_pivot.reset_index()\n",
    "        \n",
    "        for val in counts_and_sums[glist].unique():\n",
    "            #  relative metrics for time based tables\n",
    "            cs_pivot['ns_per_order_' + str(val)] = cs_pivot['net_spend_' + str(val)] / cs_pivot['order_id_' + str(val)]\n",
    "            cs_pivot['gs_per_order_' + str(val)] = cs_pivot['gross_spend_' + str(val)] / cs_pivot['order_id_' + str(val)]\n",
    "            cs_pivot['ib_per_order_' + str(val)] = cs_pivot['item_bought_' + str(val)] / cs_pivot['order_id_' + str(val)]\n",
    "            cs_pivot['ir_per_order_' + str(val)] = cs_pivot['item_returned_' + str(val)] / cs_pivot['order_id_' + str(val)]\n",
    "            cs_pivot['ns_per_ib_' + str(val)] = cs_pivot['net_spend_' + str(val)] / cs_pivot['item_bought_' + str(val)]\n",
    "            cs_pivot['gs_per_item_' + str(val)] = cs_pivot['gross_spend_' + str(val)] / (cs_pivot['item_bought_' + str(val)]\n",
    "                                         + cs_pivot['item_returned_' + str(val)])\n",
    "            \n",
    "        return cs_pivot.fillna(0)\n",
    "    \n",
    "    else:\n",
    "        # relative metrics for tot table\n",
    "        counts_and_sums['ns_per_order'] = counts_and_sums['net_spend'] / counts_and_sums['order_id']\n",
    "        counts_and_sums['gs_per_order'] = counts_and_sums['gross_spend'] / counts_and_sums['order_id']\n",
    "        counts_and_sums['ib_per_order'] = counts_and_sums['item_bought'] / counts_and_sums['order_id']\n",
    "        counts_and_sums['ir_per_order'] = counts_and_sums['item_returned'] / counts_and_sums['order_id']\n",
    "        counts_and_sums['ns_per_ib'] = counts_and_sums['net_spend'] / counts_and_sums['item_bought']\n",
    "        counts_and_sums['gs_per_item'] = counts_and_sums['gross_spend'] / (counts_and_sums['item_bought']\n",
    "                          + counts_and_sums['item_returned'])\n",
    "            \n",
    "    return counts_and_sums.fillna(0)\n",
    "\n",
    "quarterly_stats = get_piv_counts_and_sums(df_trans, ['customer_id', 'order_quarter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the account table with the aggregations\n",
    "df_qua = pd.merge(left=df_acc, right=quarterly_stats, on='customer_id', how='inner')\n",
    "\n",
    "print('quarterly aggregations dataframe dimension: ({:,}, {:,})'.format(df_qua.shape[0], df_qua.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transformation to factors\n",
    "df_qua['var3'] = df_qua['var3'].astype(float).astype('category')\n",
    "df_qua['var5'] = df_qua['var5'].astype(float).astype('category')\n",
    "df_qua['var6'] = df_qua['var6'].astype(float).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transformation of columns\n",
    "def boxcox_func(x, l=1):\n",
    "    return np.log(x + l)\n",
    "\n",
    "def boxcox_trans(df, var_list):\n",
    "    for var in var_list:\n",
    "        if df[var].max() <= 1:\n",
    "            df[var] = df[var].apply(boxcox_func, l=0.001)\n",
    "        else:\n",
    "            df[var] = df[var].apply(boxcox_func)\n",
    "    return df\n",
    "\n",
    "skewed_cols = pd.read_csv('log_trans_cols.csv')\n",
    "skewed_cols = list(skewed_cols['col_name'])\n",
    "\n",
    "df_qua_log = boxcox_trans(df_qua, skewed_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to find the columns on a dataframe and remove them\n",
    "def column_remover(df, col_list, noprint=False):    \n",
    "    if type(col_list) != list:\n",
    "        cl = []\n",
    "        cl.append(col_list)\n",
    "        col_list =cl\n",
    "    \n",
    "    col_to_remove = []\n",
    "    for col in col_list:\n",
    "        #print(col)\n",
    "        col_to_remove.extend(column_finder(df, col, method=2))\n",
    "    \n",
    "    df.drop(col_to_remove, axis=1, inplace=True)\n",
    "    if not noprint:\n",
    "        print(col_to_remove)\n",
    "        \n",
    "\n",
    "columns_to_remove = ['product_id', 'designer_id', 'gross_spend', 'item_bought', 'item_returned',\n",
    "                     'ns_per_order', 'ir_per_order', 'gs_per_item', '_1989', '_199001', '_199002']\n",
    "column_remover(df_qua_log, columns_to_remove, noprint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make dummies out of categorical variables\n",
    "dummy_idx = np.where(df_qua_log.dtypes == 'category')[0]\n",
    "df_dummies = pd.get_dummies(df_qua_log.iloc[:, dummy_idx])\n",
    "df = pd.concat([df_qua_log.drop(df_qua_log.iloc[:, dummy_idx].columns.values, axis=1), df_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and cols to consider in the model\n",
    "model_cols = pd.read_csv('model_cols_list.csv')\n",
    "model_cols = list(model_cols['col_name'])\n",
    "\n",
    "filename = 'ynap_best_model.sav'\n",
    "svc_best = joblib.load(filename)\n",
    "svc_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_best.predict(df[model_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall measure\n",
    "def right_classification(y_true, y_pred):\n",
    "    cf = confusion_matrix(y_true, y_pred)\n",
    "    return cf[1, 1] / cf[1, :].sum()\n",
    "\n",
    "right_classification(df['lapsed_next_period'], svc_best.predict(df[model_cols]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
