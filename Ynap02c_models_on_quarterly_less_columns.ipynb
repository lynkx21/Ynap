{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve\n",
    "from sklearn.metrics import make_scorer, confusion_matrix\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/science/shared/ipythonNotebooks/leom/Kaggle/Ynap-master/ynap_data\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(os.getcwd(), 'ynap_data')\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.listdir(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tot = pd.read_csv(os.path.join(data_path, 'df_quarterly_log.csv'))\n",
    "df_tot['var3'] = df_tot['var3'].astype('category')\n",
    "df_tot['var5'] = df_tot['var5'].astype('category')\n",
    "df_tot['var6'] = df_tot['var6'].astype('category')\n",
    "\n",
    "# make dummies out of categorical variables\n",
    "dummy_idx = np.where(df_tot.dtypes == 'category')[0]\n",
    "df_dummies = pd.get_dummies(df_tot.iloc[:, dummy_idx])\n",
    "df = pd.concat([df_tot.drop(df_tot.iloc[:, dummy_idx].columns.values, axis=1), df_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15695 entries, 0 to 15694\n",
      "Columns: 113 entries, customer_id to var6_1.0\n",
      "dtypes: float64(103), int64(2), uint8(8)\n",
      "memory usage: 12.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the indications of the exploratory let's drop the variables we saw are highly correlated between each other.\n",
    "Also let's keep only the last 6 months of data: it's more likely that this is the period where a customer could potentially become a lapser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def column_finder(df, col, method=2):\n",
    "    if method == 1:\n",
    "        # method 1:\n",
    "        cols_as_string = ' '.join(df.columns.values)\n",
    "        cols_found = list(re.findall(col + '.*?\\ ', cols_as_string))\n",
    "        cols_found = [x.strip(' ') for x in cols_found]\n",
    "    elif method == 2:\n",
    "        # method 2:\n",
    "        cols_found = []\n",
    "        for col_elem in df.columns.values:\n",
    "            if col in col_elem:\n",
    "                cols_found.append(col_elem)\n",
    "    else:\n",
    "        raise ValueError\n",
    "            \n",
    "    return cols_found\n",
    "\n",
    "\n",
    "def column_remover(df, col_list, noprint=False):    \n",
    "    if type(col_list) != list:\n",
    "        cl = []\n",
    "        cl.append(col_list)\n",
    "        col_list =cl\n",
    "    \n",
    "    col_to_remove = []\n",
    "    for col in col_list:\n",
    "        #print(col)\n",
    "        col_to_remove.extend(column_finder(df, col, method=2))\n",
    "    \n",
    "    df.drop(col_to_remove, axis=1, inplace=True)\n",
    "    if not noprint:\n",
    "        print(col_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_remover(df, ['product_id', 'designer_id', 'gross_spend', 'item_bought', 'item_returned',\n",
    "                        'ns_per_order', 'ir_per_order', 'gs_per_item', '_1989', '_199001', '_199002'\n",
    "                       ], noprint=True)\n",
    "# print(df_tot.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance\n",
    "My objective is to get the highest recall possibile. The overall accuracy doesn't fit for this problem, because classifying the right 1s has more importance than classifying everyone correctly, whether they are 0s or 1s.\n",
    "The recall measure is perfect for this problem. I want to have the highest percentage possible on classifying the customers I know are true 1s (in other words, I want to minimize the outcome set to 0 to those I know are 1), while I consider less important to have an error on those I classify 1 when they are actually 0.\n",
    "\n",
    "Let's try different proportions of data between for not lapsed customers and lapsed customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_for_model = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15695, 27)\n",
      "(15695, 2)\n"
     ]
    }
   ],
   "source": [
    "X = df_for_model.drop(['customer_id', 'lapsed_next_period'], axis=1)\n",
    "y = df_for_model[['customer_id', 'lapsed_next_period']]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(657, 27)\n",
      "(11899, 27)\n",
      "0.05232558139534884\n"
     ]
    }
   ],
   "source": [
    "X_train_1 = X_train.loc[y_train[y_train['lapsed_next_period'] == 1].index]\n",
    "X_train_0 = X_train.loc[y_train[y_train['lapsed_next_period'] == 0].index]\n",
    "\n",
    "print(X_train_1.shape)\n",
    "print(X_train_0.shape)\n",
    "print(X_train_1.shape[0]/X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.052245938196877985"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[y_test['lapsed_next_period'] == 1].shape[0] / y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try different balance ratios with a couple of easy models\n",
    "def balance_tester(Xtr0, Xtr1, ytr, Xte, yte, zero_quotas = [0.5, 0.66, 0.75], seeds = [21, 2121, 1212], \n",
    "                   method='log', method_params=None):\n",
    "    \n",
    "    if (method == 'rf') & (method_params == None):\n",
    "        # Random Forest parameters\n",
    "        rf_params_bal = {'n_jobs': -1, 'n_estimators': 500, 'warm_start': True, \n",
    "                         'max_depth': 4, 'min_samples_leaf': 2, 'max_features' : 'sqrt',\n",
    "                         'verbose': 0}\n",
    "    \n",
    "    results = pd.DataFrame(columns=['quota', 'accuracy', 'pred_perc', 'recall'])\n",
    "    \n",
    "    for quota in zero_quotas:\n",
    "        accuracy = []\n",
    "        pred_perc = []\n",
    "        recall = []\n",
    "        \n",
    "        n_sample = int(Xtr1.shape[0] * quota / (1 - quota) // 1)\n",
    "        \n",
    "        for seed in seeds:\n",
    "            Xtr0_ = Xtr0.sample(n=n_sample, random_state=seed)\n",
    "            Xtr_ = Xtr0_.append(Xtr1)\n",
    "            ytr_ = ytr.loc[Xtr_.index]\n",
    "            \n",
    "            if method == 'log':\n",
    "                clf = LogisticRegression(max_iter=100, solver='liblinear')\n",
    "            elif method == 'rf':\n",
    "                clf = RandomForestClassifier(**rf_params_bal)\n",
    "            else:\n",
    "                return 'Method not recognized'\n",
    "            \n",
    "            clf.fit(Xtr_, ytr_)\n",
    "            clf_pred = clf.predict(Xte)\n",
    "            cf = confusion_matrix(yte, clf_pred)\n",
    "            \n",
    "            pred_perc.append(clf_pred.sum()/len(clf_pred))\n",
    "            accuracy.append(accuracy_score(yte, clf_pred))\n",
    "            recall.append(cf[1, 1] / cf[1, :].sum())\n",
    "            \n",
    "            \n",
    "        # print(np.array(accuracy).mean())\n",
    "        acc_mean = np.array(accuracy).mean()\n",
    "        pp_mean = np.array(pred_perc).mean()\n",
    "        rec_mean = np.array(recall).mean()\n",
    "        \n",
    "        results = results.append(pd.Series({'quota': quota, 'accuracy': acc_mean,\n",
    "                                            'pred_perc': pp_mean, 'recall': rec_mean}),\n",
    "                                 ignore_index=True)\n",
    "    \n",
    "    return results\n",
    "            \n",
    "quotas = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.825, 0.85, 0.9]\n",
    "# quotas = [0.8, 0.805, 0.81, 0.815, 0.82, 0.825, 0.83, 0.835, 0.84, 0.845, 0.85]\n",
    "balance_res_rf = balance_tester(X_train_0, X_train_1, y_train['lapsed_next_period'], X_test, \n",
    "                                y_test['lapsed_next_period'], quotas, method='rf')\n",
    "\n",
    "balance_res_log = balance_tester(X_train_0, X_train_1, y_train['lapsed_next_period'], X_test, \n",
    "                                y_test['lapsed_next_period'], quotas, method='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quota</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>pred_perc</th>\n",
       "      <th>recall</th>\n",
       "      <th>acc_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.735691</td>\n",
       "      <td>0.297866</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.786959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550</td>\n",
       "      <td>0.762345</td>\n",
       "      <td>0.266752</td>\n",
       "      <td>0.778455</td>\n",
       "      <td>0.772011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.781671</td>\n",
       "      <td>0.240629</td>\n",
       "      <td>0.713415</td>\n",
       "      <td>0.740717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.650</td>\n",
       "      <td>0.804502</td>\n",
       "      <td>0.210152</td>\n",
       "      <td>0.640244</td>\n",
       "      <td>0.705947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.700</td>\n",
       "      <td>0.831156</td>\n",
       "      <td>0.176702</td>\n",
       "      <td>0.575203</td>\n",
       "      <td>0.677585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.861739</td>\n",
       "      <td>0.133376</td>\n",
       "      <td>0.453252</td>\n",
       "      <td>0.616647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.800</td>\n",
       "      <td>0.890836</td>\n",
       "      <td>0.092386</td>\n",
       "      <td>0.339431</td>\n",
       "      <td>0.559993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.825</td>\n",
       "      <td>0.903791</td>\n",
       "      <td>0.073059</td>\n",
       "      <td>0.278455</td>\n",
       "      <td>0.528590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.850</td>\n",
       "      <td>0.913773</td>\n",
       "      <td>0.055007</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>0.486241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.900</td>\n",
       "      <td>0.947542</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quota  accuracy  pred_perc    recall  acc_recall\n",
       "0  0.500  0.735691   0.297866  0.821138    0.786959\n",
       "1  0.550  0.762345   0.266752  0.778455    0.772011\n",
       "2  0.600  0.781671   0.240629  0.713415    0.740717\n",
       "3  0.650  0.804502   0.210152  0.640244    0.705947\n",
       "4  0.700  0.831156   0.176702  0.575203    0.677585\n",
       "5  0.750  0.861739   0.133376  0.453252    0.616647\n",
       "6  0.800  0.890836   0.092386  0.339431    0.559993\n",
       "7  0.825  0.903791   0.073059  0.278455    0.528590\n",
       "8  0.850  0.913773   0.055007  0.201220    0.486241\n",
       "9  0.900  0.947542   0.000212  0.000000    0.379017"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_res_log['acc_recall'] = 0.4 * balance_res_log['accuracy'] + 0.6 * balance_res_log['recall']\n",
    "balance_res_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quota</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>pred_perc</th>\n",
       "      <th>recall</th>\n",
       "      <th>acc_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.748965</td>\n",
       "      <td>0.280132</td>\n",
       "      <td>0.778455</td>\n",
       "      <td>0.766659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550</td>\n",
       "      <td>0.769035</td>\n",
       "      <td>0.257513</td>\n",
       "      <td>0.754065</td>\n",
       "      <td>0.760053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.791335</td>\n",
       "      <td>0.232027</td>\n",
       "      <td>0.723577</td>\n",
       "      <td>0.750680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.650</td>\n",
       "      <td>0.809600</td>\n",
       "      <td>0.208665</td>\n",
       "      <td>0.674797</td>\n",
       "      <td>0.728718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.700</td>\n",
       "      <td>0.826165</td>\n",
       "      <td>0.182967</td>\n",
       "      <td>0.587398</td>\n",
       "      <td>0.682905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.843899</td>\n",
       "      <td>0.158649</td>\n",
       "      <td>0.524390</td>\n",
       "      <td>0.652194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.800</td>\n",
       "      <td>0.882978</td>\n",
       "      <td>0.105979</td>\n",
       "      <td>0.394309</td>\n",
       "      <td>0.589776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.825</td>\n",
       "      <td>0.945843</td>\n",
       "      <td>0.006584</td>\n",
       "      <td>0.044715</td>\n",
       "      <td>0.405166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.850</td>\n",
       "      <td>0.947223</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.004065</td>\n",
       "      <td>0.381328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.900</td>\n",
       "      <td>0.947754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quota  accuracy  pred_perc    recall  acc_recall\n",
       "0  0.500  0.748965   0.280132  0.778455    0.766659\n",
       "1  0.550  0.769035   0.257513  0.754065    0.760053\n",
       "2  0.600  0.791335   0.232027  0.723577    0.750680\n",
       "3  0.650  0.809600   0.208665  0.674797    0.728718\n",
       "4  0.700  0.826165   0.182967  0.587398    0.682905\n",
       "5  0.750  0.843899   0.158649  0.524390    0.652194\n",
       "6  0.800  0.882978   0.105979  0.394309    0.589776\n",
       "7  0.825  0.945843   0.006584  0.044715    0.405166\n",
       "8  0.850  0.947223   0.000956  0.004065    0.381328\n",
       "9  0.900  0.947754   0.000000  0.000000    0.379102"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_res_rf['acc_recall'] = 0.4 * balance_res_rf['accuracy'] + 0.6 * balance_res_rf['recall']\n",
    "balance_res_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic proportion of 50% for not lapsed values seems to lead to good results. Let's use that for the GridSearch step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1314, 27)\n",
      "(1314, 2)\n"
     ]
    }
   ],
   "source": [
    "quota = 0.5\n",
    "n_sample = int(X_train_1.shape[0] * quota / (1 - quota) // 1)\n",
    "X_train_0_ = X_train_0.sample(n=n_sample, random_state=101)\n",
    "\n",
    "X_train_ = X_train_0_.append(X_train_1)\n",
    "print(X_train_.shape)\n",
    "\n",
    "y_train_ = y_train.loc[X_train_.index]\n",
    "print(y_train_.shape)\n",
    "\n",
    "df_ = y_train_.join(X_train_)\n",
    "y_corr = df_.drop('customer_id', axis=1).corr().loc['lapsed_next_period']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "model_cols = list(X_train_.columns.values)\n",
    "print(len(model_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77701674277\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression for feature selection\n",
    "log_test = LogisticRegression()\n",
    "log_test = log_test.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "score_log = log_test.score(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "print(score_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ib_per_order_199004</td>\n",
       "      <td>[-0.0213764750621]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>var4</td>\n",
       "      <td>[-0.0210824772484]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>net_spend_199004</td>\n",
       "      <td>[-0.00102609225257]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>quote_var1_199003</td>\n",
       "      <td>[0.0133148643324]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>quote_var1_199004</td>\n",
       "      <td>[0.0417052177444]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0                    1\n",
       "14  ib_per_order_199004   [-0.0213764750621]\n",
       "0                  var4   [-0.0210824772484]\n",
       "6      net_spend_199004  [-0.00102609225257]\n",
       "9     quote_var1_199003    [0.0133148643324]\n",
       "10    quote_var1_199004    [0.0417052177444]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_features = pd.DataFrame(list(zip(X_train_[model_cols].columns, np.transpose(log_test.coef_))))\n",
    "log_features[(log_features[1] >= -0.05) & (log_features[1] <= 0.05)].sort_values(1, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['var4', 'net_spend_199004', 'quote_var1_199003', 'quote_var1_199004', 'ib_per_order_199004']\n"
     ]
    }
   ],
   "source": [
    "weak_predictors = list(log_features[(log_features[1] >= -0.05) & (log_features[1] <= 0.05)][0])\n",
    "#weak_predictors.remove('var6_0.0') # I can't remove this without the other categories of the variable\n",
    "print(weak_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "model_cols2 = [x for x in model_cols if x not in weak_predictors]\n",
    "print(len(model_cols2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.777777777778\n"
     ]
    }
   ],
   "source": [
    "# REPEAT Logistic Regression for feature selection\n",
    "log_test = LogisticRegression()\n",
    "log_test = log_test.fit(X_train_[model_cols2], y_train_['lapsed_next_period'])\n",
    "score_log = log_test.score(X_train_[model_cols2], y_train_['lapsed_next_period'])\n",
    "print(score_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>quote_var2_199004</td>\n",
       "      <td>[0.0264463613984]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0                  1\n",
       "8  quote_var2_199004  [0.0264463613984]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_features2 = pd.DataFrame(list(zip(X_train_[model_cols2].columns, np.transpose(log_test.coef_))))\n",
    "log_features2[(log_features2[1] >= -0.05) & (log_features2[1] <= 0.05)].sort_values(1, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.274609748327\n",
      "0.753424657534\n"
     ]
    }
   ],
   "source": [
    "# Random Forest example\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 200,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 4,\n",
    "    'min_samples_leaf': 4,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(**rf_params)\n",
    "rf.fit(X_train_[model_cols2], y_train_['lapsed_next_period'])\n",
    "\n",
    "rf_pred = rf.predict(X_test[model_cols2])\n",
    "\n",
    "print(rf_pred.sum()/len(rf_pred))\n",
    "\n",
    "print(accuracy_score(y_test['lapsed_next_period'], rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76829268292682928"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf = confusion_matrix(y_test['lapsed_next_period'], rf_pred)\n",
    "cf[1,1]/cf[1,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def right_classification(y_true, y_pred):\n",
    "    cf = confusion_matrix(y_true, y_pred)\n",
    "    return cf[1, 1] / cf[1, :].sum()\n",
    "\n",
    "def acc_recall_v2(y_true, y_pred):\n",
    "    cf = confusion_matrix(y_true, y_pred)\n",
    "    ac = accuracy_score(y_true, y_pred)\n",
    "    rc = cf[1, 1] / cf[1, :].sum()\n",
    "    return (0.45 * ac + 0.65 * rc) / (abs(rc-ac) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting multiple hyperparameters for every classifier we are going to implement\n",
    "\n",
    "# Random Forest\n",
    "rf_params_gs = {\n",
    "    'n_jobs': [-1],\n",
    "    'n_estimators': [100, 125, 150, 175, 200], #, 250, 300, 350, 400, 500],\n",
    "    'warm_start': [True], \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'min_samples_leaf': [4, 5, 6], #[2, 3]\n",
    "    'max_features' : ['sqrt'],\n",
    "    'verbose': [0]\n",
    "}\n",
    "\n",
    "# Extra Trees\n",
    "et_params_gs = {\n",
    "    'n_jobs': [-1],\n",
    "    'n_estimators': [600, 650, 700, 800],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'min_samples_leaf': [2, 3, 4],\n",
    "    'verbose': [0]\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params_gs = {\n",
    "    'n_estimators': [100, 125, 150, 175, 200],\n",
    "    'learning_rate': [0.03, 0.05, 0.07]\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params_gs = {\n",
    "    'n_estimators': [100, 125, 150, 175], # 200, 250, 300],\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'min_samples_leaf': [4, 5, 6],\n",
    "    'verbose': [0]\n",
    "}\n",
    "\n",
    "# SVC parameters\n",
    "svc_params_gs = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [1e-8, 5e-8, 1e-7, 5e-7, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 0.01, 0.05]\n",
    "}\n",
    "\n",
    "# Logistic regression parameters\n",
    "log_params_gs = {\n",
    "    'solver': ['liblinear'],\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'verbose': [0]\n",
    "}\n",
    "\n",
    "# XGBoosting parameters\n",
    "xgb_params_gs = {\n",
    "    'objective':['binary:logistic'],\n",
    "    'learning_rate': [5e-4, 1e-3, 2e-3], #0.01], #so called `eta` value\n",
    "    'max_depth': [2, 3, 4], #5, 6],\n",
    "    'min_child_weight': [11],\n",
    "    'silent': [1],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.7],\n",
    "    'n_estimators': [300, 400, 500], #number of trees, change it to 1000 for better results\n",
    "    'missing':[-999]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoring = {'acc_recall': make_scorer(acc_recall_v2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Istantiate the classifiers\n",
    "rf = GridSearchCV(RandomForestClassifier(), rf_params_gs, cv=5, scoring=scoring, refit='acc_recall')\n",
    "et = GridSearchCV(ExtraTreesClassifier(), et_params_gs, cv=5, scoring=scoring, refit='acc_recall')\n",
    "ada = GridSearchCV(AdaBoostClassifier(), ada_params_gs, cv=5, scoring=scoring, refit='acc_recall')\n",
    "gb = GridSearchCV(GradientBoostingClassifier(), gb_params_gs, cv=5, scoring=scoring, refit='acc_recall')\n",
    "svc = GridSearchCV(SVC(), svc_params_gs, cv=5, scoring=scoring, refit='acc_recall')\n",
    "log = GridSearchCV(LogisticRegression(), log_params_gs, cv=5, scoring=scoring, refit='acc_recall')\n",
    "xgbm = GridSearchCV(xgb.XGBClassifier(), xgb_params_gs, cv=5, scoring=scoring, refit='acc_recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier_runner(clf, Xtr, ytr, Xte, yte):\n",
    "    print('-'*40)\n",
    "    print(clf.estimator)\n",
    "    print('-'*40)\n",
    "    clf.fit(Xtr, ytr)\n",
    "    clf.best_params_\n",
    "    print(clf.best_params_)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "#     means = clf.cv_results_['mean_test_score']\n",
    "#     stds = clf.cv_results_['std_test_score']\n",
    "#     for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "#         print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std*2, params))\n",
    "        \n",
    "    print()\n",
    "    y_true, y_pred = yte, clf.predict(Xte)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(accuracy_score(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(right_classification(y_true, y_pred))\n",
    "    \n",
    "    return clf, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_cols = model_cols2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------------------------------------\n",
      "{'n_estimators': 150, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'warm_start': True, 'verbose': 0, 'n_jobs': -1, 'max_depth': 4}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.74      0.84      2975\n",
      "          1       0.14      0.78      0.24       164\n",
      "\n",
      "avg / total       0.94      0.74      0.81      3139\n",
      "\n",
      "0.742274609748\n",
      "[[2202  773]\n",
      " [  36  128]]\n",
      "0.780487804878\n"
     ]
    }
   ],
   "source": [
    "rf_clf, rf_ypred = classifier_runner(rf, X_train_[model_cols], y_train_['lapsed_next_period'],\n",
    "                                      X_test[model_cols], y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------------------------------------\n",
      "{'n_estimators': 650, 'n_jobs': -1, 'max_depth': 5, 'verbose': 0, 'min_samples_leaf': 3}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.73      0.84      2975\n",
      "          1       0.14      0.77      0.24       164\n",
      "\n",
      "avg / total       0.94      0.74      0.81      3139\n",
      "\n",
      "0.736858872252\n",
      "[[2186  789]\n",
      " [  37  127]]\n",
      "0.774390243902\n"
     ]
    }
   ],
   "source": [
    "et_clf, et_ypred = classifier_runner(et, X_train_[model_cols], y_train_['lapsed_next_period'],\n",
    "                                      X_test[model_cols], y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "----------------------------------------\n",
      "{'n_estimators': 175, 'learning_rate': 0.07}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.74      0.84      2975\n",
      "          1       0.14      0.80      0.25       164\n",
      "\n",
      "avg / total       0.94      0.74      0.81      3139\n",
      "\n",
      "0.741000318573\n",
      "[[2194  781]\n",
      " [  32  132]]\n",
      "0.80487804878\n"
     ]
    }
   ],
   "source": [
    "ada_clf, ada_ypred = classifier_runner(ada, X_train_[model_cols], y_train_['lapsed_next_period'],\n",
    "                                        X_test[model_cols], y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------------------------------------\n",
      "{'n_estimators': 125, 'max_depth': 2, 'verbose': 0, 'min_samples_leaf': 6}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.73      0.84      2975\n",
      "          1       0.14      0.82      0.25       164\n",
      "\n",
      "avg / total       0.94      0.74      0.81      3139\n",
      "\n",
      "0.73749601784\n",
      "[[2181  794]\n",
      " [  30  134]]\n",
      "0.817073170732\n"
     ]
    }
   ],
   "source": [
    "gb_clf, gb_ypred = classifier_runner(gb, X_train_, y_train_['lapsed_next_period'], X_test, y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "----------------------------------------\n",
      "{'C': 0.01, 'kernel': 'linear'}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.73      0.84      2975\n",
      "          1       0.14      0.79      0.24       164\n",
      "\n",
      "avg / total       0.94      0.73      0.81      3139\n",
      "\n",
      "0.732398853138\n",
      "[[2169  806]\n",
      " [  34  130]]\n",
      "0.792682926829\n"
     ]
    }
   ],
   "source": [
    "svc_clf, svc_ypred = classifier_runner(svc, X_train_, y_train_['lapsed_next_period'], X_test, y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "----------------------------------------\n",
      "{'solver': 'liblinear', 'max_iter': 100, 'verbose': 0}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.74      0.84      2975\n",
      "          1       0.14      0.79      0.24       164\n",
      "\n",
      "avg / total       0.94      0.74      0.81      3139\n",
      "\n",
      "0.738770309016\n",
      "[[2190  785]\n",
      " [  35  129]]\n",
      "0.786585365854\n"
     ]
    }
   ],
   "source": [
    "log_clf, log_ypred =classifier_runner(log, X_train_[model_cols], y_train_['lapsed_next_period'],\n",
    "                                      X_test[model_cols], y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "----------------------------------------\n",
      "{'n_estimators': 500, 'objective': 'binary:logistic', 'silent': 1, 'colsample_bytree': 0.7, 'missing': -999, 'subsample': 0.8, 'min_child_weight': 11, 'max_depth': 3, 'learning_rate': 0.002}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.76      0.86      2975\n",
      "          1       0.15      0.77      0.25       164\n",
      "\n",
      "avg / total       0.94      0.76      0.82      3139\n",
      "\n",
      "0.758521822236\n",
      "[[2254  721]\n",
      " [  37  127]]\n",
      "0.774390243902\n"
     ]
    }
   ],
   "source": [
    "xgbm_clf, xgbm_ypred = classifier_runner(xgbm, X_train_[model_cols], y_train_['lapsed_next_period'],\n",
    "                                         X_test[model_cols], y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "rf_best = SklearnHelper(clf=RandomForestClassifier, params=rf_clf.best_params_)\n",
    "et_best = SklearnHelper(clf=ExtraTreesClassifier, params=et_clf.best_params_)\n",
    "ada_best = SklearnHelper(clf=AdaBoostClassifier, params=ada_clf.best_params_)\n",
    "gb_best = SklearnHelper(clf=GradientBoostingClassifier, params=gb_clf.best_params_)\n",
    "svc_best = SklearnHelper(clf=SVC, params=svc_clf.best_params_)\n",
    "log_best = SklearnHelper(clf=LogisticRegression, params=log_clf.best_params_)\n",
    "xgbm_best = SklearnHelper(clf=xgb.XGBClassifier, params=xgbm_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_best = rf_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "et_best = et_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "ada_best = ada_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "gb_best = gb_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "svc_best = svc_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "log_best = log_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "xgbm_best = xgbm_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_best_score = right_classification(y_pred=rf_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "et_best_score = right_classification(y_pred=et_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "ada_best_score = right_classification(y_pred=ada_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "gb_best_score = right_classification(y_pred=gb_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "svc_best_score = right_classification(y_pred=svc_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "log_best_score = right_classification(y_pred=log_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "xgbm_best_score = right_classification(y_pred=xgbm_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_best_score_test = right_classification(y_pred=rf_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "et_best_score_test = right_classification(y_pred=et_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "ada_best_score_test = right_classification(y_pred=ada_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "gb_best_score_test = right_classification(y_pred=gb_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "svc_best_score_test = right_classification(y_pred=svc_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "log_best_score_test = right_classification(y_pred=log_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "xgbm_best_score_test = right_classification(y_pred=xgbm_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score on test</th>\n",
       "      <th>Score on training</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.872146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoosting</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.831050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forests</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.828006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extratrees</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.828006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.810976</td>\n",
       "      <td>0.824962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.774390</td>\n",
       "      <td>0.812785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.811263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Score on test  Score on training\n",
       "3       Gradient Boosting       0.804878           0.872146\n",
       "2             AdaBoosting       0.804878           0.831050\n",
       "0          Random Forests       0.780488           0.828006\n",
       "1              Extratrees       0.780488           0.828006\n",
       "4  Support Vector Machine       0.810976           0.824962\n",
       "6                 XGBoost       0.774390           0.812785\n",
       "5     Logistic Regression       0.786585           0.811263"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Random Forests', 'Extratrees', 'AdaBoosting', \n",
    "              'Gradient Boosting', 'Support Vector Machine', \n",
    "              'Logistic Regression','XGBoost'],\n",
    "    'Score on training': [rf_best_score, et_best_score, ada_best_score, gb_best_score, \n",
    "              svc_best_score, log_best_score, xgbm_best_score],\n",
    "    'Score on test': [rf_best_score_test, et_best_score_test, ada_best_score_test, gb_best_score_test, \n",
    "              svc_best_score_test, log_best_score_test, xgbm_best_score_test]})\n",
    "models.sort_values(by='Score on training', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>RF</th>\n",
       "      <th>Extra</th>\n",
       "      <th>Ada</th>\n",
       "      <th>GB</th>\n",
       "      <th>XGB</th>\n",
       "      <th>Median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>order_id_199004</td>\n",
       "      <td>0.145753</td>\n",
       "      <td>0.185703</td>\n",
       "      <td>0.108571</td>\n",
       "      <td>0.097599</td>\n",
       "      <td>0.165395</td>\n",
       "      <td>0.145753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gs_per_order_199004</td>\n",
       "      <td>0.170076</td>\n",
       "      <td>0.081671</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.124455</td>\n",
       "      <td>0.134574</td>\n",
       "      <td>0.134574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>product_type_id_199004</td>\n",
       "      <td>0.196244</td>\n",
       "      <td>0.215018</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.071432</td>\n",
       "      <td>0.131523</td>\n",
       "      <td>0.131523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ns_per_ib_199004</td>\n",
       "      <td>0.098034</td>\n",
       "      <td>0.057496</td>\n",
       "      <td>0.108571</td>\n",
       "      <td>0.153155</td>\n",
       "      <td>0.031431</td>\n",
       "      <td>0.098034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ns_per_ib_199003</td>\n",
       "      <td>0.027389</td>\n",
       "      <td>0.016060</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.082357</td>\n",
       "      <td>0.085444</td>\n",
       "      <td>0.082357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gs_per_order_199003</td>\n",
       "      <td>0.036514</td>\n",
       "      <td>0.020510</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.110610</td>\n",
       "      <td>0.082698</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>net_spend_199003</td>\n",
       "      <td>0.035222</td>\n",
       "      <td>0.039968</td>\n",
       "      <td>0.131429</td>\n",
       "      <td>0.068863</td>\n",
       "      <td>0.069576</td>\n",
       "      <td>0.068863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>order_id_199003</td>\n",
       "      <td>0.056243</td>\n",
       "      <td>0.070407</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>0.063168</td>\n",
       "      <td>0.056243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>quote_spend_returned_199004</td>\n",
       "      <td>0.068504</td>\n",
       "      <td>0.067396</td>\n",
       "      <td>0.051429</td>\n",
       "      <td>0.039393</td>\n",
       "      <td>0.030821</td>\n",
       "      <td>0.051429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quote_spend_returned_199003</td>\n",
       "      <td>0.034502</td>\n",
       "      <td>0.047177</td>\n",
       "      <td>0.074286</td>\n",
       "      <td>0.037039</td>\n",
       "      <td>0.095209</td>\n",
       "      <td>0.047177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>product_type_id_199003</td>\n",
       "      <td>0.060908</td>\n",
       "      <td>0.085688</td>\n",
       "      <td>0.045714</td>\n",
       "      <td>0.016175</td>\n",
       "      <td>0.020751</td>\n",
       "      <td>0.045714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>quote_var2_199004</td>\n",
       "      <td>0.027438</td>\n",
       "      <td>0.042835</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.028354</td>\n",
       "      <td>0.006713</td>\n",
       "      <td>0.028354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ib_per_order_199003</td>\n",
       "      <td>0.022558</td>\n",
       "      <td>0.019165</td>\n",
       "      <td>0.011429</td>\n",
       "      <td>0.101427</td>\n",
       "      <td>0.031736</td>\n",
       "      <td>0.022558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>var5_0.0</td>\n",
       "      <td>0.002732</td>\n",
       "      <td>0.009627</td>\n",
       "      <td>0.011429</td>\n",
       "      <td>0.015134</td>\n",
       "      <td>0.014342</td>\n",
       "      <td>0.011429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>var5_1.0</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.009862</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.018330</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.009862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>quote_var2_199003</td>\n",
       "      <td>0.009588</td>\n",
       "      <td>0.016986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014062</td>\n",
       "      <td>0.009155</td>\n",
       "      <td>0.009588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>var6_1.0</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.002834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006002</td>\n",
       "      <td>0.003052</td>\n",
       "      <td>0.002834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>var6_0.0</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014452</td>\n",
       "      <td>0.010375</td>\n",
       "      <td>0.002691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>var3_1.0</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005798</td>\n",
       "      <td>0.000960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>var3_2.0</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.000915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>var3_3.0</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.004361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.000851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>var3_0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Feature        RF     Extra       Ada        GB  \\\n",
       "1               order_id_199004  0.145753  0.185703  0.108571  0.097599   \n",
       "9           gs_per_order_199004  0.170076  0.081671  0.160000  0.124455   \n",
       "3        product_type_id_199004  0.196244  0.215018  0.057143  0.071432   \n",
       "10             ns_per_ib_199004  0.098034  0.057496  0.108571  0.153155   \n",
       "13             ns_per_ib_199003  0.027389  0.016060  0.085714  0.082357   \n",
       "11          gs_per_order_199003  0.036514  0.020510  0.080000  0.110610   \n",
       "4              net_spend_199003  0.035222  0.039968  0.131429  0.068863   \n",
       "0               order_id_199003  0.056243  0.070407  0.005714  0.001160   \n",
       "6   quote_spend_returned_199004  0.068504  0.067396  0.051429  0.039393   \n",
       "5   quote_spend_returned_199003  0.034502  0.047177  0.074286  0.037039   \n",
       "2        product_type_id_199003  0.060908  0.085688  0.045714  0.016175   \n",
       "8             quote_var2_199004  0.027438  0.042835  0.028571  0.028354   \n",
       "12          ib_per_order_199003  0.022558  0.019165  0.011429  0.101427   \n",
       "18                     var5_0.0  0.002732  0.009627  0.011429  0.015134   \n",
       "19                     var5_1.0  0.002347  0.009862  0.040000  0.018330   \n",
       "7             quote_var2_199003  0.009588  0.016986  0.000000  0.014062   \n",
       "21                     var6_1.0  0.001388  0.002834  0.000000  0.006002   \n",
       "20                     var6_0.0  0.001438  0.002691  0.000000  0.014452   \n",
       "15                     var3_1.0  0.000960  0.002821  0.000000  0.000000   \n",
       "16                     var3_2.0  0.001310  0.001654  0.000000  0.000000   \n",
       "17                     var3_3.0  0.000851  0.004361  0.000000  0.000000   \n",
       "14                     var3_0.0  0.000000  0.000070  0.000000  0.000000   \n",
       "\n",
       "         XGB    Median  \n",
       "1   0.165395  0.145753  \n",
       "9   0.134574  0.134574  \n",
       "3   0.131523  0.131523  \n",
       "10  0.031431  0.098034  \n",
       "13  0.085444  0.082357  \n",
       "11  0.082698  0.080000  \n",
       "4   0.069576  0.068863  \n",
       "0   0.063168  0.056243  \n",
       "6   0.030821  0.051429  \n",
       "5   0.095209  0.047177  \n",
       "2   0.020751  0.045714  \n",
       "8   0.006713  0.028354  \n",
       "12  0.031736  0.022558  \n",
       "18  0.014342  0.011429  \n",
       "19  0.004883  0.009862  \n",
       "7   0.009155  0.009588  \n",
       "21  0.003052  0.002834  \n",
       "20  0.010375  0.002691  \n",
       "15  0.005798  0.000960  \n",
       "16  0.000915  0.000915  \n",
       "17  0.002441  0.000851  \n",
       "14  0.000000  0.000000  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = pd.DataFrame(list(zip(X_train_[model_cols].columns,\n",
    "    np.transpose(rf_best.feature_importances_),\n",
    "    np.transpose(et_best.feature_importances_),\n",
    "    np.transpose(ada_best.feature_importances_),\n",
    "    np.transpose(gb_best.feature_importances_),\n",
    "    np.transpose(xgbm_best.feature_importances_),\n",
    "    )), columns=['Feature','RF','Extra','Ada','GB','XGB'])\n",
    "  \n",
    "summary['Median'] = summary.median(1)\n",
    "summary.sort_values('Median', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAucAAAKVCAYAAABs7UUCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7b5b8a24a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# correlation between model predictions on training data\n",
    "base_predictions_train = pd.DataFrame( {'RandomForest': rf_best.predict(X_train_[model_cols]),\n",
    "     'ExtraTrees': et_best.predict(X_train_[model_cols]),\n",
    "     'AdaBoost': ada_best.predict(X_train_[model_cols]),\n",
    "     'SVM' : svc_best.predict(X_train_[model_cols]),\n",
    "     'GradientBoost': gb_best.predict(X_train_[model_cols]),\n",
    "     'Logistic Regression': log_best.predict(X_train_[model_cols]),\n",
    "     'XGBoost': xgbm_best.predict(X_train_[model_cols])\n",
    "    })\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "foo = sns.heatmap(base_predictions_train.corr(), vmax=1.0, square=True, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=4, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=5, min_samples_split=2,\n",
       "            min_weigh...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard',\n",
       "         weights=[2, 1, 1, 2, 2, 3, 1])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Voting classifier\n",
    "clf_vote = VotingClassifier(\n",
    "    estimators=[\n",
    "        #('tree', clf_tree),\n",
    "        ('rf', rf_best),\n",
    "        ('et', et_best),\n",
    "        ('ada', ada_best),\n",
    "        ('gb', gb_best),\n",
    "        ('xgb', xgbm_best),\n",
    "        ('svm', svc_best),\n",
    "        ('logistic', log_best)\n",
    "        ],\n",
    "    weights=[2,1,1,2,2,3,1],\n",
    "    voting='hard')\n",
    "clf_vote.fit(X_train_[model_cols], y_train_['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81887366818873664"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_vote_score_train = right_classification(y_pred=clf_vote.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "clf_vote_score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78048780487804881"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_vote_score_test = right_classification(y_pred=clf_vote.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "clf_vote_score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2222  753]\n",
      " [  36  128]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test['lapsed_next_period'], clf_vote.predict(X_test[model_cols])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2155  820]\n",
      " [  31  133]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test['lapsed_next_period'], svc_best.predict(X_test[model_cols])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.728894552405\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test['lapsed_next_period'], svc_best.predict(X_test[model_cols])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.810975609756\n"
     ]
    }
   ],
   "source": [
    "print(right_classification(y_test['lapsed_next_period'], svc_best.predict(X_test[model_cols])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the very last test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "----------------------------------------\n",
      "{'C': 0.005, 'kernel': 'linear'}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.74      0.84      2975\n",
      "          1       0.14      0.79      0.24       164\n",
      "\n",
      "avg / total       0.94      0.74      0.81      3139\n",
      "\n",
      "0.738133163428\n",
      "[[2187  788]\n",
      " [  34  130]]\n",
      "0.792682926829\n"
     ]
    }
   ],
   "source": [
    "def acc_recall(y_true, y_pred):\n",
    "    cf = confusion_matrix(y_true, y_pred)\n",
    "    ac = accuracy_score(y_true, y_pred)\n",
    "    return 0.45 * ac + 0.65 * (cf[1, 1] / cf[1, :].sum())\n",
    "\n",
    "def acc_recall_v2(y_true, y_pred):\n",
    "    cf = confusion_matrix(y_true, y_pred)\n",
    "    ac = accuracy_score(y_true, y_pred)\n",
    "    rc = cf[1, 1] / cf[1, :].sum()\n",
    "    return (0.45 * ac + 0.65 * rc) / abs(rc-ac)\n",
    "\n",
    "new_scoring = {'acc_recall': make_scorer(acc_recall_v2)}\n",
    "\n",
    "# SVC parameters\n",
    "svc_params_gs = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [1e-8, 5e-8, 1e-7, 5e-7, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 0.01, 0.05]\n",
    "}\n",
    "\n",
    "svc_new = GridSearchCV(SVC(), svc_params_gs, cv=5, scoring=new_scoring, refit='acc_recall')\n",
    "\n",
    "svc_clf_new, svc_ypred_new = classifier_runner(svc_new, X_train_, y_train_['lapsed_next_period'],\n",
    "                                               X_test, y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.847403825982\n"
     ]
    }
   ],
   "source": [
    "print(acc_recall(y_test['lapsed_next_period'], svc_ypred_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finalized_model.sav']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "joblib.dump(rf_best, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(X_test[model_cols].iloc[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
