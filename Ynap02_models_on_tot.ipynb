{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve\n",
    "from sklearn.metrics import make_scorer, confusion_matrix\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/science/shared/ipythonNotebooks/leom/Kaggle/Ynap-master/ynap_data\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(os.getcwd(), 'ynap_data')\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tot = pd.read_csv(os.path.join(data_path, 'df_tot_log.csv'))\n",
    "df_tot['var3'] = df_tot['var3'].astype('category')\n",
    "df_tot['var5'] = df_tot['var5'].astype('category')\n",
    "df_tot['var6'] = df_tot['var6'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(821, 23)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tot[df_tot['lapsed_next_period'] == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make dummies out of categorical variables\n",
    "dummy_idx = np.where(df_tot.dtypes == 'category')[0]\n",
    "df_dummies = pd.get_dummies(df_tot.iloc[:, dummy_idx])\n",
    "df = pd.concat([df_tot.drop(df_tot.iloc[:, dummy_idx].columns.values, axis=1), df_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def column_finder(df, col, method=2):\n",
    "    if method == 1:\n",
    "        # method 1:\n",
    "        cols_as_string = ' '.join(df.columns.values)\n",
    "        cols_found = list(re.findall(col + '.*?\\ ', cols_as_string))\n",
    "        cols_found = [x.strip(' ') for x in cols_found]\n",
    "    elif method == 2:\n",
    "        # method 2:\n",
    "        cols_found = []\n",
    "        for col_elem in df.columns.values:\n",
    "            if col in col_elem:\n",
    "                cols_found.append(col_elem)\n",
    "    else:\n",
    "        raise ValueError\n",
    "            \n",
    "    return cols_found\n",
    "\n",
    "\n",
    "def column_remover(df, col_list, noprint=False):    \n",
    "    if type(col_list) != list:\n",
    "        cl = []\n",
    "        cl.append(col_list)\n",
    "        col_list =cl\n",
    "    \n",
    "    col_to_remove = []\n",
    "    for col in col_list:\n",
    "        #print(col)\n",
    "        col_to_remove.extend(column_finder(df, col, method=2))\n",
    "    \n",
    "    df.drop(col_to_remove, axis=1, inplace=True)\n",
    "    if not noprint:\n",
    "        print(col_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['product_id', 'designer_id', 'gross_spend', 'item_bought', 'item_returned', 'ns_per_order', 'ir_per_order', 'gs_per_item']\n"
     ]
    }
   ],
   "source": [
    "# column_remover(df_tot, ['product_id', 'designer_id', '_1989' #, '_199001', '_199002'\n",
    "#                        ], noprint=True)\n",
    "\n",
    "columns_to_remove = ['product_id', 'designer_id', 'gross_spend', 'item_bought', 'item_returned',\n",
    "                                'ns_per_order', 'ir_per_order', 'gs_per_item']\n",
    "\n",
    "# columns_to_remove = ['product_id', 'designer_id']\n",
    "\n",
    "column_remover(df, columns_to_remove)\n",
    "# print(df_tot.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['customer_id', 'var4', 'lapsed_next_period', 'order_id',\n",
       "       'product_type_id', 'net_spend', 'quote_spend_returned',\n",
       "       'quote_var1', 'quote_var2', 'gs_per_order', 'ib_per_order',\n",
       "       'ns_per_ib', 'var3_0.0', 'var3_1.0', 'var3_2.0', 'var3_3.0',\n",
       "       'var5_0.0', 'var5_1.0', 'var6_0.0', 'var6_1.0'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lapsed_next_period    1.000000\n",
       "order_id             -0.228204\n",
       "product_type_id      -0.180241\n",
       "net_spend            -0.191040\n",
       "Name: lapsed_next_period, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_tot.drop('customer_id', axis=1).corr().loc['lapsed_next_period'] # response not correlated with anything...\n",
    "\n",
    "corr1 = df.drop('customer_id', axis=1).corr()\n",
    "corr1[corr1.loc['lapsed_next_period'].apply(abs) > 0.15]['lapsed_next_period']\n",
    "# corr1.loc['lapsed_next_period']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_transformer(df, unchangeable_variables, method='std'):\n",
    "    if method == 'std':\n",
    "        scaler = StandardScaler().fit_transform(df.values)\n",
    "    elif method == 'minmax':\n",
    "        scaler = MinMaxScaler().fit_transform(df.values)\n",
    "    elif method == 'normal':\n",
    "        scaler = Normalizer().fit_transform(df.values)\n",
    "    else:\n",
    "        print('Method not recognized')\n",
    "        \n",
    "    df_transformed = pd.DataFrame(scaler, index=df.index, columns=df.columns)\n",
    "    if len(unchangeable_variables) > 0:\n",
    "        df_transformed[unchangeable_variables] = df[unchangeable_variables]\n",
    "    \n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_minmax = data_transformer(df_tot, ['customer_id','lapsed_next_period'], 'minmax')\n",
    "# df_minmax.drop('customer_id', axis=1).corr().loc['lapsed_next_period']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_norm = data_transformer(df_tot, ['customer_id','lapsed_next_period'], 'normal')\n",
    "# df_norm.drop('customer_id', axis=1).corr().loc['lapsed_next_period']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_std = data_transformer(df_tot, ['customer_id','lapsed_next_period'], 'std')\n",
    "# df_std.drop('customer_id', axis=1).corr().loc['lapsed_next_period']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations don't seem to work. One reason could the high sproportion between 0s and 1s in the response variable.\n",
    "Let's try a balance method.\n",
    "\n",
    "## Balance\n",
    "Let's use the standard transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_for_model = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15695, 18)\n",
      "(15695, 2)\n"
     ]
    }
   ],
   "source": [
    "X = df_for_model.drop(['customer_id', 'lapsed_next_period'], axis=1)\n",
    "y = df_for_model[['customer_id', 'lapsed_next_period']]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(657, 18)\n",
      "(11899, 18)\n",
      "0.05232558139534884\n"
     ]
    }
   ],
   "source": [
    "X_train_1 = X_train.loc[y_train[y_train['lapsed_next_period'] == 1].index]\n",
    "X_train_0 = X_train.loc[y_train[y_train['lapsed_next_period'] == 0].index]\n",
    "\n",
    "print(X_train_1.shape)\n",
    "print(X_train_0.shape)\n",
    "print(X_train_1.shape[0]/X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.052245938196877985"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[y_test['lapsed_next_period'] == 1].shape[0] / y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try different balance ratios with a couple of easy models\n",
    "\n",
    "def balance_tester(Xtr0, Xtr1, ytr, Xte, yte, zero_quotas = [0.5, 0.66, 0.75], seeds = [21, 2121, 1212], \n",
    "                   method='log', method_params=None):\n",
    "    \n",
    "    if (method == 'rf') & (method_params == None):\n",
    "        # Random Forest parameters\n",
    "        rf_params_bal = {'n_jobs': -1, 'n_estimators': 500, 'warm_start': True, \n",
    "                         'max_depth': 4, 'min_samples_leaf': 2, 'max_features' : 'sqrt',\n",
    "                         'verbose': 0}\n",
    "    \n",
    "    results = pd.DataFrame(columns=['quota', 'accuracy', 'pred_perc', 'recall'])\n",
    "    \n",
    "    for quota in zero_quotas:\n",
    "        accuracy = []\n",
    "        pred_perc = []\n",
    "        recall = []\n",
    "        \n",
    "        n_sample = int(Xtr1.shape[0] * quota / (1 - quota) // 1)\n",
    "        \n",
    "        for seed in seeds:\n",
    "            Xtr0_ = Xtr0.sample(n=n_sample, random_state=seed)\n",
    "            Xtr_ = Xtr0_.append(Xtr1)\n",
    "            ytr_ = ytr.loc[Xtr_.index]\n",
    "            \n",
    "            if method == 'log':\n",
    "                clf = LogisticRegression(max_iter=100, solver='liblinear')\n",
    "            elif method == 'rf':\n",
    "                clf = RandomForestClassifier(**rf_params_bal)\n",
    "            else:\n",
    "                return 'Method not recognized'\n",
    "            \n",
    "            clf.fit(Xtr_, ytr_)\n",
    "            clf_pred = clf.predict(Xte)\n",
    "            cf = confusion_matrix(yte, clf_pred)\n",
    "            \n",
    "            pred_perc.append(clf_pred.sum()/len(clf_pred))\n",
    "            accuracy.append(accuracy_score(yte, clf_pred))\n",
    "            recall.append(cf[1, 1] / cf[1, :].sum())\n",
    "            \n",
    "            \n",
    "        # print(np.array(accuracy).mean())\n",
    "        acc_mean = np.array(accuracy).mean()\n",
    "        pp_mean = np.array(pred_perc).mean()\n",
    "        rec_mean = np.array(recall).mean()\n",
    "        \n",
    "        results = results.append(pd.Series({'quota': quota, 'accuracy': acc_mean,\n",
    "                                            'pred_perc': pp_mean, 'recall': rec_mean}),\n",
    "                                 ignore_index=True)\n",
    "    \n",
    "    return results\n",
    "            \n",
    "quotas = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.825, 0.85, 0.9]\n",
    "# quotas = [0.8, 0.805, 0.81, 0.815, 0.82, 0.825, 0.83, 0.835, 0.84, 0.845, 0.85]\n",
    "balance_res_rf = balance_tester(X_train_0, X_train_1, y_train['lapsed_next_period'], X_test, \n",
    "                                y_test['lapsed_next_period'], quotas, method='rf')\n",
    "\n",
    "balance_res_log = balance_tester(X_train_0, X_train_1, y_train['lapsed_next_period'], X_test, \n",
    "                                y_test['lapsed_next_period'], quotas, method='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quota</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>pred_perc</th>\n",
       "      <th>recall</th>\n",
       "      <th>acc_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.724116</td>\n",
       "      <td>0.301795</td>\n",
       "      <td>0.747967</td>\n",
       "      <td>0.738427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550</td>\n",
       "      <td>0.762982</td>\n",
       "      <td>0.254646</td>\n",
       "      <td>0.668699</td>\n",
       "      <td>0.706412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.800680</td>\n",
       "      <td>0.208453</td>\n",
       "      <td>0.587398</td>\n",
       "      <td>0.672711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.650</td>\n",
       "      <td>0.831687</td>\n",
       "      <td>0.167675</td>\n",
       "      <td>0.493902</td>\n",
       "      <td>0.629016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.700</td>\n",
       "      <td>0.862270</td>\n",
       "      <td>0.130721</td>\n",
       "      <td>0.432927</td>\n",
       "      <td>0.604664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.892322</td>\n",
       "      <td>0.090050</td>\n",
       "      <td>0.331301</td>\n",
       "      <td>0.555709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.800</td>\n",
       "      <td>0.912286</td>\n",
       "      <td>0.057131</td>\n",
       "      <td>0.207317</td>\n",
       "      <td>0.489305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.825</td>\n",
       "      <td>0.920569</td>\n",
       "      <td>0.043751</td>\n",
       "      <td>0.158537</td>\n",
       "      <td>0.463350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.850</td>\n",
       "      <td>0.930126</td>\n",
       "      <td>0.030583</td>\n",
       "      <td>0.123984</td>\n",
       "      <td>0.446441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.900</td>\n",
       "      <td>0.944887</td>\n",
       "      <td>0.004991</td>\n",
       "      <td>0.020325</td>\n",
       "      <td>0.390150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quota  accuracy  pred_perc    recall  acc_recall\n",
       "0  0.500  0.724116   0.301795  0.747967    0.738427\n",
       "1  0.550  0.762982   0.254646  0.668699    0.706412\n",
       "2  0.600  0.800680   0.208453  0.587398    0.672711\n",
       "3  0.650  0.831687   0.167675  0.493902    0.629016\n",
       "4  0.700  0.862270   0.130721  0.432927    0.604664\n",
       "5  0.750  0.892322   0.090050  0.331301    0.555709\n",
       "6  0.800  0.912286   0.057131  0.207317    0.489305\n",
       "7  0.825  0.920569   0.043751  0.158537    0.463350\n",
       "8  0.850  0.930126   0.030583  0.123984    0.446441\n",
       "9  0.900  0.944887   0.004991  0.020325    0.390150"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_res_log['acc_recall'] = 0.4 * balance_res_log['accuracy'] + 0.6 * balance_res_log['recall']\n",
    "balance_res_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quota</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>pred_perc</th>\n",
       "      <th>recall</th>\n",
       "      <th>acc_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.737708</td>\n",
       "      <td>0.281406</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.704839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550</td>\n",
       "      <td>0.775619</td>\n",
       "      <td>0.236275</td>\n",
       "      <td>0.613821</td>\n",
       "      <td>0.678540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.803865</td>\n",
       "      <td>0.199958</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.643497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.650</td>\n",
       "      <td>0.832537</td>\n",
       "      <td>0.165764</td>\n",
       "      <td>0.483740</td>\n",
       "      <td>0.623259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.700</td>\n",
       "      <td>0.865244</td>\n",
       "      <td>0.124774</td>\n",
       "      <td>0.404472</td>\n",
       "      <td>0.588780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.895827</td>\n",
       "      <td>0.085059</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.548575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.800</td>\n",
       "      <td>0.931082</td>\n",
       "      <td>0.033025</td>\n",
       "      <td>0.156504</td>\n",
       "      <td>0.466335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.825</td>\n",
       "      <td>0.940427</td>\n",
       "      <td>0.017734</td>\n",
       "      <td>0.099593</td>\n",
       "      <td>0.435927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.850</td>\n",
       "      <td>0.946267</td>\n",
       "      <td>0.005522</td>\n",
       "      <td>0.038618</td>\n",
       "      <td>0.401678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.900</td>\n",
       "      <td>0.947754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quota  accuracy  pred_perc    recall  acc_recall\n",
       "0  0.500  0.737708   0.281406  0.682927    0.704839\n",
       "1  0.550  0.775619   0.236275  0.613821    0.678540\n",
       "2  0.600  0.803865   0.199958  0.536585    0.643497\n",
       "3  0.650  0.832537   0.165764  0.483740    0.623259\n",
       "4  0.700  0.865244   0.124774  0.404472    0.588780\n",
       "5  0.750  0.895827   0.085059  0.317073    0.548575\n",
       "6  0.800  0.931082   0.033025  0.156504    0.466335\n",
       "7  0.825  0.940427   0.017734  0.099593    0.435927\n",
       "8  0.850  0.946267   0.005522  0.038618    0.401678\n",
       "9  0.900  0.947754   0.000000  0.000000    0.379102"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_res_rf['acc_recall'] = 0.4 * balance_res_rf['accuracy'] + 0.6 * balance_res_rf['recall']\n",
    "balance_res_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "My objective is to get the highest recall possibile. The overall accuracy doesn't fit for this problem, because classifying the right 1s has more importance than classifying everyone correctly, whether they are 0s or 1s.\n",
    "The recall measure is perfect for this problem. I want to have the highest percentage possible on classifying the customers I know are true 1s (in other words, I want to minimize the outcome set to 0 to those I know are 1), while I consider less important to have an error on those I classify 1 when they are actually 0.\n",
    "\n",
    "A basic proportion of 50% for not lapsed values seems to lead to good results. Let's use that for the GridSearch step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quota = 0.5\n",
    "n_sample = int(X_train_1.shape[0] * quota / (1 - quota) // 1)\n",
    "X_train_0_ = X_train_0.sample(n=n_sample, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1314, 18)\n",
      "(1314, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train_ = X_train_0_.append(X_train_1)\n",
    "print(X_train_.shape)\n",
    "\n",
    "y_train_ = y_train.loc[X_train_.index]\n",
    "print(y_train_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[y_test['lapsed_next_period'] == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ = y_train_.join(X_train_)\n",
    "y_corr = df_.drop('customer_id', axis=1).corr().loc['lapsed_next_period']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_cols = list(y_corr[y_corr.apply(lambda x: abs(x) > 0.15)].index.values)\n",
    "model_cols = list(X_train_.columns.values)\n",
    "# model_cols.remove('lapsed_next_period')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['var4',\n",
       " 'order_id',\n",
       " 'product_type_id',\n",
       " 'net_spend',\n",
       " 'quote_spend_returned',\n",
       " 'quote_var1',\n",
       " 'quote_var2',\n",
       " 'gs_per_order',\n",
       " 'ib_per_order',\n",
       " 'ns_per_ib',\n",
       " 'var3_0.0',\n",
       " 'var3_1.0',\n",
       " 'var3_2.0',\n",
       " 'var3_3.0',\n",
       " 'var5_0.0',\n",
       " 'var5_1.0',\n",
       " 'var6_0.0',\n",
       " 'var6_1.0']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.737442922374\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression for feature selection\n",
    "log_test = LogisticRegression()\n",
    "log_test = log_test.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "score_log = log_test.score(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "print(score_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>var4</td>\n",
       "      <td>[-0.0566413229487]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>order_id</td>\n",
       "      <td>[-0.809969788516]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>product_type_id</td>\n",
       "      <td>[-0.022418823623]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>net_spend</td>\n",
       "      <td>[-0.0763324628972]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>quote_spend_returned</td>\n",
       "      <td>[-1.37913763766]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quote_var1</td>\n",
       "      <td>[0.0611310977907]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>quote_var2</td>\n",
       "      <td>[-0.0517798271361]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gs_per_order</td>\n",
       "      <td>[0.651419067133]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ib_per_order</td>\n",
       "      <td>[-0.406952424115]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ns_per_ib</td>\n",
       "      <td>[-0.544371558201]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>var3_0.0</td>\n",
       "      <td>[0.101382556824]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>var3_1.0</td>\n",
       "      <td>[0.529264997992]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>var3_2.0</td>\n",
       "      <td>[0.385585564184]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>var3_3.0</td>\n",
       "      <td>[0.249219428936]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>var5_0.0</td>\n",
       "      <td>[0.889824854598]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>var5_1.0</td>\n",
       "      <td>[0.375627693338]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>var6_0.0</td>\n",
       "      <td>[0.491852847868]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>var6_1.0</td>\n",
       "      <td>[0.773599700068]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0                   1\n",
       "0                   var4  [-0.0566413229487]\n",
       "1               order_id   [-0.809969788516]\n",
       "2        product_type_id   [-0.022418823623]\n",
       "3              net_spend  [-0.0763324628972]\n",
       "4   quote_spend_returned    [-1.37913763766]\n",
       "5             quote_var1   [0.0611310977907]\n",
       "6             quote_var2  [-0.0517798271361]\n",
       "7           gs_per_order    [0.651419067133]\n",
       "8           ib_per_order   [-0.406952424115]\n",
       "9              ns_per_ib   [-0.544371558201]\n",
       "10              var3_0.0    [0.101382556824]\n",
       "11              var3_1.0    [0.529264997992]\n",
       "12              var3_2.0    [0.385585564184]\n",
       "13              var3_3.0    [0.249219428936]\n",
       "14              var5_0.0    [0.889824854598]\n",
       "15              var5_1.0    [0.375627693338]\n",
       "16              var6_0.0    [0.491852847868]\n",
       "17              var6_1.0    [0.773599700068]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(zip(X_train_[model_cols].columns, np.transpose(log_test.coef_))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*product_type_id* seems not really effective as a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-6ab687aded8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_cols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'product_type_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "model_cols.remove('product_type_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['var4',\n",
       " 'order_id',\n",
       " 'net_spend',\n",
       " 'quote_spend_returned',\n",
       " 'quote_var1',\n",
       " 'quote_var2',\n",
       " 'gs_per_order',\n",
       " 'ib_per_order',\n",
       " 'ns_per_ib',\n",
       " 'var3_0.0',\n",
       " 'var3_1.0',\n",
       " 'var3_2.0',\n",
       " 'var3_3.0',\n",
       " 'var5_0.0',\n",
       " 'var5_1.0',\n",
       " 'var6_0.0',\n",
       " 'var6_1.0']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.730593607306\n"
     ]
    }
   ],
   "source": [
    "# REPEAT Logistic Regression for feature selection\n",
    "log_test = LogisticRegression()\n",
    "log_test = log_test.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "score_log = log_test.score(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "print(score_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>var4</td>\n",
       "      <td>[-0.0592772334508]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>order_id</td>\n",
       "      <td>[-0.918768681656]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>net_spend</td>\n",
       "      <td>[-0.138552454757]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quote_spend_returned</td>\n",
       "      <td>[-1.47756606124]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>quote_var1</td>\n",
       "      <td>[0.0605165529204]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quote_var2</td>\n",
       "      <td>[-0.0595461000229]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gs_per_order</td>\n",
       "      <td>[0.649979268231]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ib_per_order</td>\n",
       "      <td>[-0.507817358798]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ns_per_ib</td>\n",
       "      <td>[-0.411519872854]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>var3_0.0</td>\n",
       "      <td>[0.118876260841]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>var3_1.0</td>\n",
       "      <td>[0.546636901413]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>var3_2.0</td>\n",
       "      <td>[0.409958713536]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>var3_3.0</td>\n",
       "      <td>[0.27244024269]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>var5_0.0</td>\n",
       "      <td>[0.941264677942]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>var5_1.0</td>\n",
       "      <td>[0.406647440539]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>var6_0.0</td>\n",
       "      <td>[0.533008867834]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>var6_1.0</td>\n",
       "      <td>[0.814903250646]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0                   1\n",
       "0                   var4  [-0.0592772334508]\n",
       "1               order_id   [-0.918768681656]\n",
       "2              net_spend   [-0.138552454757]\n",
       "3   quote_spend_returned    [-1.47756606124]\n",
       "4             quote_var1   [0.0605165529204]\n",
       "5             quote_var2  [-0.0595461000229]\n",
       "6           gs_per_order    [0.649979268231]\n",
       "7           ib_per_order   [-0.507817358798]\n",
       "8              ns_per_ib   [-0.411519872854]\n",
       "9               var3_0.0    [0.118876260841]\n",
       "10              var3_1.0    [0.546636901413]\n",
       "11              var3_2.0    [0.409958713536]\n",
       "12              var3_3.0     [0.27244024269]\n",
       "13              var5_0.0    [0.941264677942]\n",
       "14              var5_1.0    [0.406647440539]\n",
       "15              var6_0.0    [0.533008867834]\n",
       "16              var6_1.0    [0.814903250646]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(zip(X_train_[model_cols].columns, np.transpose(log_test.coef_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.300095571838\n",
      "0.721567378146\n"
     ]
    }
   ],
   "source": [
    "# Random Forest example\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 200,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 4,\n",
    "    'min_samples_leaf': 4,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(**rf_params)\n",
    "rf.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "\n",
    "rf_pred = rf.predict(X_test[model_cols])\n",
    "\n",
    "print(rf_pred.sum()/len(rf_pred))\n",
    "\n",
    "print(accuracy_score(y_test['lapsed_next_period'], rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70731707317073167"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf = confusion_matrix(y_test['lapsed_next_period'], rf_pred)\n",
    "cf[1,1]/cf[1,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def right_classification(y_true, y_pred):\n",
    "    cf = confusion_matrix(y_true, y_pred)\n",
    "    return cf[1, 1] / cf[1, :].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting multiple hyperparameters for every classifier we are going to implement\n",
    "\n",
    "# Random Forest\n",
    "rf_params_gs = {\n",
    "    'n_jobs': [-1],\n",
    "    'n_estimators': [150, 175, 200, 225 , 250, 275], #300, 350, 400, 500],\n",
    "    'warm_start': [True], \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': [2, 3, 4], # 5, 6],\n",
    "    'min_samples_leaf': [4, 5, 6], #[2, 3]\n",
    "    'max_features' : ['sqrt'],\n",
    "    'verbose': [0]\n",
    "}\n",
    "\n",
    "# Extra Trees\n",
    "et_params_gs = {\n",
    "    'n_jobs': [-1],\n",
    "    'n_estimators': [500, 550, 600],\n",
    "    'max_depth': [6, 7, 8],\n",
    "    'min_samples_leaf': [2, 3, 4],\n",
    "    'verbose': [0]\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params_gs = {\n",
    "    'n_estimators': [150, 200, 250, 300],\n",
    "    'learning_rate': [1e-3, 0.01, 0.05, 0.1]\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params_gs = {\n",
    "    'n_estimators': [150, 200, 250, 300],\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'min_samples_leaf': [4, 5, 6],\n",
    "    'verbose': [0]\n",
    "}\n",
    "\n",
    "# SVC parameters\n",
    "svc_params_gs = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [1e-4, 1e-3, 0.01, 0.05]\n",
    "}\n",
    "\n",
    "# Logistic regression parameters\n",
    "log_params_gs = {\n",
    "    'solver': ['liblinear'],\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'verbose': [0]\n",
    "}\n",
    "\n",
    "# XGBoosting parameters\n",
    "xgb_params_gs = {\n",
    "    'objective':['binary:logistic'],\n",
    "    'learning_rate': [1e-4, 1e-3, 5e-3, 0.01], #so called `eta` value\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'min_child_weight': [11],\n",
    "    'silent': [1],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.7],\n",
    "    'n_estimators': [200, 400, 600, 800, 1000], #number of trees, change it to 1000 for better results\n",
    "    'missing':[-999]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoring = {'right_class': make_scorer(right_classification)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Istantiate the classifiers\n",
    "rf = GridSearchCV(RandomForestClassifier(), rf_params_gs, cv=5, scoring=scoring, refit='right_class')\n",
    "et = GridSearchCV(ExtraTreesClassifier(), et_params_gs, cv=5, scoring=scoring, refit='right_class')\n",
    "ada = GridSearchCV(AdaBoostClassifier(), ada_params_gs, cv=5, scoring=scoring, refit='right_class')\n",
    "gb = GridSearchCV(GradientBoostingClassifier(), gb_params_gs, cv=5, scoring=scoring, refit='right_class')\n",
    "svc = GridSearchCV(SVC(), svc_params_gs, cv=5, scoring=scoring, refit='right_class')\n",
    "log = GridSearchCV(LogisticRegression(), log_params_gs, cv=5, scoring=scoring, refit='right_class')\n",
    "xgbm = GridSearchCV(xgb.XGBClassifier(), xgb_params_gs, cv=5, scoring=scoring, refit='right_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier_runner(clf, Xtr, ytr, Xte, yte):\n",
    "    print('-'*40)\n",
    "    print(clf.estimator)\n",
    "    print('-'*40)\n",
    "    clf.fit(Xtr, ytr)\n",
    "    clf.best_params_\n",
    "    print(clf.best_params_)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "#     means = clf.cv_results_['mean_test_score']\n",
    "#     stds = clf.cv_results_['std_test_score']\n",
    "#     for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "#         print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std*2, params))\n",
    "        \n",
    "    print()\n",
    "    y_true, y_pred = yte, clf.predict(Xte)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(accuracy_score(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(right_classification(y_true, y_pred))\n",
    "    \n",
    "    return clf, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------------------------------------\n",
      "{'max_depth': 4, 'n_jobs': -1, 'warm_start': True, 'verbose': 0, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.73      0.83      2975\n",
      "          1       0.12      0.70      0.21       164\n",
      "\n",
      "avg / total       0.93      0.73      0.80      3139\n",
      "\n",
      "0.72602739726\n",
      "[[2165  810]\n",
      " [  50  114]]\n",
      "0.69512195122\n"
     ]
    }
   ],
   "source": [
    "rf_clf, rf_ypred = classifier_runner(rf, X_train_[model_cols], y_train_['lapsed_next_period'],\n",
    "                                      X_test[model_cols], y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------------------------------------\n",
      "{'min_samples_leaf': 4, 'max_depth': 7, 'verbose': 0, 'n_jobs': -1, 'n_estimators': 500}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.72      0.83      2975\n",
      "          1       0.12      0.71      0.21       164\n",
      "\n",
      "avg / total       0.93      0.72      0.80      3139\n",
      "\n",
      "0.718381650207\n",
      "[[2139  836]\n",
      " [  48  116]]\n",
      "0.707317073171\n"
     ]
    }
   ],
   "source": [
    "et_clf, et_ypred = classifier_runner(et, X_train_[model_cols], y_train_['lapsed_next_period'],\n",
    "                                      X_test[model_cols], y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "----------------------------------------\n",
      "{'learning_rate': 0.05, 'n_estimators': 200}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.71      0.82      2975\n",
      "          1       0.12      0.73      0.21       164\n",
      "\n",
      "avg / total       0.93      0.71      0.79      3139\n",
      "\n",
      "0.712965912711\n",
      "[[2119  856]\n",
      " [  45  119]]\n",
      "0.725609756098\n"
     ]
    }
   ],
   "source": [
    "ada_clf, ada_ypred = classifier_runner(ada, X_train_[model_cols], y_train_['lapsed_next_period'],\n",
    "                                        X_test[model_cols], y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------------------------------------\n",
      "{'min_samples_leaf': 5, 'max_depth': 2, 'verbose': 0, 'n_estimators': 200}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.70      0.81      2975\n",
      "          1       0.12      0.74      0.20       164\n",
      "\n",
      "avg / total       0.93      0.70      0.78      3139\n",
      "\n",
      "0.698630136986\n",
      "[[2072  903]\n",
      " [  43  121]]\n",
      "0.737804878049\n"
     ]
    }
   ],
   "source": [
    "gb_clf, gb_ypred = classifier_runner(gb, X_train_, y_train_['lapsed_next_period'], X_test, y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "----------------------------------------\n",
      "{'C': 0.0001, 'kernel': 'linear'}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.62      0.76      2975\n",
      "          1       0.09      0.72      0.17       164\n",
      "\n",
      "avg / total       0.93      0.62      0.73      3139\n",
      "\n",
      "0.622809812042\n",
      "[[1837 1138]\n",
      " [  46  118]]\n",
      "0.719512195122\n"
     ]
    }
   ],
   "source": [
    "svc_clf, svc_ypred = classifier_runner(svc, X_train_, y_train_['lapsed_next_period'], X_test, y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "----------------------------------------\n",
      "{'verbose': 0, 'max_iter': 100, 'solver': 'liblinear'}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.72      0.83      2975\n",
      "          1       0.13      0.73      0.21       164\n",
      "\n",
      "avg / total       0.93      0.72      0.80      3139\n",
      "\n",
      "0.722204523734\n",
      "[[2148  827]\n",
      " [  45  119]]\n",
      "0.725609756098\n"
     ]
    }
   ],
   "source": [
    "log_clf, log_ypred =classifier_runner(log, X_train_[model_cols], y_train_['lapsed_next_period'], X_test[model_cols], y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "----------------------------------------\n",
      "{'objective': 'binary:logistic', 'max_depth': 5, 'learning_rate': 0.01, 'missing': -999, 'silent': 1, 'subsample': 0.8, 'colsample_bytree': 0.7, 'min_child_weight': 11, 'n_estimators': 400}\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.71      0.82      2975\n",
      "          1       0.12      0.71      0.20       164\n",
      "\n",
      "avg / total       0.93      0.71      0.79      3139\n",
      "\n",
      "0.706594456833\n",
      "[[2102  873]\n",
      " [  48  116]]\n",
      "0.707317073171\n"
     ]
    }
   ],
   "source": [
    "xgbm_clf, xgbm_ypred = classifier_runner(xgbm, X_train_[model_cols], y_train_['lapsed_next_period'], X_test[model_cols], y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "rf_best = SklearnHelper(clf=RandomForestClassifier, params=rf_clf.best_params_)\n",
    "et_best = SklearnHelper(clf=ExtraTreesClassifier, params=et_clf.best_params_)\n",
    "ada_best = SklearnHelper(clf=AdaBoostClassifier, params=ada_clf.best_params_)\n",
    "gb_best = SklearnHelper(clf=GradientBoostingClassifier, params=gb_clf.best_params_)\n",
    "svc_best = SklearnHelper(clf=SVC, params=svc_clf.best_params_)\n",
    "log_best = SklearnHelper(clf=LogisticRegression, params=log_clf.best_params_)\n",
    "xgbm_best = SklearnHelper(clf=xgb.XGBClassifier, params=xgbm_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_best = rf_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "et_best = et_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "ada_best = ada_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "gb_best = gb_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "svc_best = svc_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "log_best = log_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])\n",
    "xgbm_best = xgbm_best.fit(X_train_[model_cols], y_train_['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_score = right_classification(y_pred=rf_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "et_best_score = right_classification(y_pred=et_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "ada_best_score = right_classification(y_pred=ada_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "gb_best_score = right_classification(y_pred=gb_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "svc_best_score = right_classification(y_pred=svc_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "log_best_score = right_classification(y_pred=log_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "xgbm_best_score = right_classification(y_pred=xgbm_best.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_best_score_test = right_classification(y_pred=rf_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "et_best_score_test = right_classification(y_pred=et_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "ada_best_score_test = right_classification(y_pred=ada_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "gb_best_score_test = right_classification(y_pred=gb_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "svc_best_score_test = right_classification(y_pred=svc_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "log_best_score_test = right_classification(y_pred=log_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "xgbm_best_score_test = right_classification(y_pred=xgbm_best.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score on test</th>\n",
       "      <th>Score on training</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.725610</td>\n",
       "      <td>0.820396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.808219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extratrees</td>\n",
       "      <td>0.713415</td>\n",
       "      <td>0.768645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forests</td>\n",
       "      <td>0.713415</td>\n",
       "      <td>0.753425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoosting</td>\n",
       "      <td>0.725610</td>\n",
       "      <td>0.742770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.725610</td>\n",
       "      <td>0.738204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Score on test  Score on training\n",
       "4  Support Vector Machine       0.865854           0.888889\n",
       "3       Gradient Boosting       0.725610           0.820396\n",
       "6                 XGBoost       0.707317           0.808219\n",
       "1              Extratrees       0.713415           0.768645\n",
       "0          Random Forests       0.713415           0.753425\n",
       "2             AdaBoosting       0.725610           0.742770\n",
       "5     Logistic Regression       0.725610           0.738204"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Random Forests', 'Extratrees', 'AdaBoosting', \n",
    "              'Gradient Boosting', 'Support Vector Machine', \n",
    "              'Logistic Regression','XGBoost'],\n",
    "    'Score on training': [rf_best_score, et_best_score, ada_best_score, gb_best_score, \n",
    "              svc_best_score, log_best_score, xgbm_best_score],\n",
    "    'Score on test': [rf_best_score_test, et_best_score_test, ada_best_score_test, gb_best_score_test, \n",
    "              svc_best_score_test, log_best_score_test, xgbm_best_score_test]})\n",
    "models.sort_values(by='Score on training', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>RF</th>\n",
       "      <th>Extra</th>\n",
       "      <th>Ada</th>\n",
       "      <th>GB</th>\n",
       "      <th>XGB</th>\n",
       "      <th>Median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>order_id</td>\n",
       "      <td>0.370429</td>\n",
       "      <td>0.365752</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.132363</td>\n",
       "      <td>0.139013</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>net_spend</td>\n",
       "      <td>0.230516</td>\n",
       "      <td>0.191806</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.136212</td>\n",
       "      <td>0.141846</td>\n",
       "      <td>0.141846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gs_per_order</td>\n",
       "      <td>0.062132</td>\n",
       "      <td>0.035686</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.139623</td>\n",
       "      <td>0.139485</td>\n",
       "      <td>0.139485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quote_spend_returned</td>\n",
       "      <td>0.117549</td>\n",
       "      <td>0.084304</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.112191</td>\n",
       "      <td>0.123200</td>\n",
       "      <td>0.117549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>var4</td>\n",
       "      <td>0.059745</td>\n",
       "      <td>0.103787</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.113569</td>\n",
       "      <td>0.082606</td>\n",
       "      <td>0.103787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ib_per_order</td>\n",
       "      <td>0.042541</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.113463</td>\n",
       "      <td>0.106915</td>\n",
       "      <td>0.042541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ns_per_ib</td>\n",
       "      <td>0.036313</td>\n",
       "      <td>0.020351</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.116611</td>\n",
       "      <td>0.120132</td>\n",
       "      <td>0.036313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>quote_var1</td>\n",
       "      <td>0.023801</td>\n",
       "      <td>0.034588</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.040315</td>\n",
       "      <td>0.020297</td>\n",
       "      <td>0.034588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quote_var2</td>\n",
       "      <td>0.027762</td>\n",
       "      <td>0.020499</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.055828</td>\n",
       "      <td>0.079065</td>\n",
       "      <td>0.027762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>var5_0.0</td>\n",
       "      <td>0.007923</td>\n",
       "      <td>0.031661</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.006191</td>\n",
       "      <td>0.018881</td>\n",
       "      <td>0.018881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>var5_1.0</td>\n",
       "      <td>0.011694</td>\n",
       "      <td>0.029793</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.003068</td>\n",
       "      <td>0.011694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>var3_2.0</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.009327</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005310</td>\n",
       "      <td>0.005428</td>\n",
       "      <td>0.005310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>var3_1.0</td>\n",
       "      <td>0.002656</td>\n",
       "      <td>0.012781</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009031</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>0.002656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>var6_0.0</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>0.012087</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0.009441</td>\n",
       "      <td>0.002410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>var3_3.0</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.013660</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006608</td>\n",
       "      <td>0.002138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>var6_1.0</td>\n",
       "      <td>0.001515</td>\n",
       "      <td>0.011710</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009982</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.002124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>var3_0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Feature        RF     Extra    Ada        GB       XGB  \\\n",
       "1               order_id  0.370429  0.365752  0.280  0.132363  0.139013   \n",
       "2              net_spend  0.230516  0.191806  0.080  0.136212  0.141846   \n",
       "6           gs_per_order  0.062132  0.035686  0.145  0.139623  0.139485   \n",
       "3   quote_spend_returned  0.117549  0.084304  0.125  0.112191  0.123200   \n",
       "0                   var4  0.059745  0.103787  0.165  0.113569  0.082606   \n",
       "7           ib_per_order  0.042541  0.021898  0.025  0.113463  0.106915   \n",
       "8              ns_per_ib  0.036313  0.020351  0.020  0.116611  0.120132   \n",
       "4             quote_var1  0.023801  0.034588  0.100  0.040315  0.020297   \n",
       "5             quote_var2  0.027762  0.020499  0.000  0.055828  0.079065   \n",
       "13              var5_0.0  0.007923  0.031661  0.030  0.006191  0.018881   \n",
       "14              var5_1.0  0.011694  0.029793  0.030  0.006900  0.003068   \n",
       "11              var3_2.0  0.001359  0.009327  0.000  0.005310  0.005428   \n",
       "10              var3_1.0  0.002656  0.012781  0.000  0.009031  0.001888   \n",
       "15              var6_0.0  0.001926  0.012087  0.000  0.002410  0.009441   \n",
       "12              var3_3.0  0.002138  0.013660  0.000  0.000000  0.006608   \n",
       "16              var6_1.0  0.001515  0.011710  0.000  0.009982  0.002124   \n",
       "9               var3_0.0  0.000000  0.000310  0.000  0.000000  0.000000   \n",
       "\n",
       "      Median  \n",
       "1   0.280000  \n",
       "2   0.141846  \n",
       "6   0.139485  \n",
       "3   0.117549  \n",
       "0   0.103787  \n",
       "7   0.042541  \n",
       "8   0.036313  \n",
       "4   0.034588  \n",
       "5   0.027762  \n",
       "13  0.018881  \n",
       "14  0.011694  \n",
       "11  0.005310  \n",
       "10  0.002656  \n",
       "15  0.002410  \n",
       "12  0.002138  \n",
       "16  0.002124  \n",
       "9   0.000000  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = pd.DataFrame(list(zip(X_train_[model_cols].columns,\n",
    "    np.transpose(rf_best.feature_importances_),\n",
    "    np.transpose(et_best.feature_importances_),\n",
    "    np.transpose(ada_best.feature_importances_),\n",
    "    np.transpose(gb_best.feature_importances_),\n",
    "    np.transpose(xgbm_best.feature_importances_),\n",
    "    )), columns=['Feature','RF','Extra','Ada','GB','XGB'])\n",
    "  \n",
    "summary['Median'] = summary.median(1)\n",
    "summary.sort_values('Median', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuEAAAKVCAYAAABh8zVFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRERE9Fgm4RERERERPfb/W1u/AvDmsBwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c210ec438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# correlation between model predictions on training data\n",
    "base_predictions_train = pd.DataFrame( {'RandomForest': rf_best.predict(X_train_[model_cols]),\n",
    "     'ExtraTrees': et_best.predict(X_train_[model_cols]),\n",
    "     'AdaBoost': ada_best.predict(X_train_[model_cols]),\n",
    "     'SVM' : svc_best.predict(X_train_[model_cols]),\n",
    "     'GradientBoost': gb_best.predict(X_train_[model_cols]),\n",
    "     'Logistic Regression': log_best.predict(X_train_[model_cols]),\n",
    "     'XGBoost': xgbm_best.predict(X_train_[model_cols])\n",
    "    })\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "foo = sns.heatmap(base_predictions_train.corr(), vmax=1.0, square=True, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=4, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=5, min_samples_split=2,\n",
       "            min_weigh...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard',\n",
       "         weights=[1, 1, 1, 2, 2, 3, 1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Voting classifier\n",
    "clf_vote = VotingClassifier(\n",
    "    estimators=[\n",
    "        #('tree', clf_tree),\n",
    "        ('rf', rf_best),\n",
    "        ('et', et_best),\n",
    "        ('ada', ada_best),\n",
    "        ('gb', gb_best),\n",
    "        ('xgb', xgbm_best),\n",
    "        ('svm', svc_best),\n",
    "        ('logistic', log_best)\n",
    "        ],\n",
    "    weights=[1,1,1,2,2,3,1],\n",
    "    voting='hard')\n",
    "clf_vote.fit(X_train_[model_cols], y_train_['lapsed_next_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78995433789954339"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_vote_score_train = right_classification(y_pred=clf_vote.predict(X_train_[model_cols]),\n",
    "                                     y_true=y_train_['lapsed_next_period'])\n",
    "clf_vote_score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74390243902439024"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_vote_score_test = right_classification(y_pred=clf_vote.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period'])\n",
    "clf_vote_score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2042  933]\n",
      " [  42  122]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_pred=clf_vote.predict(X_test[model_cols]),\n",
    "                                     y_true=y_test['lapsed_next_period']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1167 1808]\n",
      " [  22  142]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test['lapsed_next_period'], svc_best.predict(X_test[model_cols])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
